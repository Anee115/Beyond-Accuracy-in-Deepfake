{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e943bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "\n",
    "class PowerMonitor:\n",
    "    def __init__(self):\n",
    "        self.gpu_available = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        # Hardware power specifications (adjust these values for your system)\n",
    "        self.cpu_tdp = 65    # Typical TDP for desktop CPUs in watts\n",
    "        self.gpu_tdp = 250   # Typical TDP for desktop GPUs in watts\n",
    "        \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get system stats with power estimation\"\"\"\n",
    "        stats = {\n",
    "            'timestamp': time.time(),\n",
    "            'cpu_%': psutil.cpu_percent(interval=0.1),\n",
    "            'ram_mb': psutil.virtual_memory().used / (1024**2),\n",
    "            'gpu_mem_mb': 0,\n",
    "            'power_w': self.cpu_tdp * (psutil.cpu_percent()/100) * 0.85  # Base CPU power\n",
    "        }\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            try:\n",
    "                # TensorFlow GPU memory monitoring\n",
    "                mem_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                stats.update({\n",
    "                    'gpu_mem_mb': mem_info['current'] / (1024**2),\n",
    "                    'power_w': self.cpu_tdp * (psutil.cpu_percent()/100) * 0.85 + \n",
    "                              self.gpu_tdp * 0.5 * 0.75  # Add GPU power estimate\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return stats\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = PowerMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ad26a",
   "metadata": {},
   "source": [
    "# Celeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15192947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 7.5%\n",
      "RAM Used: 2094.0 MB\n",
      "Time Usage: 6.9 s\n",
      "GPU Memory Used: 0.0 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "import h5py\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "#with h5py.File(\"D:\\\\thesis\\dataset\\deepfake dataset\\images_celeb_224R_processed.h5\", \"r\") as h5f:\n",
    "with h5py.File(\"D:\\\\thesis\\dataset\\Celeb-Df-v2\\images_celeb_balanced_224R_processed.h5\", \"r\") as h5f:\n",
    "    # Load HOG features\n",
    "    real_frames_array1 = h5f[\"ori_actor\"][:]\n",
    "    fake_frames_array1 = h5f[\"ori_youtube\"][:]\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Function to resize images from (224, 224) to (160, 160)\n",
    "def resize_images(image_array, target_size=(160, 160)):\n",
    "    resized_images = np.array([cv2.resize(img, target_size) for img in image_array])\n",
    "    return resized_images\n",
    "\n",
    "\n",
    "# Resize the images\n",
    "real_frames_array1 = resize_images(real_frames_array1, target_size=(160, 160))\n",
    "fake_frames_array1 = resize_images(fake_frames_array1, target_size=(160, 160))\n",
    "\n",
    "# Checking the new shapes\n",
    "real_frames_array1.shape, fake_frames_array1.shape\n",
    "\n",
    "\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f6dc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data real shape: (6909, 160, 160, 3)\n",
      "Testing real data shape: (2961, 160, 160, 3)\n",
      "Training fake data shape: (7891, 160, 160, 3)\n",
      "Testing  fake data shape: (3383, 160, 160, 3)\n",
      "train_hog_real: 6218 images, val_hog_real: 691 images\n",
      "train_hog_fake: 7101 images, val_hog_fake: 790 images\n",
      "Total train: 13319 images\n",
      "Total test: 10852 images\n",
      "Total val: 1481 images\n",
      "Train Labels: 13319 \n",
      "Test Labels: 10852 \n",
      "Val Labels: 1481 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into train (70%) and test (30%)\n",
    "X_train_real, X_test_real = train_test_split(real_frames_array1, test_size=0.3, random_state=42)\n",
    "X_train_fake, X_test_fake = train_test_split(fake_frames_array1, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Output the shape to confirm the split dimensions\n",
    "print(\"Training data real shape:\", X_train_real.shape)\n",
    "print(\"Testing real data shape:\", X_test_real.shape)\n",
    "print(\"Training fake data shape:\", X_train_fake.shape)\n",
    "print(\"Testing  fake data shape:\", X_test_fake.shape)\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "train_hog_real, val_hog_real = extract_validation(X_train_real)\n",
    "train_hog_fake, val_hog_fake = extract_validation(X_train_fake)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"train_hog_real: {len(train_hog_real)} images, val_hog_real: {len(val_hog_real)} images\")\n",
    "print(f\"train_hog_fake: {len(train_hog_fake)} images, val_hog_fake: {len(val_hog_fake)} images\")\n",
    "\n",
    "\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "train_labels_real = np.zeros(len(train_hog_real), dtype=int)\n",
    "train_labels_fake = np.ones(len(train_hog_fake), dtype=int)\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train = np.concatenate([train_hog_real, train_hog_fake], axis=0)\n",
    "train_labels=np.concatenate([train_labels_real, train_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "test_labels_real = np.zeros(len(X_test_real), dtype=int)\n",
    "test_labels_fake = np.ones(len(X_train_fake), dtype=int)\n",
    "\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test = np.concatenate([X_test_real, X_train_fake], axis=0)\n",
    "test_labels = np.concatenate([test_labels_real, test_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "val_labels_real = np.zeros(len(val_hog_real), dtype=int)\n",
    "val_labels_fake = np.ones(len(val_hog_fake), dtype=int)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val = np.concatenate([val_hog_real, val_hog_fake], axis=0)\n",
    "val_labels = np.concatenate([val_labels_real, val_labels_fake], axis=0)\n",
    "\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train)} images\")\n",
    "print(f\"Total test: {len(test)} images\")\n",
    "print(f\"Total val: {len(val)} images\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels)} \")\n",
    "print(f\"Test Labels: {len(test_labels)} \")\n",
    "print(f\"Val Labels: {len(val_labels)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec6e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6307ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "def build_xception_model(input_shape=(160, 160, 3)):\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        weights='imagenet',  # Pretrained on ImageNet\n",
    "        include_top=False,   # Exclude the top classification layer\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_xception_model(input_shape=(160, 160, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5bbe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "417/417 [==============================] - 21s 38ms/step - loss: 1.7117 - accuracy: 0.5295 - val_loss: 0.7041 - val_accuracy: 0.5658\n",
      "Epoch 2/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.7186 - accuracy: 0.5669 - val_loss: 0.6706 - val_accuracy: 0.5854\n",
      "Epoch 3/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.6792 - accuracy: 0.5940 - val_loss: 0.6589 - val_accuracy: 0.6104\n",
      "Epoch 4/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6620 - accuracy: 0.5961 - val_loss: 0.6511 - val_accuracy: 0.6354\n",
      "Epoch 5/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.6559 - accuracy: 0.6138 - val_loss: 0.6470 - val_accuracy: 0.6307\n",
      "Epoch 6/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6452 - accuracy: 0.6290 - val_loss: 0.6387 - val_accuracy: 0.6442\n",
      "Epoch 7/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6353 - accuracy: 0.6326 - val_loss: 0.6414 - val_accuracy: 0.6388\n",
      "Epoch 8/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6306 - accuracy: 0.6410 - val_loss: 0.6271 - val_accuracy: 0.6624\n",
      "Epoch 9/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6237 - accuracy: 0.6485 - val_loss: 0.6275 - val_accuracy: 0.6529\n",
      "Epoch 10/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.6199 - accuracy: 0.6564 - val_loss: 0.6217 - val_accuracy: 0.6583\n",
      "Epoch 11/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.6136 - accuracy: 0.6585 - val_loss: 0.6182 - val_accuracy: 0.6739\n",
      "Epoch 12/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.6051 - accuracy: 0.6660 - val_loss: 0.6156 - val_accuracy: 0.6590\n",
      "Epoch 13/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5986 - accuracy: 0.6748 - val_loss: 0.6033 - val_accuracy: 0.6739\n",
      "Epoch 14/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5948 - accuracy: 0.6751 - val_loss: 0.5951 - val_accuracy: 0.6955\n",
      "Epoch 15/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5863 - accuracy: 0.6803 - val_loss: 0.6059 - val_accuracy: 0.6793\n",
      "Epoch 16/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5813 - accuracy: 0.6898 - val_loss: 0.5908 - val_accuracy: 0.6962\n",
      "Epoch 17/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5716 - accuracy: 0.6929 - val_loss: 0.5944 - val_accuracy: 0.6894\n",
      "Epoch 18/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5655 - accuracy: 0.7004 - val_loss: 0.5771 - val_accuracy: 0.7083\n",
      "Epoch 19/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5655 - accuracy: 0.6967 - val_loss: 0.5751 - val_accuracy: 0.7144\n",
      "Epoch 20/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5562 - accuracy: 0.7109 - val_loss: 0.5726 - val_accuracy: 0.7245\n",
      "Epoch 21/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5503 - accuracy: 0.7145 - val_loss: 0.5725 - val_accuracy: 0.7097\n",
      "Epoch 22/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5455 - accuracy: 0.7169 - val_loss: 0.5603 - val_accuracy: 0.7076\n",
      "Epoch 23/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5385 - accuracy: 0.7222 - val_loss: 0.5598 - val_accuracy: 0.7157\n",
      "Epoch 24/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5307 - accuracy: 0.7272 - val_loss: 0.5495 - val_accuracy: 0.7225\n",
      "Epoch 25/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5247 - accuracy: 0.7323 - val_loss: 0.5507 - val_accuracy: 0.7306\n",
      "Epoch 26/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5250 - accuracy: 0.7326 - val_loss: 0.5430 - val_accuracy: 0.7394\n",
      "Epoch 27/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.5178 - accuracy: 0.7369 - val_loss: 0.5327 - val_accuracy: 0.7238\n",
      "Epoch 28/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5138 - accuracy: 0.7404 - val_loss: 0.5350 - val_accuracy: 0.7508\n",
      "Epoch 29/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5072 - accuracy: 0.7460 - val_loss: 0.5397 - val_accuracy: 0.7306\n",
      "Epoch 30/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.5016 - accuracy: 0.7491 - val_loss: 0.5330 - val_accuracy: 0.7346\n",
      "Epoch 31/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4914 - accuracy: 0.7524 - val_loss: 0.5246 - val_accuracy: 0.7360\n",
      "Epoch 32/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4997 - accuracy: 0.7497 - val_loss: 0.5311 - val_accuracy: 0.7414\n",
      "Epoch 33/100\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.4893 - accuracy: 0.7568 - val_loss: 0.5322 - val_accuracy: 0.7427\n",
      "Epoch 34/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.4822 - accuracy: 0.7621 - val_loss: 0.5127 - val_accuracy: 0.7562\n",
      "Epoch 35/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.4778 - accuracy: 0.7626 - val_loss: 0.5139 - val_accuracy: 0.7616\n",
      "Epoch 36/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4712 - accuracy: 0.7691 - val_loss: 0.5221 - val_accuracy: 0.7650\n",
      "Epoch 37/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4726 - accuracy: 0.7653 - val_loss: 0.5071 - val_accuracy: 0.7481\n",
      "Epoch 38/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4672 - accuracy: 0.7700 - val_loss: 0.5066 - val_accuracy: 0.7502\n",
      "Epoch 39/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4630 - accuracy: 0.7743 - val_loss: 0.4968 - val_accuracy: 0.7589\n",
      "Epoch 40/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4555 - accuracy: 0.7780 - val_loss: 0.4949 - val_accuracy: 0.7670\n",
      "Epoch 41/100\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.4531 - accuracy: 0.7778 - val_loss: 0.5134 - val_accuracy: 0.7441\n",
      "Epoch 42/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4480 - accuracy: 0.7862 - val_loss: 0.4914 - val_accuracy: 0.7616\n",
      "Epoch 43/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4439 - accuracy: 0.7860 - val_loss: 0.4866 - val_accuracy: 0.7738\n",
      "Epoch 44/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4376 - accuracy: 0.7913 - val_loss: 0.4949 - val_accuracy: 0.7616\n",
      "Epoch 45/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.4386 - accuracy: 0.7871 - val_loss: 0.4959 - val_accuracy: 0.7637\n",
      "Epoch 46/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4355 - accuracy: 0.7938 - val_loss: 0.5019 - val_accuracy: 0.7515\n",
      "Epoch 47/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4297 - accuracy: 0.7968 - val_loss: 0.4878 - val_accuracy: 0.7670\n",
      "Epoch 48/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4258 - accuracy: 0.8016 - val_loss: 0.4842 - val_accuracy: 0.7785\n",
      "Epoch 49/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4182 - accuracy: 0.8037 - val_loss: 0.4739 - val_accuracy: 0.7684\n",
      "Epoch 50/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4214 - accuracy: 0.8014 - val_loss: 0.4862 - val_accuracy: 0.7643\n",
      "Epoch 51/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4145 - accuracy: 0.8040 - val_loss: 0.4733 - val_accuracy: 0.7799\n",
      "Epoch 52/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4182 - accuracy: 0.8036 - val_loss: 0.4692 - val_accuracy: 0.7833\n",
      "Epoch 53/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4102 - accuracy: 0.8028 - val_loss: 0.4741 - val_accuracy: 0.7745\n",
      "Epoch 54/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4113 - accuracy: 0.8026 - val_loss: 0.4689 - val_accuracy: 0.7799\n",
      "Epoch 55/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.4071 - accuracy: 0.8085 - val_loss: 0.4772 - val_accuracy: 0.7785\n",
      "Epoch 56/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.4018 - accuracy: 0.8097 - val_loss: 0.4654 - val_accuracy: 0.7839\n",
      "Epoch 57/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3956 - accuracy: 0.8182 - val_loss: 0.4779 - val_accuracy: 0.7779\n",
      "Epoch 58/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.3942 - accuracy: 0.8157 - val_loss: 0.4507 - val_accuracy: 0.7927\n",
      "Epoch 59/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3966 - accuracy: 0.8140 - val_loss: 0.4626 - val_accuracy: 0.7880\n",
      "Epoch 60/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3808 - accuracy: 0.8209 - val_loss: 0.4569 - val_accuracy: 0.8028\n",
      "Epoch 61/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3817 - accuracy: 0.8207 - val_loss: 0.4646 - val_accuracy: 0.7806\n",
      "Epoch 62/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3852 - accuracy: 0.8188 - val_loss: 0.4621 - val_accuracy: 0.7907\n",
      "Epoch 63/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3830 - accuracy: 0.8206 - val_loss: 0.4585 - val_accuracy: 0.7873\n",
      "Epoch 64/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.3767 - accuracy: 0.8247 - val_loss: 0.4483 - val_accuracy: 0.8015\n",
      "Epoch 65/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3736 - accuracy: 0.8244 - val_loss: 0.4533 - val_accuracy: 0.7799\n",
      "Epoch 66/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3696 - accuracy: 0.8293 - val_loss: 0.4458 - val_accuracy: 0.8055\n",
      "Epoch 67/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3732 - accuracy: 0.8227 - val_loss: 0.4437 - val_accuracy: 0.8042\n",
      "Epoch 68/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3755 - accuracy: 0.8257 - val_loss: 0.4452 - val_accuracy: 0.8008\n",
      "Epoch 69/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3634 - accuracy: 0.8331 - val_loss: 0.4369 - val_accuracy: 0.8089\n",
      "Epoch 70/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3603 - accuracy: 0.8327 - val_loss: 0.4399 - val_accuracy: 0.8022\n",
      "Epoch 71/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3552 - accuracy: 0.8333 - val_loss: 0.4570 - val_accuracy: 0.7853\n",
      "Epoch 72/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3619 - accuracy: 0.8329 - val_loss: 0.4350 - val_accuracy: 0.8076\n",
      "Epoch 73/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3624 - accuracy: 0.8329 - val_loss: 0.4431 - val_accuracy: 0.7954\n",
      "Epoch 74/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3459 - accuracy: 0.8399 - val_loss: 0.4477 - val_accuracy: 0.7927\n",
      "Epoch 75/100\n",
      "417/417 [==============================] - 16s 37ms/step - loss: 0.3496 - accuracy: 0.8399 - val_loss: 0.4528 - val_accuracy: 0.8001\n",
      "Epoch 76/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3426 - accuracy: 0.8468 - val_loss: 0.4434 - val_accuracy: 0.8035\n",
      "Epoch 77/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3484 - accuracy: 0.8421 - val_loss: 0.4483 - val_accuracy: 0.8015\n",
      "Epoch 78/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3422 - accuracy: 0.8419 - val_loss: 0.4498 - val_accuracy: 0.8015\n",
      "Epoch 79/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3430 - accuracy: 0.8455 - val_loss: 0.4374 - val_accuracy: 0.8143\n",
      "Epoch 80/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3383 - accuracy: 0.8452 - val_loss: 0.4407 - val_accuracy: 0.8062\n",
      "Epoch 81/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3276 - accuracy: 0.8507 - val_loss: 0.4252 - val_accuracy: 0.8136\n",
      "Epoch 82/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3283 - accuracy: 0.8534 - val_loss: 0.4467 - val_accuracy: 0.7995\n",
      "Epoch 83/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3297 - accuracy: 0.8495 - val_loss: 0.4384 - val_accuracy: 0.8109\n",
      "Epoch 84/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3368 - accuracy: 0.8472 - val_loss: 0.4297 - val_accuracy: 0.8103\n",
      "Epoch 85/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3278 - accuracy: 0.8531 - val_loss: 0.4341 - val_accuracy: 0.8096\n",
      "Epoch 86/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3268 - accuracy: 0.8497 - val_loss: 0.4252 - val_accuracy: 0.7961\n",
      "Epoch 87/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3282 - accuracy: 0.8539 - val_loss: 0.4282 - val_accuracy: 0.8231\n",
      "Epoch 88/100\n",
      "417/417 [==============================] - 16s 38ms/step - loss: 0.3264 - accuracy: 0.8504 - val_loss: 0.4362 - val_accuracy: 0.8055\n",
      "Epoch 89/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3219 - accuracy: 0.8559 - val_loss: 0.4339 - val_accuracy: 0.8082\n",
      "Epoch 90/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3238 - accuracy: 0.8512 - val_loss: 0.4318 - val_accuracy: 0.8116\n",
      "Epoch 91/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3191 - accuracy: 0.8572 - val_loss: 0.4330 - val_accuracy: 0.8163\n",
      "Epoch 92/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3128 - accuracy: 0.8600 - val_loss: 0.4309 - val_accuracy: 0.8204\n",
      "Epoch 93/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3183 - accuracy: 0.8555 - val_loss: 0.4346 - val_accuracy: 0.8123\n",
      "Epoch 94/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3074 - accuracy: 0.8640 - val_loss: 0.4363 - val_accuracy: 0.8055\n",
      "Epoch 95/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3045 - accuracy: 0.8607 - val_loss: 0.4291 - val_accuracy: 0.8123\n",
      "Epoch 96/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3101 - accuracy: 0.8600 - val_loss: 0.4318 - val_accuracy: 0.8096\n",
      "Epoch 97/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3107 - accuracy: 0.8594 - val_loss: 0.4349 - val_accuracy: 0.8123\n",
      "Epoch 98/100\n",
      "417/417 [==============================] - 15s 36ms/step - loss: 0.3106 - accuracy: 0.8615 - val_loss: 0.4309 - val_accuracy: 0.8217\n",
      "Epoch 99/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.2978 - accuracy: 0.8649 - val_loss: 0.4295 - val_accuracy: 0.8143\n",
      "Epoch 100/100\n",
      "417/417 [==============================] - 15s 37ms/step - loss: 0.3056 - accuracy: 0.8617 - val_loss: 0.4355 - val_accuracy: 0.8062\n",
      "340/340 [==============================] - 11s 33ms/step - loss: 0.3047 - accuracy: 0.8919\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train, train_labels,\n",
    "    validation_data=(val, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "test_loss, test_acc = model.evaluate(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd151f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Celeb-df on Xception NET: 89.19%\n",
      "340/340 [==============================] - 11s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79      2961\n",
      "           1       0.90      0.95      0.93      7891\n",
      "\n",
      "    accuracy                           0.89     10852\n",
      "   macro avg       0.88      0.84      0.86     10852\n",
      "weighted avg       0.89      0.89      0.89     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Accuracy of Celeb-df on Xception NET: {test_acc * 100:.2f}%')\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions (returns probabilities between 0-1)\n",
    "y_pred_probs = model.predict(test)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_pred_labels = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Ensure test_labels is 1D array (remove extra dimensions if needed)\n",
    "test_labels_flat = np.squeeze(test_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_labels_flat, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# After training the model\n",
    "model.save('xception_160_celeb.h5')  # Saves the entire model to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf978adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 5.4%\n",
      "RAM Used: 4369.7 MB\n",
      "Time Usage: 1566.1 s\n",
      "GPU Memory Used: 1975.6 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600b60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 22s 40ms/step - loss: 0.3047 - accuracy: 0.8919\n",
      "Test Accuracy of Celeb-df on Xception NET: 89.19%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('xception_160_celeb.h5')\n",
    "test_loss, test_acc = model.evaluate(test, test_labels)\n",
    "print(f'Test Accuracy of Celeb-df on Xception NET: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e9f82",
   "metadata": {},
   "source": [
    "#just testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b52774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 0.9%\n",
      "RAM Used: 3425.2 MB\n",
      "Time Usage: 40.9 s\n",
      "GPU Memory Used: 892.8 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07b39d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 31ms/step - loss: 3.7307 - accuracy: 0.5080\n",
      "\n",
      "Test Accuracy of DFC on Xceptionnet from Celebdf: 50.80%\n",
      "94/94 [==============================] - 3s 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.94      0.66      1500\n",
      "           1       0.56      0.08      0.14      1500\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.53      0.51      0.40      3000\n",
      "weighted avg       0.53      0.51      0.40      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('xception_160_celeb.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_hog, test_labels)\n",
    "print(f'\\nTest Accuracy of DFC on Xceptionnet from Celebdf: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_hog)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15bb756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 5s 32ms/step - loss: 3.6365 - accuracy: 0.3733\n",
      "\n",
      "Test Accuracy of FF++ on Xceptionnet from Celebdf: 37.33%\n",
      "169/169 [==============================] - 5s 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.89      0.44      1468\n",
      "           1       0.81      0.18      0.29      3911\n",
      "\n",
      "    accuracy                           0.37      5379\n",
      "   macro avg       0.55      0.53      0.37      5379\n",
      "weighted avg       0.67      0.37      0.33      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'\\nTest Accuracy of FF++ on Xceptionnet from Celebdf: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_ff)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels_ff, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ef59b",
   "metadata": {},
   "source": [
    "# DFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc165b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeb shape: (5000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (5000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "celeb shape: (2500, 160, 160, 3), dtype: uint8\n",
      "ffhq shape: (2500, 160, 160, 3), dtype: uint8\n",
      "gdwct shape: (1000, 160, 160, 3), dtype: uint8\n",
      "attagan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stargan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stylegan2 shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stylegan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "celeb_train: 1750 images, celeb_test: 750 images\n",
      "ffhq_train: 1750 images, ffhq_test: 750 images\n",
      "attgan_train: 700 images, attgan_test: 300 images\n",
      "stargan_train: 700 images, stargan_test: 300 images\n",
      "gdwct_train: 700 images, gdwct_test: 300 images\n",
      "stylegan2_train: 700 images, stylegan2_test: 300 images\n",
      "stylegan_train: 700 images, stylegan_test: 300 images\n",
      "celeb_train: 1575 images, celeb_val: 175 images\n",
      "ffhq_train: 1575 images, ffhq_val: 175 images\n",
      "attgan_train: 630 images, attgan_val: 70 images\n",
      "stargan_train: 630 images, stargan_val: 70 images\n",
      "gdwct_train: 630 images, gdwct_val: 70 images\n",
      "stylegan2_train: 630 images, stylegan2_val: 70 images\n",
      "stylegan_train: 630 images, stylegan_val: 70 images\n",
      "Total train: 6300 images\n",
      "Total test: 3000 images\n",
      "Total val: 700 images\n",
      "Train Labels: 6300 \n",
      "Test Labels: 3000 \n",
      "Val Labels: 700 \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('D://thesis//dataset//deepfake dataset//resized_images.h5', 'r') as h5f:\n",
    "    # Access each dataset\n",
    "    celeb = np.array(h5f['celeb'])\n",
    "    ffhq = np.array(h5f['ffhq'])\n",
    "    gdwct = np.array(h5f['gdwct'])\n",
    "    attgan = np.array(h5f['attgan'])\n",
    "    stargan = np.array(h5f['stargan'])\n",
    "    stylegan2 = np.array(h5f['stylegan2'])\n",
    "    stylegan = np.array(h5f['stylegan'])\n",
    "\n",
    "# Now, 'celeb', 'ffhq', etc., are NumPy arrays containing your datasets\n",
    "print(f\"celeb shape: {celeb.shape}, dtype: {celeb.dtype}\")\n",
    "print(f\"ffhq shape: {ffhq.shape}, dtype: {ffhq.dtype}\")\n",
    "print(f\"ffhq shape: {gdwct.shape}, dtype: {gdwct.dtype}\")\n",
    "print(f\"ffhq shape: {attgan.shape}, dtype: {attgan.dtype}\")\n",
    "print(f\"ffhq shape: {stargan.shape}, dtype: {stargan.dtype}\")\n",
    "print(f\"ffhq shape: {stylegan2.shape}, dtype: {stylegan2.dtype}\")\n",
    "print(f\"ffhq shape: {stylegan.shape}, dtype: {stylegan.dtype}\")\n",
    "# Repeat for other datasets as needed\n",
    "import cv2\n",
    "# Function to resize images from (224, 224) to (160, 160)\n",
    "def resize_images(image_array, target_size=(160, 160)):\n",
    "    resized_images = np.array([cv2.resize(img, target_size) for img in image_array])\n",
    "    return resized_images\n",
    "\n",
    "celeb = resize_images(celeb, target_size=(160, 160))\n",
    "ffhq = resize_images(ffhq, target_size=(160, 160))\n",
    "gdwct = resize_images(gdwct, target_size=(160, 160))\n",
    "attgan = resize_images(attgan, target_size=(160, 160))\n",
    "stargan = resize_images(stargan, target_size=(160, 160))\n",
    "stylegan = resize_images(stargan, target_size=(160, 160))\n",
    "stylegan2 = resize_images(stylegan2, target_size=(160, 160))\n",
    "import random\n",
    "# Randomly select 2500 distinct images\n",
    "random_indices = random.sample(range(len(celeb)), 2500)  # Get 2500 random indices\n",
    "celeb = celeb[random_indices]  # Select the random subse\n",
    "\n",
    "import random\n",
    "# Randomly select 2500 distinct images\n",
    "random_indices = random.sample(range(len(ffhq)), 2500)  # Get 2500 random indices\n",
    "ffhq = ffhq[random_indices]  # Select the random subse\n",
    "print(f\"celeb shape: {celeb.shape}, dtype: {celeb.dtype}\")\n",
    "print(f\"ffhq shape: {ffhq.shape}, dtype: {ffhq.dtype}\")\n",
    "print(f\"gdwct shape: {gdwct.shape}, dtype: {gdwct.dtype}\")\n",
    "print(f\"attagan shape: {attgan.shape}, dtype: {attgan.dtype}\")\n",
    "print(f\"stargan shape: {stargan.shape}, dtype: {stargan.dtype}\")\n",
    "print(f\"stylegan2 shape: {stylegan2.shape}, dtype: {stylegan2.dtype}\")\n",
    "print(f\"stylegan shape: {stylegan.shape}, dtype: {stylegan.dtype}\")\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def split_data(data, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing sets based on the specified ratio.\n",
    "\n",
    "    Parameters:\n",
    "        data (list or np.array): The dataset to split.\n",
    "        train_ratio (float): The ratio of the data to include in the training set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two datasets - train and test.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Split `celeb` into 70% train and 30% test\n",
    "celeb_train_hog, celeb_test_hog = split_data(celeb, train_ratio=0.7)\n",
    "\n",
    "# Split `ffhq` into 70% train and 30% test\n",
    "ffhq_train_hog, ffhq_test_hog = split_data(ffhq, train_ratio=0.7)\n",
    "\n",
    "# Split `attgan` into 70% train and 30% test\n",
    "attgan_train_hog, attgan_test_hog = split_data(attgan, train_ratio=0.7)\n",
    "\n",
    "# Split `stargan` into 70% train and 30% test\n",
    "stargan_train_hog, stargan_test_hog = split_data(stargan, train_ratio=0.7)\n",
    "\n",
    "# Split `gdwct` into 70% train and 30% test\n",
    "gdwct_train_hog, gdwct_test_hog = split_data(gdwct, train_ratio=0.7)\n",
    "\n",
    "# Split `stylegan2` into 70% train and 30% test_hog\n",
    "stylegan2_train_hog, stylegan2_test_hog = split_data(stylegan2, train_ratio=0.7)\n",
    "\n",
    "# Split `stylegan` into 70% train and 30% test_hog\n",
    "stylegan_train_hog, stylegan_test_hog = split_data(stylegan, train_ratio=0.7)\n",
    "\n",
    "# Convert to NumPy arrays if needed\n",
    "celeb_train_hog, celeb_test_hog = np.array(celeb_train_hog), np.array(celeb_test_hog)\n",
    "ffhq_train_hog, ffhq_test_hog = np.array(ffhq_train_hog), np.array(ffhq_test_hog)\n",
    "attgan_train_hog, attgan_test_hog = np.array(attgan_train_hog), np.array(attgan_test_hog)\n",
    "stargan_train_hog, stargan_test_hog = np.array(stargan_train_hog), np.array(stargan_test_hog)\n",
    "gdwct_train_hog, gdwct_test_hog = np.array(gdwct_train_hog), np.array(gdwct_test_hog)\n",
    "stylegan2_train_hog, stylegan2_test_hog = np.array(stylegan2_train_hog), np.array(stylegan2_test_hog)\n",
    "stylegan_train_hog, stylegan_test_hog = np.array(stylegan_train_hog), np.array(stylegan_test_hog)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"celeb_train: {len(celeb_train_hog)} images, celeb_test: {len(celeb_test_hog)} images\")\n",
    "print(f\"ffhq_train: {len(ffhq_train_hog)} images, ffhq_test: {len(ffhq_test_hog)} images\")\n",
    "print(f\"attgan_train: {len(attgan_train_hog)} images, attgan_test: {len(attgan_test_hog)} images\")\n",
    "print(f\"stargan_train: {len(stargan_train_hog)} images, stargan_test: {len(stargan_test_hog)} images\")\n",
    "print(f\"gdwct_train: {len(gdwct_train_hog)} images, gdwct_test: {len(gdwct_test_hog)} images\")\n",
    "print(f\"stylegan2_train: {len(stylegan2_train_hog)} images, stylegan2_test: {len(stylegan2_test_hog)} images\")\n",
    "print(f\"stylegan_train: {len(stylegan_train_hog)} images, stylegan_test: {len(stylegan_test_hog)} images\")\n",
    "\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "celeb_train_hog, celeb_val_hog = extract_validation(celeb_train_hog)\n",
    "ffhq_train_hog, ffhq_val_hog = extract_validation(ffhq_train_hog)\n",
    "attgan_train_hog, attgan_val_hog = extract_validation(attgan_train_hog)\n",
    "stargan_train_hog, stargan_val_hog = extract_validation(stargan_train_hog)\n",
    "gdwct_train_hog, gdwct_val_hog = extract_validation(gdwct_train_hog)\n",
    "stylegan2_train_hog, stylegan2_val_hog = extract_validation(stylegan2_train_hog)\n",
    "stylegan_train_hog, stylegan_val_hog = extract_validation(stylegan_train_hog)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"celeb_train: {len(celeb_train_hog)} images, celeb_val: {len(celeb_val_hog)} images\")\n",
    "print(f\"ffhq_train: {len(ffhq_train_hog)} images, ffhq_val: {len(ffhq_val_hog)} images\")\n",
    "print(f\"attgan_train: {len(attgan_train_hog)} images, attgan_val: {len(attgan_val_hog)} images\")\n",
    "print(f\"stargan_train: {len(stargan_train_hog)} images, stargan_val: {len(stargan_val_hog)} images\")\n",
    "print(f\"gdwct_train: {len(gdwct_train_hog)} images, gdwct_val: {len(gdwct_val_hog)} images\")\n",
    "print(f\"stylegan2_train: {len(stylegan2_train_hog)} images, stylegan2_val: {len(stylegan2_val_hog)} images\")\n",
    "print(f\"stylegan_train: {len(stylegan_train_hog)} images, stylegan_val: {len(stylegan_val_hog)} images\")\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "celeb_train_labels = np.zeros(len(celeb_train_hog), dtype=int)\n",
    "ffhq_train_labels = np.zeros(len(ffhq_train_hog), dtype=int)\n",
    "atta_train_labels = np.ones(len(attgan_train_hog), dtype=int)\n",
    "star_train_labels = np.ones(len(stargan_train_hog), dtype=int)\n",
    "gdwct_train_labels = np.ones(len(gdwct_train_hog), dtype=int)\n",
    "stylegan2_train_labels = np.ones(len(stylegan2_train_hog), dtype=int)\n",
    "stylegan_train_labels = np.ones(len(stylegan_train_hog), dtype=int)\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train_hog = np.concatenate([celeb_train_hog, ffhq_train_hog, attgan_train_hog, stargan_train_hog, gdwct_train_hog, stylegan2_train_hog, stylegan_train_hog], axis=0)\n",
    "train_labels=np.concatenate([celeb_train_labels, ffhq_train_labels, atta_train_labels, star_train_labels, gdwct_train_labels, stylegan2_train_labels,\n",
    "                              stylegan_train_labels], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "celeb_test_labels = np.zeros(len(celeb_test_hog), dtype=int)\n",
    "ffhq_test_labels = np.zeros(len(ffhq_test_hog), dtype=int)\n",
    "atta_test_labels = np.ones(len(attgan_test_hog), dtype=int)\n",
    "star_test_labels = np.ones(len(stargan_test_hog), dtype=int)\n",
    "gdwct_test_labels = np.ones(len(gdwct_test_hog), dtype=int)\n",
    "stylegan2_test_labels = np.ones(len(stylegan2_test_hog), dtype=int)\n",
    "stylegan_test_labels = np.ones(len(stylegan_test_hog), dtype=int)\n",
    "\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test_hog = np.concatenate([celeb_test_hog, ffhq_test_hog, attgan_test_hog, stargan_test_hog, gdwct_test_hog, stylegan2_test_hog, stylegan_test_hog], axis=0)\n",
    "test_labels = np.concatenate([celeb_test_labels, ffhq_test_labels, atta_test_labels, star_test_labels, gdwct_test_labels, stylegan2_test_labels,\n",
    "                        stylegan_test_labels], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "celeb_val_labels = np.zeros(len(celeb_val_hog), dtype=int)\n",
    "ffhq_val_labels = np.zeros(len(ffhq_val_hog), dtype=int)\n",
    "atta_val_labels = np.ones(len(attgan_val_hog), dtype=int)\n",
    "star_val_labels = np.ones(len(stargan_val_hog), dtype=int)\n",
    "gdwct_val_labels = np.ones(len(gdwct_val_hog), dtype=int)\n",
    "stylegan2_val_labels = np.ones(len(stylegan2_val_hog), dtype=int)\n",
    "stylegan_val_labels = np.ones(len(stylegan_val_hog), dtype=int)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val_hog = np.concatenate([celeb_val_hog, ffhq_val_hog, attgan_val_hog, stargan_val_hog, gdwct_val_hog, stylegan2_val_hog, stylegan_val_hog], axis=0)\n",
    "val_labels = np.concatenate([celeb_val_labels, ffhq_val_labels, atta_val_labels, star_val_labels, gdwct_val_labels, stylegan2_val_labels,\n",
    "                       stylegan_val_labels], axis=0)\n",
    "\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train_hog)} images\")\n",
    "print(f\"Total test: {len(test_hog)} images\")\n",
    "print(f\"Total val: {len(val_hog)} images\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels)} \")\n",
    "print(f\"Test Labels: {len(test_labels)} \")\n",
    "print(f\"Val Labels: {len(val_labels)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c6a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "197/197 [==============================] - 13s 40ms/step - loss: 2.9255 - accuracy: 0.5354 - val_loss: 0.8598 - val_accuracy: 0.6143\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 1.0222 - accuracy: 0.6054 - val_loss: 0.6267 - val_accuracy: 0.6543\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.6906 - accuracy: 0.6394 - val_loss: 0.5801 - val_accuracy: 0.7086\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.6117 - accuracy: 0.6787 - val_loss: 0.5364 - val_accuracy: 0.7371\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.5799 - accuracy: 0.7041 - val_loss: 0.5560 - val_accuracy: 0.7543\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.5515 - accuracy: 0.7229 - val_loss: 0.5094 - val_accuracy: 0.7743\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.5358 - accuracy: 0.7306 - val_loss: 0.4955 - val_accuracy: 0.7771\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.5096 - accuracy: 0.7506 - val_loss: 0.4905 - val_accuracy: 0.7843\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4952 - accuracy: 0.7670 - val_loss: 0.4657 - val_accuracy: 0.7771\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4793 - accuracy: 0.7713 - val_loss: 0.4551 - val_accuracy: 0.7857\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4476 - accuracy: 0.7870 - val_loss: 0.4417 - val_accuracy: 0.8029\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4375 - accuracy: 0.7978 - val_loss: 0.4151 - val_accuracy: 0.8329\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4236 - accuracy: 0.8067 - val_loss: 0.4057 - val_accuracy: 0.8329\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.4035 - accuracy: 0.8230 - val_loss: 0.3959 - val_accuracy: 0.8486\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.3971 - accuracy: 0.8252 - val_loss: 0.3920 - val_accuracy: 0.8371\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3744 - accuracy: 0.8359 - val_loss: 0.3699 - val_accuracy: 0.8471\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3655 - accuracy: 0.8424 - val_loss: 0.3679 - val_accuracy: 0.8557\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3521 - accuracy: 0.8486 - val_loss: 0.3658 - val_accuracy: 0.8357\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3403 - accuracy: 0.8594 - val_loss: 0.3415 - val_accuracy: 0.8757\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.3260 - accuracy: 0.8651 - val_loss: 0.3233 - val_accuracy: 0.8729\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.3158 - accuracy: 0.8662 - val_loss: 0.3308 - val_accuracy: 0.8771\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3189 - accuracy: 0.8681 - val_loss: 0.3235 - val_accuracy: 0.8671\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.3003 - accuracy: 0.8792 - val_loss: 0.3161 - val_accuracy: 0.8900\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2915 - accuracy: 0.8825 - val_loss: 0.3151 - val_accuracy: 0.8671\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2713 - accuracy: 0.8906 - val_loss: 0.2996 - val_accuracy: 0.8900\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2695 - accuracy: 0.8908 - val_loss: 0.3034 - val_accuracy: 0.8843\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.2641 - accuracy: 0.8952 - val_loss: 0.2920 - val_accuracy: 0.8900\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.2497 - accuracy: 0.9013 - val_loss: 0.2843 - val_accuracy: 0.8957\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.2427 - accuracy: 0.9032 - val_loss: 0.2921 - val_accuracy: 0.8814\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2369 - accuracy: 0.9070 - val_loss: 0.3043 - val_accuracy: 0.8771\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2372 - accuracy: 0.9070 - val_loss: 0.2885 - val_accuracy: 0.8843\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.2226 - accuracy: 0.9137 - val_loss: 0.2861 - val_accuracy: 0.8957\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2147 - accuracy: 0.9184 - val_loss: 0.2620 - val_accuracy: 0.9071\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2144 - accuracy: 0.9167 - val_loss: 0.2927 - val_accuracy: 0.8986\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.2006 - accuracy: 0.9249 - val_loss: 0.2553 - val_accuracy: 0.8971\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1955 - accuracy: 0.9292 - val_loss: 0.2529 - val_accuracy: 0.9086\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1854 - accuracy: 0.9329 - val_loss: 0.2545 - val_accuracy: 0.8971\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1841 - accuracy: 0.9322 - val_loss: 0.2582 - val_accuracy: 0.9086\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1802 - accuracy: 0.9340 - val_loss: 0.2512 - val_accuracy: 0.9100\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.1791 - accuracy: 0.9321 - val_loss: 0.2562 - val_accuracy: 0.9100\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1743 - accuracy: 0.9303 - val_loss: 0.2364 - val_accuracy: 0.9100\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1586 - accuracy: 0.9441 - val_loss: 0.2404 - val_accuracy: 0.9129\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.1614 - accuracy: 0.9400 - val_loss: 0.2619 - val_accuracy: 0.9071\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1539 - accuracy: 0.9451 - val_loss: 0.2482 - val_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1552 - accuracy: 0.9403 - val_loss: 0.2364 - val_accuracy: 0.9157\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1496 - accuracy: 0.9487 - val_loss: 0.2270 - val_accuracy: 0.9143\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1433 - accuracy: 0.9519 - val_loss: 0.2293 - val_accuracy: 0.9200\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1439 - accuracy: 0.9473 - val_loss: 0.2276 - val_accuracy: 0.9129\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1407 - accuracy: 0.9498 - val_loss: 0.2403 - val_accuracy: 0.9157\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1296 - accuracy: 0.9532 - val_loss: 0.2320 - val_accuracy: 0.9143\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1250 - accuracy: 0.9538 - val_loss: 0.2344 - val_accuracy: 0.9200\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1247 - accuracy: 0.9567 - val_loss: 0.2231 - val_accuracy: 0.9243\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.1182 - accuracy: 0.9605 - val_loss: 0.2272 - val_accuracy: 0.9200\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1185 - accuracy: 0.9594 - val_loss: 0.2252 - val_accuracy: 0.9286\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1217 - accuracy: 0.9560 - val_loss: 0.2501 - val_accuracy: 0.9086\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1176 - accuracy: 0.9562 - val_loss: 0.2369 - val_accuracy: 0.9229\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1168 - accuracy: 0.9557 - val_loss: 0.2271 - val_accuracy: 0.9186\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1087 - accuracy: 0.9606 - val_loss: 0.2306 - val_accuracy: 0.9214\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1048 - accuracy: 0.9652 - val_loss: 0.2158 - val_accuracy: 0.9329\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1030 - accuracy: 0.9638 - val_loss: 0.2097 - val_accuracy: 0.9300\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.1030 - accuracy: 0.9624 - val_loss: 0.2326 - val_accuracy: 0.9271\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0981 - accuracy: 0.9640 - val_loss: 0.2213 - val_accuracy: 0.9343\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0980 - accuracy: 0.9663 - val_loss: 0.2286 - val_accuracy: 0.9214\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0985 - accuracy: 0.9676 - val_loss: 0.2291 - val_accuracy: 0.9229\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0938 - accuracy: 0.9671 - val_loss: 0.2295 - val_accuracy: 0.9229\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0891 - accuracy: 0.9695 - val_loss: 0.2191 - val_accuracy: 0.9343\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0940 - accuracy: 0.9692 - val_loss: 0.2305 - val_accuracy: 0.9329\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0885 - accuracy: 0.9681 - val_loss: 0.2282 - val_accuracy: 0.9314\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0844 - accuracy: 0.9732 - val_loss: 0.2265 - val_accuracy: 0.9371\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0768 - accuracy: 0.9760 - val_loss: 0.2189 - val_accuracy: 0.9329\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0826 - accuracy: 0.9717 - val_loss: 0.2479 - val_accuracy: 0.9214\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0799 - accuracy: 0.9711 - val_loss: 0.2183 - val_accuracy: 0.9271\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0791 - accuracy: 0.9732 - val_loss: 0.2405 - val_accuracy: 0.9229\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0825 - accuracy: 0.9710 - val_loss: 0.2313 - val_accuracy: 0.9257\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0804 - accuracy: 0.9744 - val_loss: 0.2432 - val_accuracy: 0.9243\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0740 - accuracy: 0.9765 - val_loss: 0.2386 - val_accuracy: 0.9357\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 8s 39ms/step - loss: 0.0700 - accuracy: 0.9768 - val_loss: 0.2196 - val_accuracy: 0.9386\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 10s 50ms/step - loss: 0.0727 - accuracy: 0.9759 - val_loss: 0.2218 - val_accuracy: 0.9314\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0726 - accuracy: 0.9759 - val_loss: 0.2318 - val_accuracy: 0.9314\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 10s 50ms/step - loss: 0.0738 - accuracy: 0.9748 - val_loss: 0.2103 - val_accuracy: 0.9329\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 8s 40ms/step - loss: 0.0728 - accuracy: 0.9762 - val_loss: 0.2308 - val_accuracy: 0.9286\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0698 - accuracy: 0.9781 - val_loss: 0.2574 - val_accuracy: 0.9300\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 0.2428 - val_accuracy: 0.9286\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 7s 38ms/step - loss: 0.0655 - accuracy: 0.9795 - val_loss: 0.2239 - val_accuracy: 0.9314\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 10s 49ms/step - loss: 0.0629 - accuracy: 0.9806 - val_loss: 0.2313 - val_accuracy: 0.9300\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0701 - accuracy: 0.9749 - val_loss: 0.2150 - val_accuracy: 0.9314\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 10s 52ms/step - loss: 0.0643 - accuracy: 0.9786 - val_loss: 0.2371 - val_accuracy: 0.9329\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0602 - accuracy: 0.9814 - val_loss: 0.2529 - val_accuracy: 0.9171\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 10s 49ms/step - loss: 0.0667 - accuracy: 0.9775 - val_loss: 0.2502 - val_accuracy: 0.9271\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 10s 50ms/step - loss: 0.0633 - accuracy: 0.9776 - val_loss: 0.2243 - val_accuracy: 0.9314\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0630 - accuracy: 0.9794 - val_loss: 0.2208 - val_accuracy: 0.9357\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0541 - accuracy: 0.9825 - val_loss: 0.2522 - val_accuracy: 0.9286\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 10s 49ms/step - loss: 0.0559 - accuracy: 0.9808 - val_loss: 0.2302 - val_accuracy: 0.9271\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 10s 51ms/step - loss: 0.0588 - accuracy: 0.9797 - val_loss: 0.2868 - val_accuracy: 0.9300\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 9s 47ms/step - loss: 0.0633 - accuracy: 0.9794 - val_loss: 0.2494 - val_accuracy: 0.9314\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0628 - accuracy: 0.9790 - val_loss: 0.2472 - val_accuracy: 0.9343\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 7s 36ms/step - loss: 0.0538 - accuracy: 0.9822 - val_loss: 0.2588 - val_accuracy: 0.9200\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.2563 - val_accuracy: 0.9271\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0541 - accuracy: 0.9816 - val_loss: 0.2610 - val_accuracy: 0.9229\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 7s 37ms/step - loss: 0.0576 - accuracy: 0.9816 - val_loss: 0.2679 - val_accuracy: 0.9329\n",
      "94/94 [==============================] - 3s 32ms/step - loss: 0.6392 - accuracy: 0.8353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_hog, train_labels,\n",
    "    validation_data=(val_hog, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "test_loss, test_acc = model.evaluate(test_hog, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e4dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DFC on Xception NET: 83.53%\n",
      "94/94 [==============================] - 4s 33ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84      1500\n",
      "           1       0.87      0.79      0.83      1500\n",
      "\n",
      "    accuracy                           0.84      3000\n",
      "   macro avg       0.84      0.84      0.83      3000\n",
      "weighted avg       0.84      0.84      0.83      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Accuracy of DFC on Xception NET: {test_acc * 100:.2f}%')\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions (returns probabilities between 0-1)\n",
    "y_pred_probs = model.predict(test_hog)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_pred_labels = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Ensure test_labels is 1D array (remove extra dimensions if needed)\n",
    "test_labels_flat = np.squeeze(test_labels)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_labels_flat, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6067be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# After training the model\n",
    "model.save('xception_160_dfc.h5')  # Saves the entire model to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0babbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 12.4%\n",
      "RAM Used: 4343.3 MB\n",
      "Time Usage: 843.3 s\n",
      "GPU Memory Used: 829.2 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3381c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "94/94 [==============================] - 6s 35ms/step - loss: 0.7223 - accuracy: 0.8070\n",
      "Test Accuracy of DFC on Xception NET: 80.70%\n",
      "\n",
      "=== RESOURCE USAGE === only testing\n",
      "CPU Usage: 5.4%\n",
      "RAM Used: 3306.9 MB\n",
      "Time Usage: 12.2 s\n",
      "GPU Memory Used: 317.7 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('xception_160_dfc.h5')\n",
    "test_loss, test_acc = model.evaluate(test_hog, test_labels)\n",
    "print(f'Test Accuracy of DFC on Xception NET: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE === only testing\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f9507ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 11s 32ms/step - loss: 1.9827 - accuracy: 0.4907\n",
      "\n",
      "Test Accuracy of Celeb-df dataset on Xceptionnet for DFC: 49.07%\n",
      "340/340 [==============================] - 12s 33ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.55      0.37      2961\n",
      "           1       0.74      0.47      0.57      7891\n",
      "\n",
      "    accuracy                           0.49     10852\n",
      "   macro avg       0.51      0.51      0.47     10852\n",
      "weighted avg       0.61      0.49      0.52     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('xception_160_dfc.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-df dataset on Xceptionnet for DFC: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d36583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 5s 32ms/step - loss: 2.6148 - accuracy: 0.4759\n",
      "\n",
      "Test Accuracy of FF++ on Xceptionnet from DFC: 47.59%\n",
      "169/169 [==============================] - 5s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.62      0.39      1468\n",
      "           1       0.75      0.42      0.54      3911\n",
      "\n",
      "    accuracy                           0.48      5379\n",
      "   macro avg       0.52      0.52      0.47      5379\n",
      "weighted avg       0.62      0.48      0.50      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'\\nTest Accuracy of FF++ on Xceptionnet from DFC: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_ff)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels_ff, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd5e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Shape of the concatenated array: (2808, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (3299, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (1742, 160, 160, 3)\n",
      "Shape of the concatenated array: (1742, 160, 160, 3)\n",
      "Training data ACTOR real shape: (1965, 160, 160, 3) Testing real data shape: (843, 160, 160, 3)\n",
      "Training data Youtube real shape: (1458, 160, 160, 3) Testing real data shape: (625, 160, 160, 3)\n",
      "Training data DFD fake shape: (2309, 160, 160, 3) Testing fake data shape: (990, 160, 160, 3)\n",
      "Training data DF fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data f2f fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data fshifter fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data fswap fake shape: (1219, 160, 160, 3) Testing fake data shape: (523, 160, 160, 3)\n",
      "Training data nt fake shape: (1219, 160, 160, 3) Testing fake data shape: (523, 160, 160, 3)\n",
      "train_ori_actor hog_real: 1768 images, val_ori_actor hog_real: 197 images\n",
      "train_ ori_youtube hog_real: 1312 images, val_ ori_youtube hog_real: 146 images\n",
      "train_hog_mni_dfd_fake: 2078 images, val_hog_mni_dfd_fake: 231 images\n",
      "train_hog_mni_df_fake: 1312 images, val_hog_mni_df_fake: 146 images\n",
      "train_hog_mni_f2f_fake: 1312 images, val_hog_mni_f2f_fake: 146 images\n",
      "train_hog_mni_fshifter_fake: 1312 images, val_hog_mni_fshifter_fake: 146 images\n",
      "train_hog_mni_fswap_fake: 1097 images, val_hog_mni_fswap_fake: 122 images\n",
      "train_hog_mni_nt_fake: 1097 images, val_hog_mni_nt_fake: 122 images\n",
      "Total train: 11288 images, and shape:(11288, 160, 160, 3)\n",
      "Total test: 5379 images, and shape:(5379, 160, 160, 3)\n",
      "Total val: 1256 images, and shape:(1256, 160, 160, 3)\n",
      "Train Labels: 11288 \n",
      "Test Labels: 5379 \n",
      "Val Labels: 1256 \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_orignal_processed.h5\", \"r\") as h5f:\n",
    "    org_seq_actor_array = h5f[\"ori_actor\"][:]\n",
    "    org_seq_youtube_array = h5f[\"ori_youtube\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake1_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_dfd_array = h5f[\"mni_dfd\"][:]\n",
    "    meni_seq_df_array = h5f[\"mni_df\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake2_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_f2f_array = h5f[\"mni_f2f\"][:]\n",
    "    meni_seq_fshifter_array = h5f[\"mni_fshifter\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake3_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_fswap_array = h5f[\"mni_fswap\"][:]\n",
    "    meni_seq_nt_array = h5f[\"mni_nt\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "# Output the shape of the resulting array\n",
    "print(f\"Shape of the concatenated array: {org_seq_actor_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {org_seq_youtube_array.shape}\")\n",
    "\n",
    "print(f\"Shape of the concatenated array: {meni_seq_dfd_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_df_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_fshifter_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_f2f_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_fswap_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_nt_array.shape}\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ori_actor_train_real, ori_actor_test_real = train_test_split(org_seq_actor_array, test_size=0.3, random_state=42)\n",
    "ori_youtube_train_real, ori_youtube_test_real = train_test_split(org_seq_youtube_array, test_size=0.3, random_state=42)\n",
    "# Split the data into train (70%) and test (30%)\n",
    "mni_dfd_train_fake, mni_dfd_test_fake = train_test_split(meni_seq_dfd_array, test_size=0.3, random_state=42)\n",
    "mni_df_train_fake, mni_df_test_fake = train_test_split(meni_seq_df_array, test_size=0.3, random_state=42)\n",
    "mni_f2f_train_fake, mni_f2f_test_fake = train_test_split(meni_seq_f2f_array, test_size=0.3, random_state=42)\n",
    "mni_fshifter_train_fake, mni_fshifter_test_fake = train_test_split(meni_seq_fshifter_array, test_size=0.3, random_state=42)\n",
    "mni_fswap_train_fake, mni_fswap_test_fake = train_test_split(meni_seq_fswap_array, test_size=0.3, random_state=42)\n",
    "mni_nt_train_fake, mni_nt_test_fake = train_test_split(meni_seq_nt_array, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Output the shape to confirm the split dimensions\n",
    "print(\"Training data ACTOR real shape:\", ori_actor_train_real.shape, \"Testing real data shape:\", ori_actor_test_real.shape)\n",
    "print(\"Training data Youtube real shape:\", ori_youtube_train_real.shape, \"Testing real data shape:\", ori_youtube_test_real.shape)\n",
    "\n",
    "print(\"Training data DFD fake shape:\", mni_dfd_train_fake.shape, \"Testing fake data shape:\", mni_dfd_test_fake.shape)\n",
    "print(\"Training data DF fake shape:\", mni_df_train_fake.shape, \"Testing fake data shape:\", mni_df_test_fake.shape)\n",
    "print(\"Training data f2f fake shape:\", mni_f2f_train_fake.shape, \"Testing fake data shape:\", mni_f2f_test_fake.shape)\n",
    "print(\"Training data fshifter fake shape:\", mni_fshifter_train_fake.shape, \"Testing fake data shape:\", mni_fshifter_test_fake.shape)\n",
    "print(\"Training data fswap fake shape:\", mni_fswap_train_fake.shape, \"Testing fake data shape:\", mni_fswap_test_fake.shape)\n",
    "print(\"Training data nt fake shape:\", mni_nt_train_fake.shape, \"Testing fake data shape:\", mni_nt_test_fake.shape)\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "train_hog_ori_actor_real, val_hog_ori_actor_real = extract_validation(ori_actor_train_real)\n",
    "train_hog_ori_youtube_real, val_hog_ori_youtube_real = extract_validation(ori_youtube_train_real)\n",
    "train_hog_mni_dfd_fake, val_hog_mni_dfd_fake = extract_validation(mni_dfd_train_fake)\n",
    "train_hog_mni_df_fake, val_hog_mni_df_fake = extract_validation(mni_df_train_fake)\n",
    "train_hog_mni_f2f_fake, val_hog_mni_f2f_fake = extract_validation(mni_f2f_train_fake)\n",
    "train_hog_mni_fshifter_fake, val_hog_mni_fshifter_fake = extract_validation(mni_fshifter_train_fake)\n",
    "train_hog_mni_fswap_fake, val_hog_mni_fswap_fake = extract_validation(mni_fswap_train_fake)\n",
    "train_hog_mni_nt_fake, val_hog_mni_nt_fake = extract_validation(mni_nt_train_fake)\n",
    "# Print results for verification\n",
    "print(f\"train_ori_actor hog_real: {len(train_hog_ori_actor_real)} images, val_ori_actor hog_real: {len(val_hog_ori_actor_real)} images\")\n",
    "print(f\"train_ ori_youtube hog_real: {len(train_hog_ori_youtube_real)} images, val_ ori_youtube hog_real: {len(val_hog_ori_youtube_real)} images\")\n",
    "print(f\"train_hog_mni_dfd_fake: {len(train_hog_mni_dfd_fake)} images, val_hog_mni_dfd_fake: {len(val_hog_mni_dfd_fake)} images\")\n",
    "print(f\"train_hog_mni_df_fake: {len(train_hog_mni_df_fake)} images, val_hog_mni_df_fake: {len(val_hog_mni_df_fake)} images\")\n",
    "print(f\"train_hog_mni_f2f_fake: {len(train_hog_mni_f2f_fake)} images, val_hog_mni_f2f_fake: {len(val_hog_mni_f2f_fake)} images\")\n",
    "print(f\"train_hog_mni_fshifter_fake: {len(train_hog_mni_fshifter_fake)} images, val_hog_mni_fshifter_fake: {len(val_hog_mni_fshifter_fake)} images\")\n",
    "print(f\"train_hog_mni_fswap_fake: {len(train_hog_mni_fswap_fake)} images, val_hog_mni_fswap_fake: {len(val_hog_mni_fswap_fake)} images\")\n",
    "print(f\"train_hog_mni_nt_fake: {len(train_hog_mni_nt_fake)} images, val_hog_mni_nt_fake: {len(val_hog_mni_nt_fake)} images\")\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "train_ori_actor_labels_real = np.zeros(len(train_hog_ori_actor_real), dtype=int)\n",
    "train_ori_youtube_labels_real = np.zeros(len(train_hog_ori_youtube_real), dtype=int)\n",
    "train_mni_dfd_labels_fake = np.ones(len(train_hog_mni_dfd_fake), dtype=int)\n",
    "train_mni_df_labels_fake = np.ones(len(train_hog_mni_df_fake), dtype=int)\n",
    "train_mni_f2f_labels_fake = np.ones(len(train_hog_mni_f2f_fake), dtype=int)\n",
    "train_mni_fshifter_labels_fake = np.ones(len(train_hog_mni_fshifter_fake), dtype=int)\n",
    "train_mni_fswap_labels_fake = np.ones(len(train_hog_mni_fswap_fake), dtype=int)\n",
    "train_mni_nt_labels_fake = np.ones(len(train_hog_mni_nt_fake), dtype=int)\n",
    "\n",
    "test_ori_actor_labels_real = np.zeros(len(ori_actor_test_real), dtype=int)\n",
    "test_ori_youtube_labels_real = np.zeros(len(ori_youtube_test_real), dtype=int)\n",
    "test_mni_dfd_labels_fake = np.ones(len(mni_dfd_test_fake), dtype=int)\n",
    "test_mni_df_labels_fake = np.ones(len(mni_df_test_fake), dtype=int)\n",
    "test_mni_f2f_labels_fake = np.ones(len(mni_f2f_test_fake), dtype=int)\n",
    "test_mni_fshifter_labels_fake = np.ones(len(mni_fshifter_test_fake), dtype=int)\n",
    "test_mni_fswap_labels_fake = np.ones(len(mni_fswap_test_fake), dtype=int)\n",
    "test_mni_nt_labels_fake = np.ones(len(mni_nt_test_fake), dtype=int)\n",
    "\n",
    "\n",
    "val_ori_actor_labels_real = np.zeros(len(val_hog_ori_actor_real), dtype=int)\n",
    "val_ori_youtube_labels_real = np.zeros(len(val_hog_ori_youtube_real), dtype=int)\n",
    "val_mni_dfd_labels_fake = np.ones(len(val_hog_mni_dfd_fake), dtype=int)\n",
    "val_mni_df_labels_fake = np.ones(len(val_hog_mni_df_fake), dtype=int)\n",
    "val_mni_f2f_labels_fake = np.ones(len(val_hog_mni_f2f_fake), dtype=int)\n",
    "val_mni_fshifter_labels_fake = np.ones(len(val_hog_mni_fshifter_fake), dtype=int)\n",
    "val_mni_fswap_labels_fake = np.ones(len(val_hog_mni_fswap_fake), dtype=int)\n",
    "val_mni_nt_labels_fake = np.ones(len(val_hog_mni_nt_fake), dtype=int)\n",
    "##################################################################################################################\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train_ff = np.concatenate([train_hog_ori_actor_real, train_hog_ori_youtube_real,train_hog_mni_dfd_fake,train_hog_mni_df_fake,\n",
    "                            train_hog_mni_f2f_fake,train_hog_mni_fshifter_fake,train_hog_mni_fswap_fake,train_hog_mni_nt_fake], axis=0)\n",
    "\n",
    "train_labels_ff = np.concatenate([train_ori_actor_labels_real, train_ori_youtube_labels_real,train_mni_dfd_labels_fake,train_mni_df_labels_fake,\n",
    "                            train_mni_f2f_labels_fake,train_mni_fshifter_labels_fake,train_mni_fswap_labels_fake,train_mni_nt_labels_fake], axis=0)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val_ff = np.concatenate([val_hog_ori_actor_real, val_hog_ori_youtube_real, val_hog_mni_dfd_fake, val_hog_mni_df_fake,\n",
    "                            val_hog_mni_f2f_fake, val_hog_mni_fshifter_fake, val_hog_mni_fswap_fake, val_hog_mni_nt_fake], axis=0)\n",
    "val_labels_ff = np.concatenate([val_ori_actor_labels_real, val_ori_youtube_labels_real, val_mni_dfd_labels_fake, val_mni_df_labels_fake,\n",
    "                            val_mni_f2f_labels_fake, val_mni_fshifter_labels_fake, val_mni_fswap_labels_fake, val_mni_nt_labels_fake], axis=0)\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test_ff = np.concatenate([ori_actor_test_real, ori_youtube_test_real, mni_dfd_test_fake,\n",
    "                           mni_df_test_fake, mni_f2f_test_fake, mni_fshifter_test_fake,\n",
    "                           mni_fswap_test_fake, mni_nt_test_fake], axis=0)\n",
    "test_labels_ff = np.concatenate([test_ori_actor_labels_real, test_ori_youtube_labels_real, test_mni_dfd_labels_fake, test_mni_df_labels_fake,\n",
    "                            test_mni_f2f_labels_fake, test_mni_fshifter_labels_fake, test_mni_fswap_labels_fake, test_mni_nt_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "# Print the results for verification\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train_ff)} images, and shape:{train_ff.shape}\")\n",
    "print(f\"Total test: {len(test_ff)} images, and shape:{test_ff.shape}\")\n",
    "print(f\"Total val: {len(val_ff)} images, and shape:{val_ff.shape}\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels_ff)} \")\n",
    "print(f\"Test Labels: {len(test_labels_ff)} \")\n",
    "print(f\"Val Labels: {len(val_labels_ff)} \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1c97a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "353/353 [==============================] - 19s 39ms/step - loss: 1.9686 - accuracy: 0.6395 - val_loss: 0.6430 - val_accuracy: 0.7229\n",
      "Epoch 2/100\n",
      "353/353 [==============================] - 13s 36ms/step - loss: 0.6698 - accuracy: 0.7009 - val_loss: 0.5751 - val_accuracy: 0.7333\n",
      "Epoch 3/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5931 - accuracy: 0.7250 - val_loss: 0.5661 - val_accuracy: 0.7373\n",
      "Epoch 4/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5742 - accuracy: 0.7325 - val_loss: 0.5616 - val_accuracy: 0.7420\n",
      "Epoch 5/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5653 - accuracy: 0.7351 - val_loss: 0.5590 - val_accuracy: 0.7492\n",
      "Epoch 6/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5646 - accuracy: 0.7368 - val_loss: 0.5628 - val_accuracy: 0.7404\n",
      "Epoch 7/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5589 - accuracy: 0.7389 - val_loss: 0.5607 - val_accuracy: 0.7460\n",
      "Epoch 8/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5504 - accuracy: 0.7439 - val_loss: 0.5588 - val_accuracy: 0.7476\n",
      "Epoch 9/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5482 - accuracy: 0.7471 - val_loss: 0.5553 - val_accuracy: 0.7468\n",
      "Epoch 10/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5498 - accuracy: 0.7481 - val_loss: 0.5516 - val_accuracy: 0.7436\n",
      "Epoch 11/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5488 - accuracy: 0.7505 - val_loss: 0.5527 - val_accuracy: 0.7524\n",
      "Epoch 12/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5411 - accuracy: 0.7496 - val_loss: 0.5549 - val_accuracy: 0.7476\n",
      "Epoch 13/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5385 - accuracy: 0.7524 - val_loss: 0.5513 - val_accuracy: 0.7516\n",
      "Epoch 14/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5387 - accuracy: 0.7542 - val_loss: 0.5500 - val_accuracy: 0.7468\n",
      "Epoch 15/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5356 - accuracy: 0.7595 - val_loss: 0.5516 - val_accuracy: 0.7532\n",
      "Epoch 16/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5383 - accuracy: 0.7572 - val_loss: 0.5477 - val_accuracy: 0.7540\n",
      "Epoch 17/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5326 - accuracy: 0.7569 - val_loss: 0.5527 - val_accuracy: 0.7532\n",
      "Epoch 18/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5286 - accuracy: 0.7570 - val_loss: 0.5470 - val_accuracy: 0.7627\n",
      "Epoch 19/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5267 - accuracy: 0.7628 - val_loss: 0.5495 - val_accuracy: 0.7516\n",
      "Epoch 20/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5293 - accuracy: 0.7613 - val_loss: 0.5537 - val_accuracy: 0.7532\n",
      "Epoch 21/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5251 - accuracy: 0.7659 - val_loss: 0.5521 - val_accuracy: 0.7524\n",
      "Epoch 22/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5221 - accuracy: 0.7665 - val_loss: 0.5481 - val_accuracy: 0.7516\n",
      "Epoch 23/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5194 - accuracy: 0.7647 - val_loss: 0.5484 - val_accuracy: 0.7572\n",
      "Epoch 24/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5182 - accuracy: 0.7660 - val_loss: 0.5477 - val_accuracy: 0.7619\n",
      "Epoch 25/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5146 - accuracy: 0.7696 - val_loss: 0.5464 - val_accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5152 - accuracy: 0.7645 - val_loss: 0.5510 - val_accuracy: 0.7564\n",
      "Epoch 27/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5129 - accuracy: 0.7695 - val_loss: 0.5520 - val_accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5107 - accuracy: 0.7750 - val_loss: 0.5480 - val_accuracy: 0.7492\n",
      "Epoch 29/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5055 - accuracy: 0.7756 - val_loss: 0.5468 - val_accuracy: 0.7476\n",
      "Epoch 30/100\n",
      "353/353 [==============================] - 13s 36ms/step - loss: 0.5029 - accuracy: 0.7757 - val_loss: 0.5503 - val_accuracy: 0.7460\n",
      "Epoch 31/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5019 - accuracy: 0.7758 - val_loss: 0.5550 - val_accuracy: 0.7564\n",
      "Epoch 32/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5024 - accuracy: 0.7764 - val_loss: 0.5541 - val_accuracy: 0.7428\n",
      "Epoch 33/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.5022 - accuracy: 0.7741 - val_loss: 0.5476 - val_accuracy: 0.7516\n",
      "Epoch 34/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4991 - accuracy: 0.7764 - val_loss: 0.5502 - val_accuracy: 0.7508\n",
      "Epoch 35/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4953 - accuracy: 0.7791 - val_loss: 0.5496 - val_accuracy: 0.7548\n",
      "Epoch 36/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4938 - accuracy: 0.7807 - val_loss: 0.5528 - val_accuracy: 0.7548\n",
      "Epoch 37/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4911 - accuracy: 0.7852 - val_loss: 0.5696 - val_accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4921 - accuracy: 0.7829 - val_loss: 0.5540 - val_accuracy: 0.7548\n",
      "Epoch 39/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4870 - accuracy: 0.7848 - val_loss: 0.5580 - val_accuracy: 0.7436\n",
      "Epoch 40/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4882 - accuracy: 0.7844 - val_loss: 0.5537 - val_accuracy: 0.7524\n",
      "Epoch 41/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4808 - accuracy: 0.7895 - val_loss: 0.5636 - val_accuracy: 0.7365\n",
      "Epoch 42/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4775 - accuracy: 0.7921 - val_loss: 0.5536 - val_accuracy: 0.7500\n",
      "Epoch 43/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4761 - accuracy: 0.7914 - val_loss: 0.5608 - val_accuracy: 0.7476\n",
      "Epoch 44/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4802 - accuracy: 0.7879 - val_loss: 0.5619 - val_accuracy: 0.7524\n",
      "Epoch 45/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4778 - accuracy: 0.7882 - val_loss: 0.5550 - val_accuracy: 0.7508\n",
      "Epoch 46/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4702 - accuracy: 0.7944 - val_loss: 0.5721 - val_accuracy: 0.7436\n",
      "Epoch 47/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4713 - accuracy: 0.7918 - val_loss: 0.5616 - val_accuracy: 0.7444\n",
      "Epoch 48/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4710 - accuracy: 0.7925 - val_loss: 0.5634 - val_accuracy: 0.7611\n",
      "Epoch 49/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4706 - accuracy: 0.7967 - val_loss: 0.5686 - val_accuracy: 0.7444\n",
      "Epoch 50/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4661 - accuracy: 0.7982 - val_loss: 0.5608 - val_accuracy: 0.7468\n",
      "Epoch 51/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4657 - accuracy: 0.7967 - val_loss: 0.5619 - val_accuracy: 0.7436\n",
      "Epoch 52/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4619 - accuracy: 0.7977 - val_loss: 0.5588 - val_accuracy: 0.7508\n",
      "Epoch 53/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4632 - accuracy: 0.7985 - val_loss: 0.5718 - val_accuracy: 0.7412\n",
      "Epoch 54/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4591 - accuracy: 0.8012 - val_loss: 0.5701 - val_accuracy: 0.7516\n",
      "Epoch 55/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4556 - accuracy: 0.8029 - val_loss: 0.5646 - val_accuracy: 0.7452\n",
      "Epoch 56/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4534 - accuracy: 0.8015 - val_loss: 0.5732 - val_accuracy: 0.7564\n",
      "Epoch 57/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4562 - accuracy: 0.8009 - val_loss: 0.5781 - val_accuracy: 0.7460\n",
      "Epoch 58/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4581 - accuracy: 0.8016 - val_loss: 0.5693 - val_accuracy: 0.7468\n",
      "Epoch 59/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4495 - accuracy: 0.8034 - val_loss: 0.5892 - val_accuracy: 0.7444\n",
      "Epoch 60/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4511 - accuracy: 0.8066 - val_loss: 0.5833 - val_accuracy: 0.7389\n",
      "Epoch 61/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4472 - accuracy: 0.8053 - val_loss: 0.5828 - val_accuracy: 0.7436\n",
      "Epoch 62/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4476 - accuracy: 0.8079 - val_loss: 0.5727 - val_accuracy: 0.7516\n",
      "Epoch 63/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4431 - accuracy: 0.8075 - val_loss: 0.5879 - val_accuracy: 0.7365\n",
      "Epoch 64/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4372 - accuracy: 0.8096 - val_loss: 0.5942 - val_accuracy: 0.7412\n",
      "Epoch 65/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4426 - accuracy: 0.8087 - val_loss: 0.5959 - val_accuracy: 0.7444\n",
      "Epoch 66/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4401 - accuracy: 0.8073 - val_loss: 0.5986 - val_accuracy: 0.7404\n",
      "Epoch 67/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4382 - accuracy: 0.8101 - val_loss: 0.5859 - val_accuracy: 0.7524\n",
      "Epoch 68/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4342 - accuracy: 0.8130 - val_loss: 0.6013 - val_accuracy: 0.7428\n",
      "Epoch 69/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4364 - accuracy: 0.8100 - val_loss: 0.6006 - val_accuracy: 0.7325\n",
      "Epoch 70/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4310 - accuracy: 0.8115 - val_loss: 0.5939 - val_accuracy: 0.7460\n",
      "Epoch 71/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4286 - accuracy: 0.8147 - val_loss: 0.5883 - val_accuracy: 0.7436\n",
      "Epoch 72/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4317 - accuracy: 0.8132 - val_loss: 0.5979 - val_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "353/353 [==============================] - 13s 38ms/step - loss: 0.4318 - accuracy: 0.8106 - val_loss: 0.6010 - val_accuracy: 0.7381\n",
      "Epoch 74/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4274 - accuracy: 0.8139 - val_loss: 0.6122 - val_accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4285 - accuracy: 0.8139 - val_loss: 0.6069 - val_accuracy: 0.7428\n",
      "Epoch 76/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4251 - accuracy: 0.8156 - val_loss: 0.6062 - val_accuracy: 0.7444\n",
      "Epoch 77/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4227 - accuracy: 0.8153 - val_loss: 0.6019 - val_accuracy: 0.7412\n",
      "Epoch 78/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4157 - accuracy: 0.8176 - val_loss: 0.6106 - val_accuracy: 0.7412\n",
      "Epoch 79/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4165 - accuracy: 0.8199 - val_loss: 0.6080 - val_accuracy: 0.7404\n",
      "Epoch 80/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4170 - accuracy: 0.8211 - val_loss: 0.6093 - val_accuracy: 0.7373\n",
      "Epoch 81/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4149 - accuracy: 0.8168 - val_loss: 0.6050 - val_accuracy: 0.7396\n",
      "Epoch 82/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4142 - accuracy: 0.8189 - val_loss: 0.6424 - val_accuracy: 0.7396\n",
      "Epoch 83/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4084 - accuracy: 0.8243 - val_loss: 0.6061 - val_accuracy: 0.7412\n",
      "Epoch 84/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4126 - accuracy: 0.8216 - val_loss: 0.6220 - val_accuracy: 0.7404\n",
      "Epoch 85/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4131 - accuracy: 0.8236 - val_loss: 0.6109 - val_accuracy: 0.7404\n",
      "Epoch 86/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4074 - accuracy: 0.8218 - val_loss: 0.6254 - val_accuracy: 0.7373\n",
      "Epoch 87/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4017 - accuracy: 0.8277 - val_loss: 0.6224 - val_accuracy: 0.7452\n",
      "Epoch 88/100\n",
      "353/353 [==============================] - 13s 38ms/step - loss: 0.4063 - accuracy: 0.8228 - val_loss: 0.6312 - val_accuracy: 0.7404\n",
      "Epoch 89/100\n",
      "353/353 [==============================] - 13s 38ms/step - loss: 0.4016 - accuracy: 0.8280 - val_loss: 0.6308 - val_accuracy: 0.7508\n",
      "Epoch 90/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4024 - accuracy: 0.8255 - val_loss: 0.6410 - val_accuracy: 0.7365\n",
      "Epoch 91/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4001 - accuracy: 0.8306 - val_loss: 0.6380 - val_accuracy: 0.7420\n",
      "Epoch 92/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.4009 - accuracy: 0.8265 - val_loss: 0.6274 - val_accuracy: 0.7436\n",
      "Epoch 93/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3956 - accuracy: 0.8285 - val_loss: 0.6312 - val_accuracy: 0.7325\n",
      "Epoch 94/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3979 - accuracy: 0.8308 - val_loss: 0.6457 - val_accuracy: 0.7420\n",
      "Epoch 95/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3959 - accuracy: 0.8263 - val_loss: 0.6236 - val_accuracy: 0.7381\n",
      "Epoch 96/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3973 - accuracy: 0.8277 - val_loss: 0.6540 - val_accuracy: 0.7412\n",
      "Epoch 97/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3878 - accuracy: 0.8318 - val_loss: 0.6513 - val_accuracy: 0.7412\n",
      "Epoch 98/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3908 - accuracy: 0.8281 - val_loss: 0.6598 - val_accuracy: 0.7285\n",
      "Epoch 99/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3926 - accuracy: 0.8302 - val_loss: 0.6499 - val_accuracy: 0.7404\n",
      "Epoch 100/100\n",
      "353/353 [==============================] - 13s 37ms/step - loss: 0.3829 - accuracy: 0.8360 - val_loss: 0.6653 - val_accuracy: 0.7269\n",
      "169/169 [==============================] - 5s 32ms/step - loss: 0.6684 - accuracy: 0.7377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ff, train_labels_ff,\n",
    "    validation_data=(val_ff, val_labels_ff),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "test_loss, test_acc = model.evaluate(test_ff, test_labels_ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01a8499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of FF++ on Xception NET: 73.77%\n",
      "169/169 [==============================] - 5s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.34      0.41      1468\n",
      "           1       0.78      0.89      0.83      3911\n",
      "\n",
      "    accuracy                           0.74      5379\n",
      "   macro avg       0.66      0.61      0.62      5379\n",
      "weighted avg       0.71      0.74      0.72      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Accuracy of FF++ on Xception NET: {test_acc * 100:.2f}%')\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions (returns probabilities between 0-1)\n",
    "y_pred_probs = model.predict(test_ff)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_pred_labels = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Ensure test_labels is 1D array (remove extra dimensions if needed)\n",
    "test_labels_flat = np.squeeze(test_labels_ff)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_labels_flat, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72f34e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# After training the model\n",
    "model.save('xception_160_ff.h5')  # Saves the entire model to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad515d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 7.2%\n",
      "RAM Used: 2635.0 MB\n",
      "Time Usage: 1462.5 s\n",
      "GPU Memory Used: 490.7 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882588c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "169/169 [==============================] - 9s 34ms/step - loss: 0.6684 - accuracy: 0.7375\n",
      "Test Accuracy of FF on Xception NET: 73.75%\n",
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 6.6%\n",
      "RAM Used: 3263.3 MB\n",
      "Time Usage: 15.3 s\n",
      "GPU Memory Used: 492.0 MB\n",
      "Power Consumption: 135W\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "model = tf.keras.models.load_model('xception_160_ff.h5')\n",
    "test_loss, test_acc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'Test Accuracy of FF on Xception NET: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2cb9eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 12s 34ms/step - loss: 0.7560 - accuracy: 0.7195\n",
      "\n",
      "Test Accuracy of Celeb-df dataset on Xceptionnet for FF++: 71.95%\n",
      "340/340 [==============================] - 11s 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.05      0.08      2961\n",
      "           1       0.73      0.97      0.83      7891\n",
      "\n",
      "    accuracy                           0.72     10852\n",
      "   macro avg       0.56      0.51      0.46     10852\n",
      "weighted avg       0.64      0.72      0.63     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('xception_160_ff.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-df dataset on Xceptionnet for FF++: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4819fc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 34ms/step - loss: 0.9595 - accuracy: 0.5447\n",
      "\n",
      "Test Accuracy of DFC on Xceptionnet from FF++: 54.47%\n",
      "94/94 [==============================] - 3s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.29      0.39      1500\n",
      "           1       0.53      0.80      0.64      1500\n",
      "\n",
      "    accuracy                           0.54      3000\n",
      "   macro avg       0.56      0.54      0.51      3000\n",
      "weighted avg       0.56      0.54      0.51      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_hog, test_labels)\n",
    "print(f'\\nTest Accuracy of DFC on Xceptionnet from FF++: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_hog)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
