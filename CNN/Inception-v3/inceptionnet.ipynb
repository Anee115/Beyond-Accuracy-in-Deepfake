{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a643551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "\n",
    "class PowerMonitor:\n",
    "    def __init__(self):\n",
    "        self.gpu_available = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        # Hardware power specifications (adjust these values for your system)\n",
    "        self.cpu_tdp = 65    # Typical TDP for desktop CPUs in watts\n",
    "        self.gpu_tdp = 250   # Typical TDP for desktop GPUs in watts\n",
    "        \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get system stats with power estimation\"\"\"\n",
    "        stats = {\n",
    "            'timestamp': time.time(),\n",
    "            'cpu_%': psutil.cpu_percent(interval=0.1),\n",
    "            'ram_mb': psutil.virtual_memory().used / (1024**2),\n",
    "            'gpu_mem_mb': 0,\n",
    "            'power_w': self.cpu_tdp * (psutil.cpu_percent()/100) * 0.85  # Base CPU power\n",
    "        }\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            try:\n",
    "                # TensorFlow GPU memory monitoring\n",
    "                mem_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                stats.update({\n",
    "                    'gpu_mem_mb': mem_info['current'] / (1024**2),\n",
    "                    'power_w': self.cpu_tdp * (psutil.cpu_percent()/100) * 0.85 + \n",
    "                              self.gpu_tdp * 0.5 * 0.75  # Add GPU power estimate\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return stats\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = PowerMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a293c1",
   "metadata": {},
   "source": [
    "# Celeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63683390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9870, 160, 160, 3), (11274, 160, 160, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import h5py\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "#with h5py.File(\"D:\\\\thesis\\dataset\\deepfake dataset\\images_celeb_224R_processed.h5\", \"r\") as h5f:\n",
    "with h5py.File(\"D:\\\\thesis\\dataset\\Celeb-Df-v2\\images_celeb_balanced_224R_processed.h5\", \"r\") as h5f:\n",
    "    # Load HOG features\n",
    "    real_frames_array1 = h5f[\"ori_actor\"][:]\n",
    "    fake_frames_array1 = h5f[\"ori_youtube\"][:]\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Function to resize images from (224, 224) to (160, 160)\n",
    "def resize_images(image_array, target_size=(160, 160)):\n",
    "    resized_images = np.array([cv2.resize(img, target_size) for img in image_array])\n",
    "    return resized_images\n",
    "\n",
    "\n",
    "# Resize the images\n",
    "real_frames_array1 = resize_images(real_frames_array1, target_size=(160, 160))\n",
    "fake_frames_array1 = resize_images(fake_frames_array1, target_size=(160, 160))\n",
    "\n",
    "# Checking the new shapes\n",
    "real_frames_array1.shape, fake_frames_array1.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed4a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data real shape: (6909, 160, 160, 3)\n",
      "Testing real data shape: (2961, 160, 160, 3)\n",
      "Training fake data shape: (7891, 160, 160, 3)\n",
      "Testing  fake data shape: (3383, 160, 160, 3)\n",
      "train_hog_real: 6218 images, val_hog_real: 691 images\n",
      "train_hog_fake: 7101 images, val_hog_fake: 790 images\n",
      "Total train: 13319 images\n",
      "Total test: 10852 images\n",
      "Total val: 1481 images\n",
      "Train Labels: 13319 \n",
      "Test Labels: 10852 \n",
      "Val Labels: 1481 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into train (70%) and test (30%)\n",
    "X_train_real, X_test_real = train_test_split(real_frames_array1, test_size=0.3, random_state=42)\n",
    "X_train_fake, X_test_fake = train_test_split(fake_frames_array1, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Output the shape to confirm the split dimensions\n",
    "print(\"Training data real shape:\", X_train_real.shape)\n",
    "print(\"Testing real data shape:\", X_test_real.shape)\n",
    "print(\"Training fake data shape:\", X_train_fake.shape)\n",
    "print(\"Testing  fake data shape:\", X_test_fake.shape)\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "train_hog_real, val_hog_real = extract_validation(X_train_real)\n",
    "train_hog_fake, val_hog_fake = extract_validation(X_train_fake)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"train_hog_real: {len(train_hog_real)} images, val_hog_real: {len(val_hog_real)} images\")\n",
    "print(f\"train_hog_fake: {len(train_hog_fake)} images, val_hog_fake: {len(val_hog_fake)} images\")\n",
    "\n",
    "\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "train_labels_real = np.zeros(len(train_hog_real), dtype=int)\n",
    "train_labels_fake = np.ones(len(train_hog_fake), dtype=int)\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train = np.concatenate([train_hog_real, train_hog_fake], axis=0)\n",
    "train_labels=np.concatenate([train_labels_real, train_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "test_labels_real = np.zeros(len(X_test_real), dtype=int)\n",
    "test_labels_fake = np.ones(len(X_train_fake), dtype=int)\n",
    "\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test = np.concatenate([X_test_real, X_train_fake], axis=0)\n",
    "test_labels = np.concatenate([test_labels_real, test_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "val_labels_real = np.zeros(len(val_hog_real), dtype=int)\n",
    "val_labels_fake = np.ones(len(val_hog_fake), dtype=int)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val = np.concatenate([val_hog_real, val_hog_fake], axis=0)\n",
    "val_labels = np.concatenate([val_labels_real, val_labels_fake], axis=0)\n",
    "\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train)} images\")\n",
    "print(f\"Total test: {len(test)} images\")\n",
    "print(f\"Total val: {len(val)} images\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels)} \")\n",
    "print(f\"Test Labels: {len(test_labels)} \")\n",
    "print(f\"Val Labels: {len(val_labels)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3add8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d4cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 79, 79, 32)   864         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 79, 79, 32)  96          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 79, 79, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 77, 77, 32)   9216        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 77, 77, 32)  96          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 77, 77, 32)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 77, 77, 64)   18432       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 77, 77, 64)  192         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 77, 77, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 38, 38, 64)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 38, 38, 80)   5120        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 38, 38, 80)  240         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 38, 38, 80)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 36, 36, 192)  138240      ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 36, 36, 192)  576        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 36, 36, 192)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 17, 17, 192)  0          ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 17, 17, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 17, 17, 64)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 17, 17, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 17, 17, 96)   55296       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 17, 17, 48)  144         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 17, 17, 96)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 17, 17, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 17, 17, 96)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 17, 17, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 17, 17, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 17, 17, 64)   76800       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 17, 17, 96)   82944       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 17, 17, 32)   6144        ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 17, 17, 64)  192         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 17, 17, 64)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 17, 17, 96)  288         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 17, 17, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 17, 17, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 17, 17, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 17, 17, 256)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_7[0][0]',           \n",
      "                                                                  'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 17, 17, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 17, 17, 64)  192         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 17, 17, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 17, 17, 96)   55296       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 17, 17, 48)  144         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 17, 17, 96)  288         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 17, 17, 48)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 17, 17, 256)  0          ['mixed0[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 17, 17, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 17, 17, 64)   76800       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 17, 17, 96)   82944       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 17, 17, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 17, 17, 64)  192         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 17, 17, 64)  192         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 17, 17, 96)  288         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 17, 17, 64)  192         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 17, 17, 288)  0           ['activation_12[0][0]',          \n",
      "                                                                  'activation_14[0][0]',          \n",
      "                                                                  'activation_17[0][0]',          \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 17, 17, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 17, 17, 64)  192         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 17, 17, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 17, 17, 96)   55296       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 17, 17, 48)  144         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 17, 17, 96)  288         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 17, 17, 48)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 17, 17, 288)  0          ['mixed1[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 17, 17, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 17, 17, 64)   76800       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 17, 17, 96)   82944       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 17, 17, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 17, 17, 64)  192         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 17, 17, 64)  192         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 17, 17, 96)  288         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 17, 17, 64)  192         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 17, 17, 288)  0           ['activation_19[0][0]',          \n",
      "                                                                  'activation_21[0][0]',          \n",
      "                                                                  'activation_24[0][0]',          \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 17, 17, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 17, 17, 64)  192         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 17, 17, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 17, 17, 96)   55296       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 17, 17, 96)  288         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 384)    995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 96)     82944       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 96)    288         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 96)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 288)   0           ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 8, 8, 768)    0           ['activation_26[0][0]',          \n",
      "                                                                  'activation_29[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 128)    98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 128)   384         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 128)    114688      ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 128)   384         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 128)    98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 128)    114688      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 128)   384         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 128)   384         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 128)    114688      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 128)    114688      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 128)   384         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 128)   384         ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 8, 8, 768)   0           ['mixed3[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 192)    172032      ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 192)    172032      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 192)    147456      ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 192)   576         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 192)   576         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 192)   576         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 8, 8, 192)   576         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 8, 8, 768)    0           ['activation_30[0][0]',          \n",
      "                                                                  'activation_33[0][0]',          \n",
      "                                                                  'activation_38[0][0]',          \n",
      "                                                                  'activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 160)    122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 160)   480         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 160)   480         ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 160)    122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 160)   480         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 160)   480         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 160)   480         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 160)   480         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 8, 8, 768)   0           ['mixed4[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 192)    215040      ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 8, 8, 192)    215040      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 8, 8, 192)    147456      ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 192)   576         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 192)   576         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 8, 8, 192)   576         ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 8, 8, 192)   576         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 8, 8, 768)    0           ['activation_40[0][0]',          \n",
      "                                                                  'activation_43[0][0]',          \n",
      "                                                                  'activation_48[0][0]',          \n",
      "                                                                  'activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 160)    122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 160)   480         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 160)   480         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 8, 8, 160)    122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 160)   480         ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 160)   480         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 160)    179200      ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 160)   480         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 8, 8, 160)   480         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 8, 8, 768)   0           ['mixed5[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 192)    215040      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 192)    215040      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 192)    147456      ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 192)   576         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 192)   576         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 8, 8, 192)   576         ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 8, 8, 192)   576         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 8, 8, 768)    0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'activation_58[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 8, 8, 192)   576         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 8, 8, 192)   576         ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 8, 8, 192)   576         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 8, 8, 192)   576         ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 8, 8, 192)   576         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 8, 8, 192)   576         ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 8, 8, 768)   0           ['mixed6[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 8, 8, 192)    147456      ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 8, 8, 192)   576         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 8, 8, 192)   576         ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 8, 8, 192)   576         ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 8, 8, 192)   576         ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 8, 8, 768)    0           ['activation_60[0][0]',          \n",
      "                                                                  'activation_63[0][0]',          \n",
      "                                                                  'activation_68[0][0]',          \n",
      "                                                                  'activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 192)   576         ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 192)   576         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 8, 8, 192)    147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 8, 8, 192)    258048      ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 192)   576         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 192)   576         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 3, 3, 320)    552960      ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 3, 3, 192)    331776      ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 3, 3, 320)   960         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 3, 3, 192)   576         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 3, 3, 1280)   0           ['activation_71[0][0]',          \n",
      "                                                                  'activation_75[0][0]',          \n",
      "                                                                  'max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 3, 3, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 3, 3, 448)   1344        ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 3, 3, 448)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 3, 3, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 3, 3, 384)    1548288     ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 3, 3, 1280)  0           ['mixed8[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 3, 3, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 3, 3, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 3, 3, 320)   960         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 3, 3, 192)   576         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 3, 3, 768)    0           ['activation_78[0][0]',          \n",
      "                                                                  'activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3, 3, 768)    0           ['activation_82[0][0]',          \n",
      "                                                                  'activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 3, 3, 2048)   0           ['activation_76[0][0]',          \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate[0][0]',            \n",
      "                                                                  'activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 3, 3, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 3, 3, 448)   1344        ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 3, 3, 448)    0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 3, 3, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 3, 3, 384)    1548288     ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 3, 3, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 3, 3, 2048)  0           ['mixed9[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 3, 3, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 3, 3, 384)   1152        ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 3, 3, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 3, 3, 320)   960         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 3, 3, 384)    0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 3, 3, 192)   576         ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 3, 3, 320)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 3, 3, 768)    0           ['activation_87[0][0]',          \n",
      "                                                                  'activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 3, 768)    0           ['activation_91[0][0]',          \n",
      "                                                                  'activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 3, 3, 192)    0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 3, 3, 2048)   0           ['activation_85[0][0]',          \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_1[0][0]',          \n",
      "                                                                  'activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,901,985\n",
      "Trainable params: 21,725,569\n",
      "Non-trainable params: 2,176,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, callbacks\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# Configuration\n",
    "INPUT_SHAPE = (160, 160, 3)\n",
    "NUM_CLASSES = 1  # Binary classification (real=0, fake=1)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# Load pre-trained InceptionV3 (excluding top layers)\n",
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=INPUT_SHAPE\n",
    ")\n",
    "\n",
    "# Freeze initial layers (optional)\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Custom Head for Deepfake Detection\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "\n",
    "# Final Model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile with class weights (adjust if dataset is imbalanced)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c2e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "417/417 [==============================] - 38s 66ms/step - loss: 0.6351 - accuracy: 0.6425 - auc: 0.6927 - val_loss: 0.6737 - val_accuracy: 0.6617 - val_auc: 0.7195\n",
      "Epoch 2/100\n",
      "417/417 [==============================] - 27s 64ms/step - loss: 0.5182 - accuracy: 0.7408 - auc: 0.8192 - val_loss: 0.5273 - val_accuracy: 0.7292 - val_auc: 0.8302\n",
      "Epoch 3/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.4299 - accuracy: 0.7983 - auc: 0.8821 - val_loss: 0.4737 - val_accuracy: 0.7650 - val_auc: 0.8649\n",
      "Epoch 4/100\n",
      "417/417 [==============================] - 27s 64ms/step - loss: 0.3601 - accuracy: 0.8378 - auc: 0.9193 - val_loss: 0.4900 - val_accuracy: 0.7738 - val_auc: 0.8684\n",
      "Epoch 5/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.3153 - accuracy: 0.8639 - auc: 0.9386 - val_loss: 0.4218 - val_accuracy: 0.8136 - val_auc: 0.9040\n",
      "Epoch 6/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.2722 - accuracy: 0.8842 - auc: 0.9550 - val_loss: 0.4063 - val_accuracy: 0.8184 - val_auc: 0.9094\n",
      "Epoch 7/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.2440 - accuracy: 0.8977 - auc: 0.9636 - val_loss: 0.4780 - val_accuracy: 0.8008 - val_auc: 0.9143\n",
      "Epoch 8/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.2112 - accuracy: 0.9151 - auc: 0.9725 - val_loss: 0.4870 - val_accuracy: 0.8001 - val_auc: 0.9065\n",
      "Epoch 9/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.1905 - accuracy: 0.9234 - auc: 0.9776 - val_loss: 0.4094 - val_accuracy: 0.8427 - val_auc: 0.9229\n",
      "Epoch 10/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.1747 - accuracy: 0.9284 - auc: 0.9810 - val_loss: 0.3965 - val_accuracy: 0.8400 - val_auc: 0.9183\n",
      "Epoch 11/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.1560 - accuracy: 0.9375 - auc: 0.9848 - val_loss: 0.4585 - val_accuracy: 0.8373 - val_auc: 0.9126\n",
      "Epoch 12/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.1502 - accuracy: 0.9431 - auc: 0.9858 - val_loss: 0.3756 - val_accuracy: 0.8413 - val_auc: 0.9194\n",
      "Epoch 13/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.1272 - accuracy: 0.9525 - auc: 0.9895 - val_loss: 0.4069 - val_accuracy: 0.8697 - val_auc: 0.9414\n",
      "Epoch 14/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.1206 - accuracy: 0.9544 - auc: 0.9902 - val_loss: 0.8234 - val_accuracy: 0.7731 - val_auc: 0.9013\n",
      "Epoch 15/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.1191 - accuracy: 0.9555 - auc: 0.9907 - val_loss: 0.3757 - val_accuracy: 0.8710 - val_auc: 0.9436\n",
      "Epoch 16/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.1062 - accuracy: 0.9594 - auc: 0.9925 - val_loss: 0.5793 - val_accuracy: 0.8231 - val_auc: 0.9209\n",
      "Epoch 17/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.1128 - accuracy: 0.9587 - auc: 0.9917 - val_loss: 0.3328 - val_accuracy: 0.8785 - val_auc: 0.9458\n",
      "Epoch 18/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0994 - accuracy: 0.9640 - auc: 0.9932 - val_loss: 0.3323 - val_accuracy: 0.8879 - val_auc: 0.9492\n",
      "Epoch 19/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0850 - accuracy: 0.9701 - auc: 0.9946 - val_loss: 0.4375 - val_accuracy: 0.8697 - val_auc: 0.9390\n",
      "Epoch 20/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0916 - accuracy: 0.9690 - auc: 0.9939 - val_loss: 0.3389 - val_accuracy: 0.8852 - val_auc: 0.9512\n",
      "Epoch 21/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0822 - accuracy: 0.9696 - auc: 0.9953 - val_loss: 0.3504 - val_accuracy: 0.8872 - val_auc: 0.9508\n",
      "Epoch 22/100\n",
      "417/417 [==============================] - 25s 61ms/step - loss: 0.0793 - accuracy: 0.9713 - auc: 0.9953 - val_loss: 0.4878 - val_accuracy: 0.8629 - val_auc: 0.9339\n",
      "Epoch 23/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0781 - accuracy: 0.9739 - auc: 0.9953 - val_loss: 0.3407 - val_accuracy: 0.8805 - val_auc: 0.9535\n",
      "Epoch 24/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0777 - accuracy: 0.9731 - auc: 0.9956 - val_loss: 0.3901 - val_accuracy: 0.8650 - val_auc: 0.9373\n",
      "Epoch 25/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0627 - accuracy: 0.9782 - auc: 0.9967 - val_loss: 0.4133 - val_accuracy: 0.8771 - val_auc: 0.9408\n",
      "Epoch 26/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0656 - accuracy: 0.9770 - auc: 0.9963 - val_loss: 0.4720 - val_accuracy: 0.8623 - val_auc: 0.9331\n",
      "Epoch 27/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0733 - accuracy: 0.9742 - auc: 0.9957 - val_loss: 0.3567 - val_accuracy: 0.8812 - val_auc: 0.9463\n",
      "Epoch 28/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0672 - accuracy: 0.9782 - auc: 0.9965 - val_loss: 0.3951 - val_accuracy: 0.8812 - val_auc: 0.9465\n",
      "Epoch 29/100\n",
      "417/417 [==============================] - 25s 61ms/step - loss: 0.0642 - accuracy: 0.9779 - auc: 0.9964 - val_loss: 0.3715 - val_accuracy: 0.8926 - val_auc: 0.9497\n",
      "Epoch 30/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0528 - accuracy: 0.9827 - auc: 0.9976 - val_loss: 0.3933 - val_accuracy: 0.8933 - val_auc: 0.9523\n",
      "Epoch 31/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0604 - accuracy: 0.9806 - auc: 0.9965 - val_loss: 0.3831 - val_accuracy: 0.8825 - val_auc: 0.9526\n",
      "Epoch 32/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0643 - accuracy: 0.9803 - auc: 0.9963 - val_loss: 0.3831 - val_accuracy: 0.8589 - val_auc: 0.9479\n",
      "Epoch 33/100\n",
      "417/417 [==============================] - 25s 61ms/step - loss: 0.0597 - accuracy: 0.9788 - auc: 0.9972 - val_loss: 0.6079 - val_accuracy: 0.8542 - val_auc: 0.9264\n",
      "Epoch 34/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0545 - accuracy: 0.9829 - auc: 0.9971 - val_loss: 0.3877 - val_accuracy: 0.8845 - val_auc: 0.9464\n",
      "Epoch 35/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0430 - accuracy: 0.9866 - auc: 0.9981 - val_loss: 0.4593 - val_accuracy: 0.8724 - val_auc: 0.9424\n",
      "Epoch 36/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0468 - accuracy: 0.9857 - auc: 0.9977 - val_loss: 0.4411 - val_accuracy: 0.8710 - val_auc: 0.9372\n",
      "Epoch 37/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0589 - accuracy: 0.9812 - auc: 0.9968 - val_loss: 0.4650 - val_accuracy: 0.8589 - val_auc: 0.9353\n",
      "Epoch 38/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0494 - accuracy: 0.9850 - auc: 0.9972 - val_loss: 0.4734 - val_accuracy: 0.8751 - val_auc: 0.9396\n",
      "Epoch 39/100\n",
      "417/417 [==============================] - 28s 67ms/step - loss: 0.0449 - accuracy: 0.9854 - auc: 0.9981 - val_loss: 0.3838 - val_accuracy: 0.8947 - val_auc: 0.9506\n",
      "Epoch 40/100\n",
      "417/417 [==============================] - 33s 79ms/step - loss: 0.0460 - accuracy: 0.9853 - auc: 0.9980 - val_loss: 0.4222 - val_accuracy: 0.8940 - val_auc: 0.9460\n",
      "Epoch 41/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0492 - accuracy: 0.9839 - auc: 0.9977 - val_loss: 0.4134 - val_accuracy: 0.8812 - val_auc: 0.9470\n",
      "Epoch 42/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0458 - accuracy: 0.9860 - auc: 0.9978 - val_loss: 0.3594 - val_accuracy: 0.9007 - val_auc: 0.9524\n",
      "Epoch 43/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0422 - accuracy: 0.9871 - auc: 0.9981 - val_loss: 0.3890 - val_accuracy: 0.8933 - val_auc: 0.9473\n",
      "Epoch 44/100\n",
      "417/417 [==============================] - 25s 61ms/step - loss: 0.0373 - accuracy: 0.9890 - auc: 0.9982 - val_loss: 0.3546 - val_accuracy: 0.9122 - val_auc: 0.9537\n",
      "Epoch 45/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0384 - accuracy: 0.9894 - auc: 0.9978 - val_loss: 0.3803 - val_accuracy: 0.8798 - val_auc: 0.9505\n",
      "Epoch 46/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0397 - accuracy: 0.9881 - auc: 0.9980 - val_loss: 0.3444 - val_accuracy: 0.8947 - val_auc: 0.9504\n",
      "Epoch 47/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0387 - accuracy: 0.9891 - auc: 0.9980 - val_loss: 0.3403 - val_accuracy: 0.9007 - val_auc: 0.9576\n",
      "Epoch 48/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0448 - accuracy: 0.9867 - auc: 0.9975 - val_loss: 0.4044 - val_accuracy: 0.8866 - val_auc: 0.9432\n",
      "Epoch 49/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0411 - accuracy: 0.9877 - auc: 0.9977 - val_loss: 0.4238 - val_accuracy: 0.8920 - val_auc: 0.9444\n",
      "Epoch 50/100\n",
      "417/417 [==============================] - 45s 108ms/step - loss: 0.0390 - accuracy: 0.9879 - auc: 0.9982 - val_loss: 0.5018 - val_accuracy: 0.8433 - val_auc: 0.9436\n",
      "Epoch 51/100\n",
      "417/417 [==============================] - 72s 173ms/step - loss: 0.0424 - accuracy: 0.9861 - auc: 0.9982 - val_loss: 0.3817 - val_accuracy: 0.9007 - val_auc: 0.9507\n",
      "Epoch 52/100\n",
      "417/417 [==============================] - 80s 191ms/step - loss: 0.0461 - accuracy: 0.9848 - auc: 0.9977 - val_loss: 0.3468 - val_accuracy: 0.8926 - val_auc: 0.9546\n",
      "Epoch 53/100\n",
      "417/417 [==============================] - 54s 130ms/step - loss: 0.0308 - accuracy: 0.9903 - auc: 0.9987 - val_loss: 0.4047 - val_accuracy: 0.8913 - val_auc: 0.9540\n",
      "Epoch 54/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0325 - accuracy: 0.9902 - auc: 0.9986 - val_loss: 0.4045 - val_accuracy: 0.9001 - val_auc: 0.9478\n",
      "Epoch 55/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.0350 - accuracy: 0.9905 - auc: 0.9984 - val_loss: 0.4172 - val_accuracy: 0.8859 - val_auc: 0.9391\n",
      "Epoch 56/100\n",
      "417/417 [==============================] - 61s 147ms/step - loss: 0.0474 - accuracy: 0.9855 - auc: 0.9977 - val_loss: 0.3353 - val_accuracy: 0.9095 - val_auc: 0.9552\n",
      "Epoch 57/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0398 - accuracy: 0.9879 - auc: 0.9981 - val_loss: 0.3555 - val_accuracy: 0.9122 - val_auc: 0.9564\n",
      "Epoch 58/100\n",
      "417/417 [==============================] - 50s 121ms/step - loss: 0.0348 - accuracy: 0.9896 - auc: 0.9984 - val_loss: 0.3672 - val_accuracy: 0.8866 - val_auc: 0.9491\n",
      "Epoch 59/100\n",
      "417/417 [==============================] - 67s 161ms/step - loss: 0.0283 - accuracy: 0.9911 - auc: 0.9987 - val_loss: 0.4147 - val_accuracy: 0.8940 - val_auc: 0.9446\n",
      "Epoch 60/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0304 - accuracy: 0.9903 - auc: 0.9985 - val_loss: 0.7194 - val_accuracy: 0.8467 - val_auc: 0.9186\n",
      "Epoch 61/100\n",
      "417/417 [==============================] - 27s 64ms/step - loss: 0.0354 - accuracy: 0.9898 - auc: 0.9980 - val_loss: 0.3491 - val_accuracy: 0.8967 - val_auc: 0.9524\n",
      "Epoch 62/100\n",
      "417/417 [==============================] - 27s 64ms/step - loss: 0.0411 - accuracy: 0.9867 - auc: 0.9982 - val_loss: 0.4237 - val_accuracy: 0.8859 - val_auc: 0.9476\n",
      "Epoch 63/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.0332 - accuracy: 0.9912 - auc: 0.9985 - val_loss: 0.4404 - val_accuracy: 0.8839 - val_auc: 0.9442\n",
      "Epoch 64/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0287 - accuracy: 0.9914 - auc: 0.9988 - val_loss: 0.3372 - val_accuracy: 0.8994 - val_auc: 0.9588\n",
      "Epoch 65/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0285 - accuracy: 0.9910 - auc: 0.9986 - val_loss: 0.4890 - val_accuracy: 0.8818 - val_auc: 0.9457\n",
      "Epoch 66/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.0393 - accuracy: 0.9882 - auc: 0.9979 - val_loss: 0.3724 - val_accuracy: 0.8940 - val_auc: 0.9539\n",
      "Epoch 67/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.0350 - accuracy: 0.9888 - auc: 0.9984 - val_loss: 0.4397 - val_accuracy: 0.8893 - val_auc: 0.9480\n",
      "Epoch 68/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0315 - accuracy: 0.9914 - auc: 0.9984 - val_loss: 0.3396 - val_accuracy: 0.8987 - val_auc: 0.9570\n",
      "Epoch 69/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0299 - accuracy: 0.9909 - auc: 0.9987 - val_loss: 0.3413 - val_accuracy: 0.9041 - val_auc: 0.9551\n",
      "Epoch 70/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0288 - accuracy: 0.9917 - auc: 0.9987 - val_loss: 0.4055 - val_accuracy: 0.8866 - val_auc: 0.9495\n",
      "Epoch 71/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0281 - accuracy: 0.9915 - auc: 0.9988 - val_loss: 0.4085 - val_accuracy: 0.9021 - val_auc: 0.9452\n",
      "Epoch 72/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0302 - accuracy: 0.9912 - auc: 0.9985 - val_loss: 0.5452 - val_accuracy: 0.8555 - val_auc: 0.9374\n",
      "Epoch 73/100\n",
      "417/417 [==============================] - 30s 73ms/step - loss: 0.0344 - accuracy: 0.9893 - auc: 0.9986 - val_loss: 0.4160 - val_accuracy: 0.8926 - val_auc: 0.9492\n",
      "Epoch 74/100\n",
      "417/417 [==============================] - 67s 160ms/step - loss: 0.0251 - accuracy: 0.9920 - auc: 0.9988 - val_loss: 0.3919 - val_accuracy: 0.9028 - val_auc: 0.9513\n",
      "Epoch 75/100\n",
      "417/417 [==============================] - 67s 161ms/step - loss: 0.0293 - accuracy: 0.9923 - auc: 0.9982 - val_loss: 0.4148 - val_accuracy: 0.8872 - val_auc: 0.9434\n",
      "Epoch 76/100\n",
      "417/417 [==============================] - 47s 112ms/step - loss: 0.0238 - accuracy: 0.9932 - auc: 0.9990 - val_loss: 0.3772 - val_accuracy: 0.9048 - val_auc: 0.9527\n",
      "Epoch 77/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0236 - accuracy: 0.9927 - auc: 0.9990 - val_loss: 0.4333 - val_accuracy: 0.8825 - val_auc: 0.9443\n",
      "Epoch 78/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0397 - accuracy: 0.9870 - auc: 0.9978 - val_loss: 0.3896 - val_accuracy: 0.9028 - val_auc: 0.9515\n",
      "Epoch 79/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0282 - accuracy: 0.9917 - auc: 0.9986 - val_loss: 0.3873 - val_accuracy: 0.9041 - val_auc: 0.9530\n",
      "Epoch 80/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0240 - accuracy: 0.9927 - auc: 0.9991 - val_loss: 0.4246 - val_accuracy: 0.8953 - val_auc: 0.9458\n",
      "Epoch 81/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0253 - accuracy: 0.9932 - auc: 0.9987 - val_loss: 0.3837 - val_accuracy: 0.8940 - val_auc: 0.9513\n",
      "Epoch 82/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0273 - accuracy: 0.9925 - auc: 0.9985 - val_loss: 0.3847 - val_accuracy: 0.8974 - val_auc: 0.9500\n",
      "Epoch 83/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0281 - accuracy: 0.9920 - auc: 0.9984 - val_loss: 0.4200 - val_accuracy: 0.8940 - val_auc: 0.9471\n",
      "Epoch 84/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0188 - accuracy: 0.9950 - auc: 0.9993 - val_loss: 0.3659 - val_accuracy: 0.9068 - val_auc: 0.9567\n",
      "Epoch 85/100\n",
      "417/417 [==============================] - 26s 63ms/step - loss: 0.0365 - accuracy: 0.9893 - auc: 0.9981 - val_loss: 0.3901 - val_accuracy: 0.9007 - val_auc: 0.9564\n",
      "Epoch 86/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0251 - accuracy: 0.9929 - auc: 0.9986 - val_loss: 0.3788 - val_accuracy: 0.8947 - val_auc: 0.9556\n",
      "Epoch 87/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0217 - accuracy: 0.9939 - auc: 0.9989 - val_loss: 0.4448 - val_accuracy: 0.8960 - val_auc: 0.9493\n",
      "Epoch 88/100\n",
      "417/417 [==============================] - 33s 78ms/step - loss: 0.0317 - accuracy: 0.9912 - auc: 0.9983 - val_loss: 0.3958 - val_accuracy: 0.9034 - val_auc: 0.9474\n",
      "Epoch 89/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0247 - accuracy: 0.9932 - auc: 0.9989 - val_loss: 0.5344 - val_accuracy: 0.8710 - val_auc: 0.9490\n",
      "Epoch 90/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0184 - accuracy: 0.9949 - auc: 0.9992 - val_loss: 0.4143 - val_accuracy: 0.8987 - val_auc: 0.9549\n",
      "Epoch 91/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0335 - accuracy: 0.9907 - auc: 0.9983 - val_loss: 0.3496 - val_accuracy: 0.9095 - val_auc: 0.9542\n",
      "Epoch 92/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0306 - accuracy: 0.9909 - auc: 0.9984 - val_loss: 0.3900 - val_accuracy: 0.8994 - val_auc: 0.9527\n",
      "Epoch 93/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0210 - accuracy: 0.9942 - auc: 0.9988 - val_loss: 0.3299 - val_accuracy: 0.9007 - val_auc: 0.9607\n",
      "Epoch 94/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0335 - accuracy: 0.9904 - auc: 0.9986 - val_loss: 0.5259 - val_accuracy: 0.8663 - val_auc: 0.9494\n",
      "Epoch 95/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0238 - accuracy: 0.9932 - auc: 0.9989 - val_loss: 0.3919 - val_accuracy: 0.9028 - val_auc: 0.9567\n",
      "Epoch 96/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0181 - accuracy: 0.9949 - auc: 0.9993 - val_loss: 0.4102 - val_accuracy: 0.9061 - val_auc: 0.9556\n",
      "Epoch 97/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0213 - accuracy: 0.9944 - auc: 0.9989 - val_loss: 0.4843 - val_accuracy: 0.8859 - val_auc: 0.9413\n",
      "Epoch 98/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0177 - accuracy: 0.9946 - auc: 0.9993 - val_loss: 0.4009 - val_accuracy: 0.8980 - val_auc: 0.9521\n",
      "Epoch 99/100\n",
      "417/417 [==============================] - 26s 61ms/step - loss: 0.0160 - accuracy: 0.9959 - auc: 0.9992 - val_loss: 0.4818 - val_accuracy: 0.8906 - val_auc: 0.9503\n",
      "Epoch 100/100\n",
      "417/417 [==============================] - 26s 62ms/step - loss: 0.0204 - accuracy: 0.9947 - auc: 0.9989 - val_loss: 0.3903 - val_accuracy: 0.9048 - val_auc: 0.9540\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    train, train_labels,\n",
    "    validation_data=(val, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672d3846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "417/417 [==============================] - 44s 80ms/step - loss: 0.6417 - accuracy: 0.6273 - auc: 0.6796 - val_loss: 0.5922 - val_accuracy: 0.6941 - val_auc: 0.7631\n",
      "Epoch 2/100\n",
      "417/417 [==============================] - 32s 77ms/step - loss: 0.5235 - accuracy: 0.7366 - auc: 0.8149 - val_loss: 0.5189 - val_accuracy: 0.7448 - val_auc: 0.8312\n",
      "Epoch 3/100\n",
      "417/417 [==============================] - 32s 77ms/step - loss: 0.4342 - accuracy: 0.7941 - auc: 0.8792 - val_loss: 0.4489 - val_accuracy: 0.7833 - val_auc: 0.8763\n",
      "Epoch 4/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.3613 - accuracy: 0.8362 - auc: 0.9180 - val_loss: 0.4346 - val_accuracy: 0.7893 - val_auc: 0.8837\n",
      "Epoch 5/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.3094 - accuracy: 0.8647 - auc: 0.9409 - val_loss: 0.3943 - val_accuracy: 0.8373 - val_auc: 0.9095\n",
      "Epoch 6/100\n",
      "417/417 [==============================] - 32s 76ms/step - loss: 0.2622 - accuracy: 0.8893 - auc: 0.9577 - val_loss: 0.4492 - val_accuracy: 0.8184 - val_auc: 0.8992\n",
      "Epoch 7/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.2331 - accuracy: 0.9037 - auc: 0.9666 - val_loss: 0.4253 - val_accuracy: 0.8298 - val_auc: 0.9112\n",
      "Epoch 8/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.2099 - accuracy: 0.9152 - auc: 0.9729 - val_loss: 0.4352 - val_accuracy: 0.8359 - val_auc: 0.9119\n",
      "Epoch 9/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1852 - accuracy: 0.9260 - auc: 0.9787 - val_loss: 0.4265 - val_accuracy: 0.8163 - val_auc: 0.9122\n",
      "Epoch 10/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1660 - accuracy: 0.9380 - auc: 0.9826 - val_loss: 0.4233 - val_accuracy: 0.8332 - val_auc: 0.9259\n",
      "Epoch 11/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1527 - accuracy: 0.9420 - auc: 0.9852 - val_loss: 0.3916 - val_accuracy: 0.8393 - val_auc: 0.9227\n",
      "Epoch 12/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1466 - accuracy: 0.9463 - auc: 0.9862 - val_loss: 0.3283 - val_accuracy: 0.8758 - val_auc: 0.9436\n",
      "Epoch 13/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1298 - accuracy: 0.9515 - auc: 0.9890 - val_loss: 0.3962 - val_accuracy: 0.8548 - val_auc: 0.9296\n",
      "Epoch 14/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1103 - accuracy: 0.9574 - auc: 0.9920 - val_loss: 0.4312 - val_accuracy: 0.8555 - val_auc: 0.9321\n",
      "Epoch 15/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1180 - accuracy: 0.9593 - auc: 0.9905 - val_loss: 0.3632 - val_accuracy: 0.8643 - val_auc: 0.9398\n",
      "Epoch 16/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.1037 - accuracy: 0.9601 - auc: 0.9927 - val_loss: 0.5022 - val_accuracy: 0.8515 - val_auc: 0.9322\n",
      "Epoch 17/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0961 - accuracy: 0.9647 - auc: 0.9936 - val_loss: 0.3683 - val_accuracy: 0.8866 - val_auc: 0.9444\n",
      "Epoch 18/100\n",
      "417/417 [==============================] - 31s 74ms/step - loss: 0.1004 - accuracy: 0.9640 - auc: 0.9930 - val_loss: 0.3512 - val_accuracy: 0.8778 - val_auc: 0.9459\n",
      "Epoch 19/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0855 - accuracy: 0.9694 - auc: 0.9949 - val_loss: 0.3690 - val_accuracy: 0.8798 - val_auc: 0.9423\n",
      "Epoch 20/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0865 - accuracy: 0.9703 - auc: 0.9944 - val_loss: 0.3964 - val_accuracy: 0.8798 - val_auc: 0.9434\n",
      "Epoch 21/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0792 - accuracy: 0.9721 - auc: 0.9954 - val_loss: 0.4100 - val_accuracy: 0.8825 - val_auc: 0.9383\n",
      "Epoch 22/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0899 - accuracy: 0.9688 - auc: 0.9944 - val_loss: 0.4015 - val_accuracy: 0.8785 - val_auc: 0.9425\n",
      "Epoch 23/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0733 - accuracy: 0.9749 - auc: 0.9962 - val_loss: 0.3788 - val_accuracy: 0.8832 - val_auc: 0.9470\n",
      "Epoch 24/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0817 - accuracy: 0.9713 - auc: 0.9949 - val_loss: 0.5172 - val_accuracy: 0.8420 - val_auc: 0.9348\n",
      "Epoch 25/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0659 - accuracy: 0.9772 - auc: 0.9962 - val_loss: 0.4504 - val_accuracy: 0.8704 - val_auc: 0.9378\n",
      "Epoch 26/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0538 - accuracy: 0.9811 - auc: 0.9974 - val_loss: 0.4655 - val_accuracy: 0.8697 - val_auc: 0.9334\n",
      "Epoch 27/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0593 - accuracy: 0.9805 - auc: 0.9966 - val_loss: 0.3702 - val_accuracy: 0.8791 - val_auc: 0.9445\n",
      "Epoch 28/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0774 - accuracy: 0.9742 - auc: 0.9950 - val_loss: 0.4154 - val_accuracy: 0.8940 - val_auc: 0.9453\n",
      "Epoch 29/100\n",
      "417/417 [==============================] - 31s 74ms/step - loss: 0.0660 - accuracy: 0.9785 - auc: 0.9963 - val_loss: 0.4073 - val_accuracy: 0.8839 - val_auc: 0.9458\n",
      "Epoch 30/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0559 - accuracy: 0.9826 - auc: 0.9965 - val_loss: 0.3734 - val_accuracy: 0.8859 - val_auc: 0.9488\n",
      "Epoch 31/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0558 - accuracy: 0.9821 - auc: 0.9971 - val_loss: 0.3581 - val_accuracy: 0.8960 - val_auc: 0.9489\n",
      "Epoch 32/100\n",
      "417/417 [==============================] - 31s 75ms/step - loss: 0.0518 - accuracy: 0.9836 - auc: 0.9974 - val_loss: 0.3564 - val_accuracy: 0.8845 - val_auc: 0.9478\n",
      "Epoch 33/100\n",
      "417/417 [==============================] - 33s 78ms/step - loss: 0.0477 - accuracy: 0.9838 - auc: 0.9977 - val_loss: 0.4638 - val_accuracy: 0.8731 - val_auc: 0.9400\n",
      "Epoch 34/100\n",
      "417/417 [==============================] - 45s 109ms/step - loss: 0.0556 - accuracy: 0.9831 - auc: 0.9966 - val_loss: 0.4233 - val_accuracy: 0.8872 - val_auc: 0.9449\n",
      "Epoch 35/100\n",
      "417/417 [==============================] - 55s 132ms/step - loss: 0.0585 - accuracy: 0.9818 - auc: 0.9968 - val_loss: 0.4092 - val_accuracy: 0.8953 - val_auc: 0.9479\n",
      "Epoch 36/100\n",
      "417/417 [==============================] - 65s 156ms/step - loss: 0.0500 - accuracy: 0.9850 - auc: 0.9973 - val_loss: 0.3781 - val_accuracy: 0.8906 - val_auc: 0.9473\n",
      "Epoch 37/100\n",
      "417/417 [==============================] - 85s 204ms/step - loss: 0.0552 - accuracy: 0.9809 - auc: 0.9974 - val_loss: 0.3692 - val_accuracy: 0.9048 - val_auc: 0.9521\n",
      "Epoch 38/100\n",
      "417/417 [==============================] - 69s 165ms/step - loss: 0.0422 - accuracy: 0.9874 - auc: 0.9977 - val_loss: 0.4473 - val_accuracy: 0.8744 - val_auc: 0.9407\n",
      "Epoch 39/100\n",
      "417/417 [==============================] - 69s 166ms/step - loss: 0.0515 - accuracy: 0.9833 - auc: 0.9975 - val_loss: 0.4072 - val_accuracy: 0.8866 - val_auc: 0.9434\n",
      "Epoch 40/100\n",
      "417/417 [==============================] - 68s 162ms/step - loss: 0.0427 - accuracy: 0.9867 - auc: 0.9981 - val_loss: 0.4165 - val_accuracy: 0.8899 - val_auc: 0.9440\n",
      "Epoch 41/100\n",
      "417/417 [==============================] - 77s 185ms/step - loss: 0.0414 - accuracy: 0.9891 - auc: 0.9975 - val_loss: 0.5364 - val_accuracy: 0.8751 - val_auc: 0.9375\n",
      "Epoch 42/100\n",
      "417/417 [==============================] - 68s 163ms/step - loss: 0.0522 - accuracy: 0.9839 - auc: 0.9972 - val_loss: 0.4098 - val_accuracy: 0.8953 - val_auc: 0.9462\n",
      "Epoch 43/100\n",
      "417/417 [==============================] - 82s 196ms/step - loss: 0.0475 - accuracy: 0.9851 - auc: 0.9975 - val_loss: 0.4377 - val_accuracy: 0.8717 - val_auc: 0.9444\n",
      "Epoch 44/100\n",
      "417/417 [==============================] - 79s 190ms/step - loss: 0.0370 - accuracy: 0.9887 - auc: 0.9983 - val_loss: 0.6276 - val_accuracy: 0.8535 - val_auc: 0.9282\n",
      "Epoch 45/100\n",
      "417/417 [==============================] - 78s 188ms/step - loss: 0.0393 - accuracy: 0.9893 - auc: 0.9980 - val_loss: 0.4704 - val_accuracy: 0.8798 - val_auc: 0.9430\n",
      "Epoch 46/100\n",
      "417/417 [==============================] - 85s 204ms/step - loss: 0.0425 - accuracy: 0.9876 - auc: 0.9977 - val_loss: 0.4284 - val_accuracy: 0.8933 - val_auc: 0.9454\n",
      "Epoch 47/100\n",
      "417/417 [==============================] - 68s 164ms/step - loss: 0.0345 - accuracy: 0.9905 - auc: 0.9985 - val_loss: 0.3680 - val_accuracy: 0.8947 - val_auc: 0.9516\n",
      "Epoch 48/100\n",
      "417/417 [==============================] - 66s 158ms/step - loss: 0.0439 - accuracy: 0.9866 - auc: 0.9976 - val_loss: 0.4167 - val_accuracy: 0.8913 - val_auc: 0.9507\n",
      "Epoch 49/100\n",
      "417/417 [==============================] - 76s 183ms/step - loss: 0.0479 - accuracy: 0.9856 - auc: 0.9976 - val_loss: 0.3799 - val_accuracy: 0.8933 - val_auc: 0.9516\n",
      "Epoch 50/100\n",
      "417/417 [==============================] - 78s 187ms/step - loss: 0.0288 - accuracy: 0.9911 - auc: 0.9987 - val_loss: 0.4066 - val_accuracy: 0.8960 - val_auc: 0.9474\n",
      "Epoch 51/100\n",
      "417/417 [==============================] - 82s 196ms/step - loss: 0.0333 - accuracy: 0.9905 - auc: 0.9984 - val_loss: 0.3828 - val_accuracy: 0.9109 - val_auc: 0.9542\n",
      "Epoch 52/100\n",
      "417/417 [==============================] - 71s 169ms/step - loss: 0.0432 - accuracy: 0.9865 - auc: 0.9981 - val_loss: 0.4046 - val_accuracy: 0.8872 - val_auc: 0.9451\n",
      "Epoch 53/100\n",
      "417/417 [==============================] - 68s 163ms/step - loss: 0.0507 - accuracy: 0.9839 - auc: 0.9975 - val_loss: 0.3968 - val_accuracy: 0.8947 - val_auc: 0.9517\n",
      "Epoch 54/100\n",
      "417/417 [==============================] - 65s 155ms/step - loss: 0.0386 - accuracy: 0.9880 - auc: 0.9981 - val_loss: 0.4156 - val_accuracy: 0.8974 - val_auc: 0.9500\n",
      "Epoch 55/100\n",
      "417/417 [==============================] - 71s 169ms/step - loss: 0.0275 - accuracy: 0.9926 - auc: 0.9985 - val_loss: 0.4026 - val_accuracy: 0.9048 - val_auc: 0.9477\n",
      "Epoch 56/100\n",
      "417/417 [==============================] - 74s 177ms/step - loss: 0.0275 - accuracy: 0.9925 - auc: 0.9987 - val_loss: 0.4869 - val_accuracy: 0.8764 - val_auc: 0.9465\n",
      "Epoch 57/100\n",
      "417/417 [==============================] - 81s 194ms/step - loss: 0.0323 - accuracy: 0.9892 - auc: 0.9987 - val_loss: 0.4573 - val_accuracy: 0.8798 - val_auc: 0.9503\n",
      "Epoch 58/100\n",
      "417/417 [==============================] - 60s 143ms/step - loss: 0.0239 - accuracy: 0.9937 - auc: 0.9990 - val_loss: 0.4774 - val_accuracy: 0.8812 - val_auc: 0.9381\n",
      "Epoch 59/100\n",
      "417/417 [==============================] - 68s 164ms/step - loss: 0.0419 - accuracy: 0.9885 - auc: 0.9978 - val_loss: 0.3874 - val_accuracy: 0.8866 - val_auc: 0.9489\n",
      "Epoch 60/100\n",
      "417/417 [==============================] - 77s 185ms/step - loss: 0.0334 - accuracy: 0.9896 - auc: 0.9984 - val_loss: 0.3498 - val_accuracy: 0.8960 - val_auc: 0.9552\n",
      "Epoch 61/100\n",
      "417/417 [==============================] - 74s 179ms/step - loss: 0.0289 - accuracy: 0.9913 - auc: 0.9988 - val_loss: 0.4242 - val_accuracy: 0.8899 - val_auc: 0.9465\n",
      "Epoch 62/100\n",
      "417/417 [==============================] - 67s 162ms/step - loss: 0.0288 - accuracy: 0.9910 - auc: 0.9985 - val_loss: 0.4503 - val_accuracy: 0.8785 - val_auc: 0.9468\n",
      "Epoch 63/100\n",
      "417/417 [==============================] - 88s 210ms/step - loss: 0.0231 - accuracy: 0.9941 - auc: 0.9987 - val_loss: 0.5902 - val_accuracy: 0.8758 - val_auc: 0.9358\n",
      "Epoch 64/100\n",
      "417/417 [==============================] - 78s 188ms/step - loss: 0.0389 - accuracy: 0.9881 - auc: 0.9982 - val_loss: 0.4695 - val_accuracy: 0.8845 - val_auc: 0.9404\n",
      "Epoch 65/100\n",
      "417/417 [==============================] - 71s 170ms/step - loss: 0.0511 - accuracy: 0.9849 - auc: 0.9969 - val_loss: 0.3820 - val_accuracy: 0.8953 - val_auc: 0.9515\n",
      "Epoch 66/100\n",
      "417/417 [==============================] - 78s 187ms/step - loss: 0.0338 - accuracy: 0.9898 - auc: 0.9986 - val_loss: 0.3943 - val_accuracy: 0.8987 - val_auc: 0.9516\n",
      "Epoch 67/100\n",
      "417/417 [==============================] - 79s 190ms/step - loss: 0.0343 - accuracy: 0.9896 - auc: 0.9981 - val_loss: 0.3921 - val_accuracy: 0.8893 - val_auc: 0.9539\n",
      "Epoch 68/100\n",
      "417/417 [==============================] - 67s 160ms/step - loss: 0.0241 - accuracy: 0.9935 - auc: 0.9988 - val_loss: 0.4651 - val_accuracy: 0.8886 - val_auc: 0.9426\n",
      "Epoch 69/100\n",
      "417/417 [==============================] - 91s 219ms/step - loss: 0.0365 - accuracy: 0.9896 - auc: 0.9982 - val_loss: 0.3305 - val_accuracy: 0.9075 - val_auc: 0.9585\n",
      "Epoch 70/100\n",
      "417/417 [==============================] - 87s 210ms/step - loss: 0.0244 - accuracy: 0.9929 - auc: 0.9989 - val_loss: 0.3523 - val_accuracy: 0.9061 - val_auc: 0.9535\n",
      "Epoch 71/100\n",
      "417/417 [==============================] - 78s 186ms/step - loss: 0.0216 - accuracy: 0.9946 - auc: 0.9987 - val_loss: 0.3887 - val_accuracy: 0.9055 - val_auc: 0.9543\n",
      "Epoch 72/100\n",
      "417/417 [==============================] - 90s 216ms/step - loss: 0.0235 - accuracy: 0.9929 - auc: 0.9990 - val_loss: 0.3461 - val_accuracy: 0.9075 - val_auc: 0.9578\n",
      "Epoch 73/100\n",
      "417/417 [==============================] - 74s 177ms/step - loss: 0.0250 - accuracy: 0.9934 - auc: 0.9989 - val_loss: 0.4055 - val_accuracy: 0.8886 - val_auc: 0.9472\n",
      "Epoch 74/100\n",
      "417/417 [==============================] - 81s 194ms/step - loss: 0.0350 - accuracy: 0.9890 - auc: 0.9984 - val_loss: 0.3896 - val_accuracy: 0.9014 - val_auc: 0.9508\n",
      "Epoch 75/100\n",
      "417/417 [==============================] - 90s 216ms/step - loss: 0.0269 - accuracy: 0.9921 - auc: 0.9987 - val_loss: 0.3644 - val_accuracy: 0.8974 - val_auc: 0.9549\n",
      "Epoch 76/100\n",
      "417/417 [==============================] - 82s 196ms/step - loss: 0.0256 - accuracy: 0.9920 - auc: 0.9988 - val_loss: 0.3801 - val_accuracy: 0.9028 - val_auc: 0.9521\n",
      "Epoch 77/100\n",
      "417/417 [==============================] - 80s 193ms/step - loss: 0.0216 - accuracy: 0.9943 - auc: 0.9990 - val_loss: 0.3709 - val_accuracy: 0.9075 - val_auc: 0.9542\n",
      "Epoch 78/100\n",
      "417/417 [==============================] - 85s 203ms/step - loss: 0.0261 - accuracy: 0.9930 - auc: 0.9988 - val_loss: 0.3264 - val_accuracy: 0.9082 - val_auc: 0.9585\n",
      "Epoch 79/100\n",
      "417/417 [==============================] - 83s 199ms/step - loss: 0.0207 - accuracy: 0.9945 - auc: 0.9990 - val_loss: 0.3320 - val_accuracy: 0.9169 - val_auc: 0.9579\n",
      "Epoch 80/100\n",
      "417/417 [==============================] - 77s 184ms/step - loss: 0.0227 - accuracy: 0.9943 - auc: 0.9987 - val_loss: 0.3285 - val_accuracy: 0.9217 - val_auc: 0.9601\n",
      "Epoch 81/100\n",
      "417/417 [==============================] - 82s 197ms/step - loss: 0.0271 - accuracy: 0.9930 - auc: 0.9986 - val_loss: 0.5413 - val_accuracy: 0.8771 - val_auc: 0.9362\n",
      "Epoch 82/100\n",
      "417/417 [==============================] - 85s 204ms/step - loss: 0.0328 - accuracy: 0.9909 - auc: 0.9982 - val_loss: 0.5767 - val_accuracy: 0.8751 - val_auc: 0.9332\n",
      "Epoch 83/100\n",
      "417/417 [==============================] - 80s 193ms/step - loss: 0.0340 - accuracy: 0.9897 - auc: 0.9982 - val_loss: 0.3901 - val_accuracy: 0.8980 - val_auc: 0.9521\n",
      "Epoch 84/100\n",
      "417/417 [==============================] - 74s 178ms/step - loss: 0.0320 - accuracy: 0.9915 - auc: 0.9985 - val_loss: 0.3434 - val_accuracy: 0.9109 - val_auc: 0.9579\n",
      "Epoch 85/100\n",
      "417/417 [==============================] - 79s 189ms/step - loss: 0.0192 - accuracy: 0.9947 - auc: 0.9992 - val_loss: 0.3704 - val_accuracy: 0.9061 - val_auc: 0.9543\n",
      "Epoch 86/100\n",
      "417/417 [==============================] - 76s 182ms/step - loss: 0.0251 - accuracy: 0.9932 - auc: 0.9987 - val_loss: 0.3842 - val_accuracy: 0.8994 - val_auc: 0.9503\n",
      "Epoch 87/100\n",
      "417/417 [==============================] - 96s 231ms/step - loss: 0.0245 - accuracy: 0.9929 - auc: 0.9988 - val_loss: 0.3943 - val_accuracy: 0.9034 - val_auc: 0.9531\n",
      "Epoch 88/100\n",
      "417/417 [==============================] - 82s 197ms/step - loss: 0.0305 - accuracy: 0.9923 - auc: 0.9983 - val_loss: 0.4167 - val_accuracy: 0.8974 - val_auc: 0.9499\n",
      "Epoch 89/100\n",
      "417/417 [==============================] - 83s 198ms/step - loss: 0.0170 - accuracy: 0.9950 - auc: 0.9992 - val_loss: 0.6019 - val_accuracy: 0.8785 - val_auc: 0.9272\n",
      "Epoch 90/100\n",
      "417/417 [==============================] - 87s 210ms/step - loss: 0.0172 - accuracy: 0.9959 - auc: 0.9991 - val_loss: 0.4808 - val_accuracy: 0.8825 - val_auc: 0.9382\n",
      "Epoch 91/100\n",
      "417/417 [==============================] - 78s 187ms/step - loss: 0.0255 - accuracy: 0.9929 - auc: 0.9986 - val_loss: 0.3635 - val_accuracy: 0.8953 - val_auc: 0.9529\n",
      "Epoch 92/100\n",
      "417/417 [==============================] - 75s 180ms/step - loss: 0.0269 - accuracy: 0.9929 - auc: 0.9984 - val_loss: 0.3651 - val_accuracy: 0.9014 - val_auc: 0.9581\n",
      "Epoch 93/100\n",
      "417/417 [==============================] - 91s 219ms/step - loss: 0.0264 - accuracy: 0.9925 - auc: 0.9989 - val_loss: 0.3558 - val_accuracy: 0.9102 - val_auc: 0.9595\n",
      "Epoch 94/100\n",
      "417/417 [==============================] - 66s 157ms/step - loss: 0.0242 - accuracy: 0.9926 - auc: 0.9991 - val_loss: 0.4244 - val_accuracy: 0.8893 - val_auc: 0.9466\n",
      "Epoch 95/100\n",
      "417/417 [==============================] - 76s 183ms/step - loss: 0.0341 - accuracy: 0.9907 - auc: 0.9980 - val_loss: 0.4634 - val_accuracy: 0.8636 - val_auc: 0.9413\n",
      "Epoch 96/100\n",
      "417/417 [==============================] - 68s 164ms/step - loss: 0.0250 - accuracy: 0.9931 - auc: 0.9989 - val_loss: 0.4682 - val_accuracy: 0.8818 - val_auc: 0.9405\n",
      "Epoch 97/100\n",
      "417/417 [==============================] - 81s 195ms/step - loss: 0.0223 - accuracy: 0.9942 - auc: 0.9990 - val_loss: 0.4248 - val_accuracy: 0.8980 - val_auc: 0.9530\n",
      "Epoch 98/100\n",
      "417/417 [==============================] - 65s 155ms/step - loss: 0.0168 - accuracy: 0.9957 - auc: 0.9993 - val_loss: 0.4254 - val_accuracy: 0.9075 - val_auc: 0.9534\n",
      "Epoch 99/100\n",
      "417/417 [==============================] - 79s 188ms/step - loss: 0.0228 - accuracy: 0.9946 - auc: 0.9988 - val_loss: 0.4017 - val_accuracy: 0.8920 - val_auc: 0.9519\n",
      "Epoch 100/100\n",
      "417/417 [==============================] - 89s 214ms/step - loss: 0.0259 - accuracy: 0.9926 - auc: 0.9983 - val_loss: 0.3705 - val_accuracy: 0.9048 - val_auc: 0.9487\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    train, train_labels,\n",
    "    validation_data=(val, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3581ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 15.2%\n",
      "RAM Used: -3787.8 MB\n",
      "Time Usage: 6240.2 s\n",
      "GPU Memory Used: 1069.1 MB\n",
      "Power Consumption: 149W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f547634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 17s 49ms/step - loss: 0.1645 - accuracy: 0.9563 - auc: 0.9765\n",
      "\n",
      "Test Accuracy of Celeb-DF on InceptionV3: 95.63%\n",
      "340/340 [==============================] - 27s 51ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.92      2961\n",
      "           1       0.95      0.99      0.97      7891\n",
      "\n",
      "    accuracy                           0.96     10852\n",
      "   macro avg       0.96      0.93      0.94     10852\n",
      "weighted avg       0.96      0.96      0.96     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-DF on InceptionV3: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6944e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 11s 32ms/step - loss: 0.1845 - accuracy: 0.9551 - auc: 0.9733\n",
      "\n",
      "Test Accuracy of Celeb-DF on InceptionV3: 95.51%\n",
      "340/340 [==============================] - 11s 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91      2961\n",
      "           1       0.95      0.99      0.97      7891\n",
      "\n",
      "    accuracy                           0.96     10852\n",
      "   macro avg       0.96      0.93      0.94     10852\n",
      "weighted avg       0.96      0.96      0.95     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-DF on InceptionV3: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3466dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# After training the model\n",
    "model.save('InceptionV3_160_celeb.h5')  # Saves the entire model to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7862e8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 7.4%\n",
      "RAM Used: -1394.8 MB\n",
      "Time Usage: 3346.4 s\n",
      "GPU Memory Used: 1069.0 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01266c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 2s 4ms/step - loss: 0.1922 - accuracy: 0.9294\n",
      "Test Accuracy of Celeb-df on Meso NET: 92.94%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_celeb.h5')\n",
    "test_loss, test_acc = model.evaluate(test, test_labels)\n",
    "print(f'Test Accuracy of Celeb-df on InceptionV3: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd91e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "340/340 [==============================] - 17s 35ms/step - loss: 0.1845 - accuracy: 0.9551 - auc: 0.9733\n",
      "Test Accuracy of Celeb-df on InceptionV3: 95.51%\n",
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 7.0%\n",
      "RAM Used: 3160.9 MB\n",
      "Time Usage: 23.6 s\n",
      "GPU Memory Used: 1081.9 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_celeb.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc,test_auc = model.evaluate(test, test_labels)\n",
    "print(f'Test Accuracy of Celeb-df on InceptionV3: {test_acc * 100:.2f}%')\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee28d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 33ms/step - loss: 2.6619 - accuracy: 0.5080 - auc: 0.5208\n",
      "\n",
      "Test Accuracy of DFC on Xceptionnet from Celebdf: 50.80%\n",
      "94/94 [==============================] - 3s 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.93      0.65      1500\n",
      "           1       0.55      0.09      0.15      1500\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.53      0.51      0.40      3000\n",
      "weighted avg       0.53      0.51      0.40      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_auc= model.evaluate(test_hog, test_labels)\n",
    "print(f'\\nTest Accuracy of DFC on Xceptionnet from Celebdf: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_hog)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33038f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 46s 266ms/step - loss: 4.6027 - accuracy: 0.3776 - auc: 0.5703\n",
      "\n",
      "Test Accuracy of FF++ on Inceptionnet from Celebdf: 37.76%\n",
      "169/169 [==============================] - 45s 261ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.92      0.45      1468\n",
      "           1       0.85      0.17      0.29      3911\n",
      "\n",
      "    accuracy                           0.38      5379\n",
      "   macro avg       0.57      0.55      0.37      5379\n",
      "weighted avg       0.70      0.38      0.33      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_celeb.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'\\nTest Accuracy of FF++ on Inceptionnet from Celebdf: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_ff)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels_ff, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768d8ae",
   "metadata": {},
   "source": [
    "# DFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5493048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeb shape: (5000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (5000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "ffhq shape: (1000, 224, 224, 3), dtype: uint8\n",
      "celeb shape: (2500, 160, 160, 3), dtype: uint8\n",
      "ffhq shape: (2500, 160, 160, 3), dtype: uint8\n",
      "gdwct shape: (1000, 160, 160, 3), dtype: uint8\n",
      "attagan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stargan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stylegan2 shape: (1000, 160, 160, 3), dtype: uint8\n",
      "stylegan shape: (1000, 160, 160, 3), dtype: uint8\n",
      "celeb_train: 1750 images, celeb_test: 750 images\n",
      "ffhq_train: 1750 images, ffhq_test: 750 images\n",
      "attgan_train: 700 images, attgan_test: 300 images\n",
      "stargan_train: 700 images, stargan_test: 300 images\n",
      "gdwct_train: 700 images, gdwct_test: 300 images\n",
      "stylegan2_train: 700 images, stylegan2_test: 300 images\n",
      "stylegan_train: 700 images, stylegan_test: 300 images\n",
      "celeb_train: 1575 images, celeb_val: 175 images\n",
      "ffhq_train: 1575 images, ffhq_val: 175 images\n",
      "attgan_train: 630 images, attgan_val: 70 images\n",
      "stargan_train: 630 images, stargan_val: 70 images\n",
      "gdwct_train: 630 images, gdwct_val: 70 images\n",
      "stylegan2_train: 630 images, stylegan2_val: 70 images\n",
      "stylegan_train: 630 images, stylegan_val: 70 images\n",
      "Total train: 6300 images\n",
      "Total test: 3000 images\n",
      "Total val: 700 images\n",
      "Train Labels: 6300 \n",
      "Test Labels: 3000 \n",
      "Val Labels: 700 \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('D://thesis//dataset//deepfake dataset//resized_images.h5', 'r') as h5f:\n",
    "    # Access each dataset\n",
    "    celeb = np.array(h5f['celeb'])\n",
    "    ffhq = np.array(h5f['ffhq'])\n",
    "    gdwct = np.array(h5f['gdwct'])\n",
    "    attgan = np.array(h5f['attgan'])\n",
    "    stargan = np.array(h5f['stargan'])\n",
    "    stylegan2 = np.array(h5f['stylegan2'])\n",
    "    stylegan = np.array(h5f['stylegan'])\n",
    "\n",
    "# Now, 'celeb', 'ffhq', etc., are NumPy arrays containing your datasets\n",
    "print(f\"celeb shape: {celeb.shape}, dtype: {celeb.dtype}\")\n",
    "print(f\"ffhq shape: {ffhq.shape}, dtype: {ffhq.dtype}\")\n",
    "print(f\"ffhq shape: {gdwct.shape}, dtype: {gdwct.dtype}\")\n",
    "print(f\"ffhq shape: {attgan.shape}, dtype: {attgan.dtype}\")\n",
    "print(f\"ffhq shape: {stargan.shape}, dtype: {stargan.dtype}\")\n",
    "print(f\"ffhq shape: {stylegan2.shape}, dtype: {stylegan2.dtype}\")\n",
    "print(f\"ffhq shape: {stylegan.shape}, dtype: {stylegan.dtype}\")\n",
    "# Repeat for other datasets as needed\n",
    "import cv2\n",
    "# Function to resize images from (224, 224) to (160, 160)\n",
    "def resize_images(image_array, target_size=(160, 160)):\n",
    "    resized_images = np.array([cv2.resize(img, target_size) for img in image_array])\n",
    "    return resized_images\n",
    "\n",
    "celeb = resize_images(celeb, target_size=(160, 160))\n",
    "ffhq = resize_images(ffhq, target_size=(160, 160))\n",
    "gdwct = resize_images(gdwct, target_size=(160, 160))\n",
    "attgan = resize_images(attgan, target_size=(160, 160))\n",
    "stargan = resize_images(stargan, target_size=(160, 160))\n",
    "stylegan = resize_images(stargan, target_size=(160, 160))\n",
    "stylegan2 = resize_images(stylegan2, target_size=(160, 160))\n",
    "import random\n",
    "# Randomly select 2500 distinct images\n",
    "random_indices = random.sample(range(len(celeb)), 2500)  # Get 2500 random indices\n",
    "celeb = celeb[random_indices]  # Select the random subse\n",
    "\n",
    "import random\n",
    "# Randomly select 2500 distinct images\n",
    "random_indices = random.sample(range(len(ffhq)), 2500)  # Get 2500 random indices\n",
    "ffhq = ffhq[random_indices]  # Select the random subse\n",
    "print(f\"celeb shape: {celeb.shape}, dtype: {celeb.dtype}\")\n",
    "print(f\"ffhq shape: {ffhq.shape}, dtype: {ffhq.dtype}\")\n",
    "print(f\"gdwct shape: {gdwct.shape}, dtype: {gdwct.dtype}\")\n",
    "print(f\"attagan shape: {attgan.shape}, dtype: {attgan.dtype}\")\n",
    "print(f\"stargan shape: {stargan.shape}, dtype: {stargan.dtype}\")\n",
    "print(f\"stylegan2 shape: {stylegan2.shape}, dtype: {stylegan2.dtype}\")\n",
    "print(f\"stylegan shape: {stylegan.shape}, dtype: {stylegan.dtype}\")\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def split_data(data, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing sets based on the specified ratio.\n",
    "\n",
    "    Parameters:\n",
    "        data (list or np.array): The dataset to split.\n",
    "        train_ratio (float): The ratio of the data to include in the training set.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two datasets - train and test.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "\n",
    "    # Split the data\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Split `celeb` into 70% train and 30% test\n",
    "celeb_train_hog, celeb_test_hog = split_data(celeb, train_ratio=0.7)\n",
    "\n",
    "# Split `ffhq` into 70% train and 30% test\n",
    "ffhq_train_hog, ffhq_test_hog = split_data(ffhq, train_ratio=0.7)\n",
    "\n",
    "# Split `attgan` into 70% train and 30% test\n",
    "attgan_train_hog, attgan_test_hog = split_data(attgan, train_ratio=0.7)\n",
    "\n",
    "# Split `stargan` into 70% train and 30% test\n",
    "stargan_train_hog, stargan_test_hog = split_data(stargan, train_ratio=0.7)\n",
    "\n",
    "# Split `gdwct` into 70% train and 30% test\n",
    "gdwct_train_hog, gdwct_test_hog = split_data(gdwct, train_ratio=0.7)\n",
    "\n",
    "# Split `stylegan2` into 70% train and 30% test_hog\n",
    "stylegan2_train_hog, stylegan2_test_hog = split_data(stylegan2, train_ratio=0.7)\n",
    "\n",
    "# Split `stylegan` into 70% train and 30% test_hog\n",
    "stylegan_train_hog, stylegan_test_hog = split_data(stylegan, train_ratio=0.7)\n",
    "\n",
    "# Convert to NumPy arrays if needed\n",
    "celeb_train_hog, celeb_test_hog = np.array(celeb_train_hog), np.array(celeb_test_hog)\n",
    "ffhq_train_hog, ffhq_test_hog = np.array(ffhq_train_hog), np.array(ffhq_test_hog)\n",
    "attgan_train_hog, attgan_test_hog = np.array(attgan_train_hog), np.array(attgan_test_hog)\n",
    "stargan_train_hog, stargan_test_hog = np.array(stargan_train_hog), np.array(stargan_test_hog)\n",
    "gdwct_train_hog, gdwct_test_hog = np.array(gdwct_train_hog), np.array(gdwct_test_hog)\n",
    "stylegan2_train_hog, stylegan2_test_hog = np.array(stylegan2_train_hog), np.array(stylegan2_test_hog)\n",
    "stylegan_train_hog, stylegan_test_hog = np.array(stylegan_train_hog), np.array(stylegan_test_hog)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"celeb_train: {len(celeb_train_hog)} images, celeb_test: {len(celeb_test_hog)} images\")\n",
    "print(f\"ffhq_train: {len(ffhq_train_hog)} images, ffhq_test: {len(ffhq_test_hog)} images\")\n",
    "print(f\"attgan_train: {len(attgan_train_hog)} images, attgan_test: {len(attgan_test_hog)} images\")\n",
    "print(f\"stargan_train: {len(stargan_train_hog)} images, stargan_test: {len(stargan_test_hog)} images\")\n",
    "print(f\"gdwct_train: {len(gdwct_train_hog)} images, gdwct_test: {len(gdwct_test_hog)} images\")\n",
    "print(f\"stylegan2_train: {len(stylegan2_train_hog)} images, stylegan2_test: {len(stylegan2_test_hog)} images\")\n",
    "print(f\"stylegan_train: {len(stylegan_train_hog)} images, stylegan_test: {len(stylegan_test_hog)} images\")\n",
    "\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "celeb_train_hog, celeb_val_hog = extract_validation(celeb_train_hog)\n",
    "ffhq_train_hog, ffhq_val_hog = extract_validation(ffhq_train_hog)\n",
    "attgan_train_hog, attgan_val_hog = extract_validation(attgan_train_hog)\n",
    "stargan_train_hog, stargan_val_hog = extract_validation(stargan_train_hog)\n",
    "gdwct_train_hog, gdwct_val_hog = extract_validation(gdwct_train_hog)\n",
    "stylegan2_train_hog, stylegan2_val_hog = extract_validation(stylegan2_train_hog)\n",
    "stylegan_train_hog, stylegan_val_hog = extract_validation(stylegan_train_hog)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"celeb_train: {len(celeb_train_hog)} images, celeb_val: {len(celeb_val_hog)} images\")\n",
    "print(f\"ffhq_train: {len(ffhq_train_hog)} images, ffhq_val: {len(ffhq_val_hog)} images\")\n",
    "print(f\"attgan_train: {len(attgan_train_hog)} images, attgan_val: {len(attgan_val_hog)} images\")\n",
    "print(f\"stargan_train: {len(stargan_train_hog)} images, stargan_val: {len(stargan_val_hog)} images\")\n",
    "print(f\"gdwct_train: {len(gdwct_train_hog)} images, gdwct_val: {len(gdwct_val_hog)} images\")\n",
    "print(f\"stylegan2_train: {len(stylegan2_train_hog)} images, stylegan2_val: {len(stylegan2_val_hog)} images\")\n",
    "print(f\"stylegan_train: {len(stylegan_train_hog)} images, stylegan_val: {len(stylegan_val_hog)} images\")\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "celeb_train_labels = np.zeros(len(celeb_train_hog), dtype=int)\n",
    "ffhq_train_labels = np.zeros(len(ffhq_train_hog), dtype=int)\n",
    "atta_train_labels = np.ones(len(attgan_train_hog), dtype=int)\n",
    "star_train_labels = np.ones(len(stargan_train_hog), dtype=int)\n",
    "gdwct_train_labels = np.ones(len(gdwct_train_hog), dtype=int)\n",
    "stylegan2_train_labels = np.ones(len(stylegan2_train_hog), dtype=int)\n",
    "stylegan_train_labels = np.ones(len(stylegan_train_hog), dtype=int)\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train_hog = np.concatenate([celeb_train_hog, ffhq_train_hog, attgan_train_hog, stargan_train_hog, gdwct_train_hog, stylegan2_train_hog, stylegan_train_hog], axis=0)\n",
    "train_labels=np.concatenate([celeb_train_labels, ffhq_train_labels, atta_train_labels, star_train_labels, gdwct_train_labels, stylegan2_train_labels,\n",
    "                              stylegan_train_labels], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "celeb_test_labels = np.zeros(len(celeb_test_hog), dtype=int)\n",
    "ffhq_test_labels = np.zeros(len(ffhq_test_hog), dtype=int)\n",
    "atta_test_labels = np.ones(len(attgan_test_hog), dtype=int)\n",
    "star_test_labels = np.ones(len(stargan_test_hog), dtype=int)\n",
    "gdwct_test_labels = np.ones(len(gdwct_test_hog), dtype=int)\n",
    "stylegan2_test_labels = np.ones(len(stylegan2_test_hog), dtype=int)\n",
    "stylegan_test_labels = np.ones(len(stylegan_test_hog), dtype=int)\n",
    "\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test_hog = np.concatenate([celeb_test_hog, ffhq_test_hog, attgan_test_hog, stargan_test_hog, gdwct_test_hog, stylegan2_test_hog, stylegan_test_hog], axis=0)\n",
    "test_labels = np.concatenate([celeb_test_labels, ffhq_test_labels, atta_test_labels, star_test_labels, gdwct_test_labels, stylegan2_test_labels,\n",
    "                        stylegan_test_labels], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "celeb_val_labels = np.zeros(len(celeb_val_hog), dtype=int)\n",
    "ffhq_val_labels = np.zeros(len(ffhq_val_hog), dtype=int)\n",
    "atta_val_labels = np.ones(len(attgan_val_hog), dtype=int)\n",
    "star_val_labels = np.ones(len(stargan_val_hog), dtype=int)\n",
    "gdwct_val_labels = np.ones(len(gdwct_val_hog), dtype=int)\n",
    "stylegan2_val_labels = np.ones(len(stylegan2_val_hog), dtype=int)\n",
    "stylegan_val_labels = np.ones(len(stylegan_val_hog), dtype=int)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val_hog = np.concatenate([celeb_val_hog, ffhq_val_hog, attgan_val_hog, stargan_val_hog, gdwct_val_hog, stylegan2_val_hog, stylegan_val_hog], axis=0)\n",
    "val_labels = np.concatenate([celeb_val_labels, ffhq_val_labels, atta_val_labels, star_val_labels, gdwct_val_labels, stylegan2_val_labels,\n",
    "                       stylegan_val_labels], axis=0)\n",
    "\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train_hog)} images\")\n",
    "print(f\"Total test: {len(test_hog)} images\")\n",
    "print(f\"Total val: {len(val_hog)} images\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels)} \")\n",
    "print(f\"Test Labels: {len(test_labels)} \")\n",
    "print(f\"Val Labels: {len(val_labels)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9a640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "197/197 [==============================] - 23s 73ms/step - loss: 0.5248 - accuracy: 0.7297 - auc: 0.8124 - val_loss: 0.5132 - val_accuracy: 0.7586 - val_auc: 0.8592\n",
      "Epoch 2/100\n",
      "197/197 [==============================] - 13s 66ms/step - loss: 0.2456 - accuracy: 0.9002 - auc: 0.9634 - val_loss: 0.2999 - val_accuracy: 0.8714 - val_auc: 0.9519\n",
      "Epoch 3/100\n",
      "197/197 [==============================] - 13s 67ms/step - loss: 0.1440 - accuracy: 0.9478 - auc: 0.9866 - val_loss: 0.2707 - val_accuracy: 0.9057 - val_auc: 0.9609\n",
      "Epoch 4/100\n",
      "197/197 [==============================] - 22s 114ms/step - loss: 0.1038 - accuracy: 0.9621 - auc: 0.9927 - val_loss: 0.2392 - val_accuracy: 0.9200 - val_auc: 0.9737\n",
      "Epoch 5/100\n",
      "197/197 [==============================] - 20s 101ms/step - loss: 0.0736 - accuracy: 0.9733 - auc: 0.9960 - val_loss: 0.2106 - val_accuracy: 0.9486 - val_auc: 0.9814\n",
      "Epoch 6/100\n",
      "197/197 [==============================] - 33s 169ms/step - loss: 0.0581 - accuracy: 0.9798 - auc: 0.9970 - val_loss: 0.2053 - val_accuracy: 0.9414 - val_auc: 0.9846\n",
      "Epoch 7/100\n",
      "197/197 [==============================] - 33s 167ms/step - loss: 0.0546 - accuracy: 0.9816 - auc: 0.9974 - val_loss: 0.2624 - val_accuracy: 0.9300 - val_auc: 0.9796\n",
      "Epoch 8/100\n",
      "197/197 [==============================] - 37s 189ms/step - loss: 0.0323 - accuracy: 0.9895 - auc: 0.9989 - val_loss: 0.2530 - val_accuracy: 0.9400 - val_auc: 0.9761\n",
      "Epoch 9/100\n",
      "197/197 [==============================] - 39s 197ms/step - loss: 0.0652 - accuracy: 0.9779 - auc: 0.9963 - val_loss: 0.1875 - val_accuracy: 0.9443 - val_auc: 0.9804\n",
      "Epoch 10/100\n",
      "197/197 [==============================] - 33s 170ms/step - loss: 0.0260 - accuracy: 0.9914 - auc: 0.9994 - val_loss: 0.2452 - val_accuracy: 0.9471 - val_auc: 0.9774\n",
      "Epoch 11/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0304 - accuracy: 0.9895 - auc: 0.9991 - val_loss: 0.4310 - val_accuracy: 0.8957 - val_auc: 0.9736\n",
      "Epoch 12/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0379 - accuracy: 0.9856 - auc: 0.9989 - val_loss: 0.2029 - val_accuracy: 0.9471 - val_auc: 0.9798\n",
      "Epoch 13/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0294 - accuracy: 0.9900 - auc: 0.9994 - val_loss: 0.3410 - val_accuracy: 0.9257 - val_auc: 0.9751\n",
      "Epoch 14/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0162 - accuracy: 0.9956 - auc: 0.9997 - val_loss: 0.3004 - val_accuracy: 0.9514 - val_auc: 0.9753\n",
      "Epoch 15/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0317 - accuracy: 0.9903 - auc: 0.9989 - val_loss: 0.2100 - val_accuracy: 0.9414 - val_auc: 0.9800\n",
      "Epoch 16/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0302 - accuracy: 0.9887 - auc: 0.9990 - val_loss: 0.3242 - val_accuracy: 0.9043 - val_auc: 0.9711\n",
      "Epoch 17/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0401 - accuracy: 0.9875 - auc: 0.9982 - val_loss: 0.2425 - val_accuracy: 0.9271 - val_auc: 0.9809\n",
      "Epoch 18/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0098 - accuracy: 0.9976 - auc: 0.9998 - val_loss: 0.1765 - val_accuracy: 0.9571 - val_auc: 0.9858\n",
      "Epoch 19/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0165 - accuracy: 0.9941 - auc: 0.9997 - val_loss: 0.2555 - val_accuracy: 0.9443 - val_auc: 0.9865\n",
      "Epoch 20/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0178 - accuracy: 0.9943 - auc: 0.9996 - val_loss: 0.2243 - val_accuracy: 0.9500 - val_auc: 0.9852\n",
      "Epoch 21/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0177 - accuracy: 0.9944 - auc: 0.9995 - val_loss: 0.2189 - val_accuracy: 0.9543 - val_auc: 0.9850\n",
      "Epoch 22/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0069 - accuracy: 0.9981 - auc: 0.9998 - val_loss: 0.2925 - val_accuracy: 0.9600 - val_auc: 0.9762\n",
      "Epoch 23/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0137 - accuracy: 0.9954 - auc: 0.9999 - val_loss: 0.3025 - val_accuracy: 0.9586 - val_auc: 0.9806\n",
      "Epoch 24/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0155 - accuracy: 0.9940 - auc: 0.9996 - val_loss: 0.2527 - val_accuracy: 0.9557 - val_auc: 0.9853\n",
      "Epoch 25/100\n",
      "197/197 [==============================] - 23s 117ms/step - loss: 0.0079 - accuracy: 0.9968 - auc: 0.9998 - val_loss: 0.2626 - val_accuracy: 0.9643 - val_auc: 0.9791\n",
      "Epoch 26/100\n",
      "197/197 [==============================] - 34s 171ms/step - loss: 0.0377 - accuracy: 0.9881 - auc: 0.9982 - val_loss: 0.1613 - val_accuracy: 0.9543 - val_auc: 0.9854\n",
      "Epoch 27/100\n",
      "197/197 [==============================] - 34s 174ms/step - loss: 0.0202 - accuracy: 0.9932 - auc: 0.9993 - val_loss: 0.3202 - val_accuracy: 0.9386 - val_auc: 0.9703\n",
      "Epoch 28/100\n",
      "197/197 [==============================] - 34s 173ms/step - loss: 0.0210 - accuracy: 0.9933 - auc: 0.9996 - val_loss: 0.2133 - val_accuracy: 0.9586 - val_auc: 0.9837\n",
      "Epoch 29/100\n",
      "197/197 [==============================] - 16s 82ms/step - loss: 0.0027 - accuracy: 0.9990 - auc: 1.0000 - val_loss: 0.1982 - val_accuracy: 0.9629 - val_auc: 0.9855\n",
      "Epoch 30/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0032 - accuracy: 0.9992 - auc: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9629 - val_auc: 0.9790\n",
      "Epoch 31/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0157 - accuracy: 0.9954 - auc: 0.9994 - val_loss: 0.7601 - val_accuracy: 0.8786 - val_auc: 0.9549\n",
      "Epoch 32/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0282 - accuracy: 0.9905 - auc: 0.9991 - val_loss: 0.2155 - val_accuracy: 0.9543 - val_auc: 0.9879\n",
      "Epoch 33/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0217 - accuracy: 0.9938 - auc: 0.9993 - val_loss: 0.2198 - val_accuracy: 0.9543 - val_auc: 0.9894\n",
      "Epoch 34/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0092 - accuracy: 0.9965 - auc: 0.9999 - val_loss: 0.2115 - val_accuracy: 0.9629 - val_auc: 0.9830\n",
      "Epoch 35/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0067 - accuracy: 0.9975 - auc: 1.0000 - val_loss: 0.2337 - val_accuracy: 0.9586 - val_auc: 0.9802\n",
      "Epoch 36/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0050 - accuracy: 0.9984 - auc: 1.0000 - val_loss: 0.4504 - val_accuracy: 0.9171 - val_auc: 0.9677\n",
      "Epoch 37/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0170 - accuracy: 0.9930 - auc: 0.9997 - val_loss: 0.4029 - val_accuracy: 0.9343 - val_auc: 0.9715\n",
      "Epoch 38/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0195 - accuracy: 0.9937 - auc: 0.9995 - val_loss: 0.1915 - val_accuracy: 0.9614 - val_auc: 0.9854\n",
      "Epoch 39/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0118 - accuracy: 0.9968 - auc: 0.9995 - val_loss: 0.2266 - val_accuracy: 0.9571 - val_auc: 0.9780\n",
      "Epoch 40/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0092 - accuracy: 0.9970 - auc: 0.9999 - val_loss: 0.3297 - val_accuracy: 0.9471 - val_auc: 0.9819\n",
      "Epoch 41/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0074 - accuracy: 0.9978 - auc: 0.9998 - val_loss: 0.2166 - val_accuracy: 0.9643 - val_auc: 0.9832\n",
      "Epoch 42/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0090 - accuracy: 0.9970 - auc: 0.9996 - val_loss: 0.2893 - val_accuracy: 0.9600 - val_auc: 0.9816\n",
      "Epoch 43/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0094 - accuracy: 0.9971 - auc: 0.9998 - val_loss: 0.2440 - val_accuracy: 0.9529 - val_auc: 0.9785\n",
      "Epoch 44/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0207 - accuracy: 0.9932 - auc: 0.9991 - val_loss: 0.3651 - val_accuracy: 0.9486 - val_auc: 0.9751\n",
      "Epoch 45/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0042 - accuracy: 0.9984 - auc: 1.0000 - val_loss: 0.2176 - val_accuracy: 0.9686 - val_auc: 0.9810\n",
      "Epoch 46/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 3.5418e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2241 - val_accuracy: 0.9686 - val_auc: 0.9816\n",
      "Epoch 47/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 1.0240e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9686 - val_auc: 0.9817\n",
      "Epoch 48/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 8.3908e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2637 - val_accuracy: 0.9671 - val_auc: 0.9803\n",
      "Epoch 49/100\n",
      "197/197 [==============================] - 12s 63ms/step - loss: 5.0077e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2566 - val_accuracy: 0.9700 - val_auc: 0.9804\n",
      "Epoch 50/100\n",
      "197/197 [==============================] - 12s 63ms/step - loss: 3.7392e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9671 - val_auc: 0.9802\n",
      "Epoch 51/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0372 - accuracy: 0.9903 - auc: 0.9986 - val_loss: 0.3179 - val_accuracy: 0.9457 - val_auc: 0.9746\n",
      "Epoch 52/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0255 - accuracy: 0.9905 - auc: 0.9995 - val_loss: 0.1531 - val_accuracy: 0.9629 - val_auc: 0.9897\n",
      "Epoch 53/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0037 - accuracy: 0.9990 - auc: 1.0000 - val_loss: 0.1838 - val_accuracy: 0.9586 - val_auc: 0.9857\n",
      "Epoch 54/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0017 - accuracy: 0.9995 - auc: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.9629 - val_auc: 0.9838\n",
      "Epoch 55/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0103 - accuracy: 0.9983 - auc: 0.9995 - val_loss: 0.1791 - val_accuracy: 0.9614 - val_auc: 0.9911\n",
      "Epoch 56/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0138 - accuracy: 0.9957 - auc: 0.9994 - val_loss: 0.1666 - val_accuracy: 0.9514 - val_auc: 0.9862\n",
      "Epoch 57/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0152 - accuracy: 0.9941 - auc: 0.9996 - val_loss: 0.3506 - val_accuracy: 0.9343 - val_auc: 0.9742\n",
      "Epoch 58/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0072 - accuracy: 0.9979 - auc: 1.0000 - val_loss: 0.1685 - val_accuracy: 0.9629 - val_auc: 0.9903\n",
      "Epoch 59/100\n",
      "197/197 [==============================] - 33s 170ms/step - loss: 0.0078 - accuracy: 0.9976 - auc: 0.9997 - val_loss: 0.2612 - val_accuracy: 0.9586 - val_auc: 0.9786\n",
      "Epoch 60/100\n",
      "197/197 [==============================] - 36s 181ms/step - loss: 0.0011 - accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.9686 - val_auc: 0.9855\n",
      "Epoch 61/100\n",
      "197/197 [==============================] - 35s 178ms/step - loss: 1.6664e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9671 - val_auc: 0.9831\n",
      "Epoch 62/100\n",
      "197/197 [==============================] - 34s 175ms/step - loss: 1.4540e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9657 - val_auc: 0.9831\n",
      "Epoch 63/100\n",
      "197/197 [==============================] - 14s 73ms/step - loss: 5.2902e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2351 - val_accuracy: 0.9657 - val_auc: 0.9802\n",
      "Epoch 64/100\n",
      "197/197 [==============================] - 14s 69ms/step - loss: 2.6065e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9671 - val_auc: 0.9803\n",
      "Epoch 65/100\n",
      "197/197 [==============================] - 39s 197ms/step - loss: 1.0969e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9643 - val_auc: 0.9873\n",
      "Epoch 66/100\n",
      "197/197 [==============================] - 39s 199ms/step - loss: 0.0290 - accuracy: 0.9921 - auc: 0.9986 - val_loss: 0.3075 - val_accuracy: 0.9329 - val_auc: 0.9738\n",
      "Epoch 67/100\n",
      "197/197 [==============================] - 39s 200ms/step - loss: 0.0144 - accuracy: 0.9951 - auc: 0.9997 - val_loss: 0.2129 - val_accuracy: 0.9543 - val_auc: 0.9842\n",
      "Epoch 68/100\n",
      "197/197 [==============================] - 37s 189ms/step - loss: 0.0138 - accuracy: 0.9963 - auc: 0.9997 - val_loss: 0.1129 - val_accuracy: 0.9771 - val_auc: 0.9910\n",
      "Epoch 69/100\n",
      "197/197 [==============================] - 37s 190ms/step - loss: 0.0018 - accuracy: 0.9994 - auc: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9714 - val_auc: 0.9898\n",
      "Epoch 70/100\n",
      "197/197 [==============================] - 36s 185ms/step - loss: 0.0078 - accuracy: 0.9986 - auc: 0.9998 - val_loss: 0.2465 - val_accuracy: 0.9614 - val_auc: 0.9785\n",
      "Epoch 71/100\n",
      "197/197 [==============================] - 34s 173ms/step - loss: 0.0181 - accuracy: 0.9946 - auc: 0.9993 - val_loss: 0.4478 - val_accuracy: 0.9029 - val_auc: 0.9767\n",
      "Epoch 72/100\n",
      "197/197 [==============================] - 33s 169ms/step - loss: 0.0035 - accuracy: 0.9987 - auc: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9714 - val_auc: 0.9843\n",
      "Epoch 73/100\n",
      "197/197 [==============================] - 35s 177ms/step - loss: 0.0016 - accuracy: 0.9995 - auc: 1.0000 - val_loss: 0.5004 - val_accuracy: 0.9257 - val_auc: 0.9629\n",
      "Epoch 74/100\n",
      "197/197 [==============================] - 35s 179ms/step - loss: 0.0085 - accuracy: 0.9971 - auc: 0.9995 - val_loss: 0.1721 - val_accuracy: 0.9700 - val_auc: 0.9883\n",
      "Epoch 75/100\n",
      "197/197 [==============================] - 35s 179ms/step - loss: 2.4735e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1848 - val_accuracy: 0.9643 - val_auc: 0.9871\n",
      "Epoch 76/100\n",
      "197/197 [==============================] - 34s 174ms/step - loss: 1.5322e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1914 - val_accuracy: 0.9700 - val_auc: 0.9858\n",
      "Epoch 77/100\n",
      "197/197 [==============================] - 36s 182ms/step - loss: 5.0361e-05 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9686 - val_auc: 0.9872\n",
      "Epoch 78/100\n",
      "197/197 [==============================] - 37s 187ms/step - loss: 0.0125 - accuracy: 0.9962 - auc: 0.9995 - val_loss: 1.5284 - val_accuracy: 0.7657 - val_auc: 0.8898\n",
      "Epoch 79/100\n",
      "197/197 [==============================] - 38s 191ms/step - loss: 0.0165 - accuracy: 0.9948 - auc: 0.9995 - val_loss: 0.2710 - val_accuracy: 0.9614 - val_auc: 0.9819\n",
      "Epoch 80/100\n",
      "197/197 [==============================] - 40s 204ms/step - loss: 0.0059 - accuracy: 0.9981 - auc: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9586 - val_auc: 0.9825\n",
      "Epoch 81/100\n",
      "197/197 [==============================] - 40s 205ms/step - loss: 0.0048 - accuracy: 0.9975 - auc: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9629 - val_auc: 0.9831\n",
      "Epoch 82/100\n",
      "197/197 [==============================] - 39s 200ms/step - loss: 0.0096 - accuracy: 0.9979 - auc: 0.9995 - val_loss: 0.1760 - val_accuracy: 0.9700 - val_auc: 0.9877\n",
      "Epoch 83/100\n",
      "197/197 [==============================] - 37s 188ms/step - loss: 0.0064 - accuracy: 0.9986 - auc: 0.9997 - val_loss: 0.2011 - val_accuracy: 0.9671 - val_auc: 0.9860\n",
      "Epoch 84/100\n",
      "197/197 [==============================] - 36s 184ms/step - loss: 0.0013 - accuracy: 0.9998 - auc: 1.0000 - val_loss: 0.2203 - val_accuracy: 0.9657 - val_auc: 0.9811\n",
      "Epoch 85/100\n",
      "197/197 [==============================] - 35s 180ms/step - loss: 0.0064 - accuracy: 0.9983 - auc: 0.9998 - val_loss: 0.2194 - val_accuracy: 0.9643 - val_auc: 0.9845\n",
      "Epoch 86/100\n",
      "197/197 [==============================] - 34s 174ms/step - loss: 0.0044 - accuracy: 0.9987 - auc: 1.0000 - val_loss: 0.2206 - val_accuracy: 0.9529 - val_auc: 0.9861\n",
      "Epoch 87/100\n",
      "197/197 [==============================] - 35s 179ms/step - loss: 0.0029 - accuracy: 0.9990 - auc: 0.9998 - val_loss: 0.2008 - val_accuracy: 0.9686 - val_auc: 0.9828\n",
      "Epoch 88/100\n",
      "197/197 [==============================] - 35s 179ms/step - loss: 7.1403e-04 - accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9671 - val_auc: 0.9845\n",
      "Epoch 89/100\n",
      "197/197 [==============================] - 35s 180ms/step - loss: 0.0103 - accuracy: 0.9975 - auc: 0.9992 - val_loss: 0.1588 - val_accuracy: 0.9686 - val_auc: 0.9824\n",
      "Epoch 90/100\n",
      "197/197 [==============================] - 34s 173ms/step - loss: 0.0017 - accuracy: 0.9995 - auc: 1.0000 - val_loss: 0.3696 - val_accuracy: 0.9343 - val_auc: 0.9772\n",
      "Epoch 91/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0138 - accuracy: 0.9957 - auc: 0.9994 - val_loss: 0.1555 - val_accuracy: 0.9629 - val_auc: 0.9888\n",
      "Epoch 92/100\n",
      "197/197 [==============================] - 13s 66ms/step - loss: 0.0044 - accuracy: 0.9990 - auc: 0.9998 - val_loss: 0.2046 - val_accuracy: 0.9643 - val_auc: 0.9842\n",
      "Epoch 93/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0012 - accuracy: 0.9995 - auc: 1.0000 - val_loss: 0.2186 - val_accuracy: 0.9643 - val_auc: 0.9856\n",
      "Epoch 94/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0026 - accuracy: 0.9990 - auc: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.9057 - val_auc: 0.9649\n",
      "Epoch 95/100\n",
      "197/197 [==============================] - 14s 70ms/step - loss: 0.0118 - accuracy: 0.9949 - auc: 0.9999 - val_loss: 0.2752 - val_accuracy: 0.9400 - val_auc: 0.9799\n",
      "Epoch 96/100\n",
      "197/197 [==============================] - 14s 69ms/step - loss: 0.0067 - accuracy: 0.9975 - auc: 0.9998 - val_loss: 0.1646 - val_accuracy: 0.9657 - val_auc: 0.9862\n",
      "Epoch 97/100\n",
      "197/197 [==============================] - 13s 65ms/step - loss: 0.0016 - accuracy: 0.9997 - auc: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.9743 - val_auc: 0.9871\n",
      "Epoch 98/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 3.4876e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9786 - val_auc: 0.9875\n",
      "Epoch 99/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 2.8526e-04 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.2098 - val_accuracy: 0.9714 - val_auc: 0.9834\n",
      "Epoch 100/100\n",
      "197/197 [==============================] - 13s 64ms/step - loss: 0.0113 - accuracy: 0.9956 - auc: 0.9998 - val_loss: 0.1587 - val_accuracy: 0.9700 - val_auc: 0.9911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_hog, train_labels,\n",
    "    validation_data=(val_hog, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f67017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 33ms/step - loss: 0.4074 - accuracy: 0.9260 - auc: 0.9672\n",
      "\n",
      "Test Accuracy of DFC on InceptionV3: 92.60%\n",
      "94/94 [==============================] - 4s 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      1500\n",
      "           1       0.93      0.92      0.93      1500\n",
      "\n",
      "    accuracy                           0.93      3000\n",
      "   macro avg       0.93      0.93      0.93      3000\n",
      "weighted avg       0.93      0.93      0.93      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_hog, test_labels)\n",
    "print(f'\\nTest Accuracy of DFC on InceptionV3: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_hog)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b46fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# After training the model\n",
    "model.save('InceptionV3_160_dfc.h5')  # Saves the entire model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c23af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 9.7%\n",
      "RAM Used: 1566.5 MB\n",
      "Time Usage: 2751.7 s\n",
      "GPU Memory Used: 494.0 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1caf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 4s 32ms/step - loss: 0.4074 - accuracy: 0.9260 - auc: 0.9672\n",
      "Test Accuracy of DFC on MInceptionV3: 92.60%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_dfc.h5')\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_hog, test_labels)\n",
    "print(f'Test Accuracy of DFC on InceptionV3: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ad586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "94/94 [==============================] - 9s 58ms/step - loss: 0.4293 - accuracy: 0.9150 - auc: 0.9608\n",
      "Test Accuracy of Hog on InceptionV3: 91.50%\n",
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 3.1%\n",
      "RAM Used: 3401.4 MB\n",
      "Time Usage: 15.8 s\n",
      "GPU Memory Used: 506.8 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_dfc.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc,test_auc = model.evaluate(test_hog, test_labels)\n",
    "print(f'Test Accuracy of Hog on InceptionV3: {test_acc * 100:.2f}%')\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e71ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 82s 238ms/step - loss: 2.4703 - accuracy: 0.5921 - auc: 0.5070\n",
      "\n",
      "Test Accuracy of Celeb-df dataset on InceptionV3 for DFC: 59.21%\n",
      "340/340 [==============================] - 89s 261ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.29      0.28      2961\n",
      "           1       0.73      0.70      0.72      7891\n",
      "\n",
      "    accuracy                           0.59     10852\n",
      "   macro avg       0.50      0.50      0.50     10852\n",
      "weighted avg       0.60      0.59      0.60     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_dfc.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-df dataset on InceptionV3 for DFC: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52637104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 40s 231ms/step - loss: 2.8194 - accuracy: 0.5297 - auc: 0.4988\n",
      "\n",
      "Test Accuracy of FF++ on Xceptionnet from DFC: 52.97%\n",
      "169/169 [==============================] - 46s 265ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.43      0.33      1468\n",
      "           1       0.73      0.57      0.64      3911\n",
      "\n",
      "    accuracy                           0.53      5379\n",
      "   macro avg       0.50      0.50      0.49      5379\n",
      "weighted avg       0.60      0.53      0.55      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('InceptionV3_160_dfc.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'\\nTest Accuracy of FF++ on Inceptionnet from DFC: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_ff)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels_ff, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4709c6",
   "metadata": {},
   "source": [
    "# FF++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9f7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Data loaded successfully from the HDF5 file.\n",
      "Shape of the concatenated array: (2808, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (3299, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (2083, 160, 160, 3)\n",
      "Shape of the concatenated array: (1742, 160, 160, 3)\n",
      "Shape of the concatenated array: (1742, 160, 160, 3)\n",
      "Training data ACTOR real shape: (1965, 160, 160, 3) Testing real data shape: (843, 160, 160, 3)\n",
      "Training data Youtube real shape: (1458, 160, 160, 3) Testing real data shape: (625, 160, 160, 3)\n",
      "Training data DFD fake shape: (2309, 160, 160, 3) Testing fake data shape: (990, 160, 160, 3)\n",
      "Training data DF fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data f2f fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data fshifter fake shape: (1458, 160, 160, 3) Testing fake data shape: (625, 160, 160, 3)\n",
      "Training data fswap fake shape: (1219, 160, 160, 3) Testing fake data shape: (523, 160, 160, 3)\n",
      "Training data nt fake shape: (1219, 160, 160, 3) Testing fake data shape: (523, 160, 160, 3)\n",
      "train_ori_actor hog_real: 1768 images, val_ori_actor hog_real: 197 images\n",
      "train_ ori_youtube hog_real: 1312 images, val_ ori_youtube hog_real: 146 images\n",
      "train_hog_mni_dfd_fake: 2078 images, val_hog_mni_dfd_fake: 231 images\n",
      "train_hog_mni_df_fake: 1312 images, val_hog_mni_df_fake: 146 images\n",
      "train_hog_mni_f2f_fake: 1312 images, val_hog_mni_f2f_fake: 146 images\n",
      "train_hog_mni_fshifter_fake: 1312 images, val_hog_mni_fshifter_fake: 146 images\n",
      "train_hog_mni_fswap_fake: 1097 images, val_hog_mni_fswap_fake: 122 images\n",
      "train_hog_mni_nt_fake: 1097 images, val_hog_mni_nt_fake: 122 images\n",
      "Total train: 11288 images, and shape:(11288, 160, 160, 3)\n",
      "Total test: 5379 images, and shape:(5379, 160, 160, 3)\n",
      "Total val: 1256 images, and shape:(1256, 160, 160, 3)\n",
      "Train Labels: 11288 \n",
      "Test Labels: 5379 \n",
      "Val Labels: 1256 \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_orignal_processed.h5\", \"r\") as h5f:\n",
    "    org_seq_actor_array = h5f[\"ori_actor\"][:]\n",
    "    org_seq_youtube_array = h5f[\"ori_youtube\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake1_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_dfd_array = h5f[\"mni_dfd\"][:]\n",
    "    meni_seq_df_array = h5f[\"mni_df\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake2_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_f2f_array = h5f[\"mni_f2f\"][:]\n",
    "    meni_seq_fshifter_array = h5f[\"mni_fshifter\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "with h5py.File(\"D://thesis//dataset//ff++//images_ff++_fake3_processed.h5\", \"r\") as h5f:\n",
    "    meni_seq_fswap_array = h5f[\"mni_fswap\"][:]\n",
    "    meni_seq_nt_array = h5f[\"mni_nt\"][:]\n",
    "print(\"Data loaded successfully from the HDF5 file.\")\n",
    "# Output the shape of the resulting array\n",
    "print(f\"Shape of the concatenated array: {org_seq_actor_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {org_seq_youtube_array.shape}\")\n",
    "\n",
    "print(f\"Shape of the concatenated array: {meni_seq_dfd_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_df_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_fshifter_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_f2f_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_fswap_array.shape}\")\n",
    "print(f\"Shape of the concatenated array: {meni_seq_nt_array.shape}\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ori_actor_train_real, ori_actor_test_real = train_test_split(org_seq_actor_array, test_size=0.3, random_state=42)\n",
    "ori_youtube_train_real, ori_youtube_test_real = train_test_split(org_seq_youtube_array, test_size=0.3, random_state=42)\n",
    "# Split the data into train (70%) and test (30%)\n",
    "mni_dfd_train_fake, mni_dfd_test_fake = train_test_split(meni_seq_dfd_array, test_size=0.3, random_state=42)\n",
    "mni_df_train_fake, mni_df_test_fake = train_test_split(meni_seq_df_array, test_size=0.3, random_state=42)\n",
    "mni_f2f_train_fake, mni_f2f_test_fake = train_test_split(meni_seq_f2f_array, test_size=0.3, random_state=42)\n",
    "mni_fshifter_train_fake, mni_fshifter_test_fake = train_test_split(meni_seq_fshifter_array, test_size=0.3, random_state=42)\n",
    "mni_fswap_train_fake, mni_fswap_test_fake = train_test_split(meni_seq_fswap_array, test_size=0.3, random_state=42)\n",
    "mni_nt_train_fake, mni_nt_test_fake = train_test_split(meni_seq_nt_array, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Output the shape to confirm the split dimensions\n",
    "print(\"Training data ACTOR real shape:\", ori_actor_train_real.shape, \"Testing real data shape:\", ori_actor_test_real.shape)\n",
    "print(\"Training data Youtube real shape:\", ori_youtube_train_real.shape, \"Testing real data shape:\", ori_youtube_test_real.shape)\n",
    "\n",
    "print(\"Training data DFD fake shape:\", mni_dfd_train_fake.shape, \"Testing fake data shape:\", mni_dfd_test_fake.shape)\n",
    "print(\"Training data DF fake shape:\", mni_df_train_fake.shape, \"Testing fake data shape:\", mni_df_test_fake.shape)\n",
    "print(\"Training data f2f fake shape:\", mni_f2f_train_fake.shape, \"Testing fake data shape:\", mni_f2f_test_fake.shape)\n",
    "print(\"Training data fshifter fake shape:\", mni_fshifter_train_fake.shape, \"Testing fake data shape:\", mni_fshifter_test_fake.shape)\n",
    "print(\"Training data fswap fake shape:\", mni_fswap_train_fake.shape, \"Testing fake data shape:\", mni_fswap_test_fake.shape)\n",
    "print(\"Training data nt fake shape:\", mni_nt_train_fake.shape, \"Testing fake data shape:\", mni_nt_test_fake.shape)\n",
    "########################################################################################################################################\n",
    "#######################################divide into 60,10 train and val\n",
    "#########################################################################################################################################\n",
    "def extract_validation(train_data):\n",
    "    \"\"\"\n",
    "    Extract every 10th sample from the training data and store it in a validation set.\n",
    "\n",
    "    Parameters:\n",
    "        train_data (list or np.array): The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated training dataset and validation dataset.\n",
    "    \"\"\"\n",
    "    # Select every 10th sample for the validation set\n",
    "    validation_data = train_data[::10]\n",
    "\n",
    "    # Remove the selected samples from the training dataset\n",
    "    updated_train_data = [train_data[i] for i in range(len(train_data)) if i % 10 != 0]\n",
    "\n",
    "    return np.array(updated_train_data), np.array(validation_data)\n",
    "\n",
    "\n",
    "# Perform the operation for each dataset\n",
    "train_hog_ori_actor_real, val_hog_ori_actor_real = extract_validation(ori_actor_train_real)\n",
    "train_hog_ori_youtube_real, val_hog_ori_youtube_real = extract_validation(ori_youtube_train_real)\n",
    "train_hog_mni_dfd_fake, val_hog_mni_dfd_fake = extract_validation(mni_dfd_train_fake)\n",
    "train_hog_mni_df_fake, val_hog_mni_df_fake = extract_validation(mni_df_train_fake)\n",
    "train_hog_mni_f2f_fake, val_hog_mni_f2f_fake = extract_validation(mni_f2f_train_fake)\n",
    "train_hog_mni_fshifter_fake, val_hog_mni_fshifter_fake = extract_validation(mni_fshifter_train_fake)\n",
    "train_hog_mni_fswap_fake, val_hog_mni_fswap_fake = extract_validation(mni_fswap_train_fake)\n",
    "train_hog_mni_nt_fake, val_hog_mni_nt_fake = extract_validation(mni_nt_train_fake)\n",
    "# Print results for verification\n",
    "print(f\"train_ori_actor hog_real: {len(train_hog_ori_actor_real)} images, val_ori_actor hog_real: {len(val_hog_ori_actor_real)} images\")\n",
    "print(f\"train_ ori_youtube hog_real: {len(train_hog_ori_youtube_real)} images, val_ ori_youtube hog_real: {len(val_hog_ori_youtube_real)} images\")\n",
    "print(f\"train_hog_mni_dfd_fake: {len(train_hog_mni_dfd_fake)} images, val_hog_mni_dfd_fake: {len(val_hog_mni_dfd_fake)} images\")\n",
    "print(f\"train_hog_mni_df_fake: {len(train_hog_mni_df_fake)} images, val_hog_mni_df_fake: {len(val_hog_mni_df_fake)} images\")\n",
    "print(f\"train_hog_mni_f2f_fake: {len(train_hog_mni_f2f_fake)} images, val_hog_mni_f2f_fake: {len(val_hog_mni_f2f_fake)} images\")\n",
    "print(f\"train_hog_mni_fshifter_fake: {len(train_hog_mni_fshifter_fake)} images, val_hog_mni_fshifter_fake: {len(val_hog_mni_fshifter_fake)} images\")\n",
    "print(f\"train_hog_mni_fswap_fake: {len(train_hog_mni_fswap_fake)} images, val_hog_mni_fswap_fake: {len(val_hog_mni_fswap_fake)} images\")\n",
    "print(f\"train_hog_mni_nt_fake: {len(train_hog_mni_nt_fake)} images, val_hog_mni_nt_fake: {len(val_hog_mni_nt_fake)} images\")\n",
    "############################################################################################################################################################\n",
    "#################################################concatenate the labels 0,1 real and fake\n",
    "#############################################################################################################################################################\n",
    "\n",
    "\n",
    "train_ori_actor_labels_real = np.zeros(len(train_hog_ori_actor_real), dtype=int)\n",
    "train_ori_youtube_labels_real = np.zeros(len(train_hog_ori_youtube_real), dtype=int)\n",
    "train_mni_dfd_labels_fake = np.ones(len(train_hog_mni_dfd_fake), dtype=int)\n",
    "train_mni_df_labels_fake = np.ones(len(train_hog_mni_df_fake), dtype=int)\n",
    "train_mni_f2f_labels_fake = np.ones(len(train_hog_mni_f2f_fake), dtype=int)\n",
    "train_mni_fshifter_labels_fake = np.ones(len(train_hog_mni_fshifter_fake), dtype=int)\n",
    "train_mni_fswap_labels_fake = np.ones(len(train_hog_mni_fswap_fake), dtype=int)\n",
    "train_mni_nt_labels_fake = np.ones(len(train_hog_mni_nt_fake), dtype=int)\n",
    "\n",
    "test_ori_actor_labels_real = np.zeros(len(ori_actor_test_real), dtype=int)\n",
    "test_ori_youtube_labels_real = np.zeros(len(ori_youtube_test_real), dtype=int)\n",
    "test_mni_dfd_labels_fake = np.ones(len(mni_dfd_test_fake), dtype=int)\n",
    "test_mni_df_labels_fake = np.ones(len(mni_df_test_fake), dtype=int)\n",
    "test_mni_f2f_labels_fake = np.ones(len(mni_f2f_test_fake), dtype=int)\n",
    "test_mni_fshifter_labels_fake = np.ones(len(mni_fshifter_test_fake), dtype=int)\n",
    "test_mni_fswap_labels_fake = np.ones(len(mni_fswap_test_fake), dtype=int)\n",
    "test_mni_nt_labels_fake = np.ones(len(mni_nt_test_fake), dtype=int)\n",
    "\n",
    "\n",
    "val_ori_actor_labels_real = np.zeros(len(val_hog_ori_actor_real), dtype=int)\n",
    "val_ori_youtube_labels_real = np.zeros(len(val_hog_ori_youtube_real), dtype=int)\n",
    "val_mni_dfd_labels_fake = np.ones(len(val_hog_mni_dfd_fake), dtype=int)\n",
    "val_mni_df_labels_fake = np.ones(len(val_hog_mni_df_fake), dtype=int)\n",
    "val_mni_f2f_labels_fake = np.ones(len(val_hog_mni_f2f_fake), dtype=int)\n",
    "val_mni_fshifter_labels_fake = np.ones(len(val_hog_mni_fshifter_fake), dtype=int)\n",
    "val_mni_fswap_labels_fake = np.ones(len(val_hog_mni_fswap_fake), dtype=int)\n",
    "val_mni_nt_labels_fake = np.ones(len(val_hog_mni_nt_fake), dtype=int)\n",
    "##################################################################################################################\n",
    "\n",
    "# Concatenate all training datasets into a single `train` variable\n",
    "train_ff = np.concatenate([train_hog_ori_actor_real, train_hog_ori_youtube_real,train_hog_mni_dfd_fake,train_hog_mni_df_fake,\n",
    "                            train_hog_mni_f2f_fake,train_hog_mni_fshifter_fake,train_hog_mni_fswap_fake,train_hog_mni_nt_fake], axis=0)\n",
    "\n",
    "train_labels_ff = np.concatenate([train_ori_actor_labels_real, train_ori_youtube_labels_real,train_mni_dfd_labels_fake,train_mni_df_labels_fake,\n",
    "                            train_mni_f2f_labels_fake,train_mni_fshifter_labels_fake,train_mni_fswap_labels_fake,train_mni_nt_labels_fake], axis=0)\n",
    "\n",
    "# Concatenate all validation datasets into a single `val` variable\n",
    "val_ff = np.concatenate([val_hog_ori_actor_real, val_hog_ori_youtube_real, val_hog_mni_dfd_fake, val_hog_mni_df_fake,\n",
    "                            val_hog_mni_f2f_fake, val_hog_mni_fshifter_fake, val_hog_mni_fswap_fake, val_hog_mni_nt_fake], axis=0)\n",
    "val_labels_ff = np.concatenate([val_ori_actor_labels_real, val_ori_youtube_labels_real, val_mni_dfd_labels_fake, val_mni_df_labels_fake,\n",
    "                            val_mni_f2f_labels_fake, val_mni_fshifter_labels_fake, val_mni_fswap_labels_fake, val_mni_nt_labels_fake], axis=0)\n",
    "# Concatenate all testing datasets into a single `test` variable\n",
    "test_ff = np.concatenate([ori_actor_test_real, ori_youtube_test_real, mni_dfd_test_fake,\n",
    "                           mni_df_test_fake, mni_f2f_test_fake, mni_fshifter_test_fake,\n",
    "                           mni_fswap_test_fake, mni_nt_test_fake], axis=0)\n",
    "test_labels_ff = np.concatenate([test_ori_actor_labels_real, test_ori_youtube_labels_real, test_mni_dfd_labels_fake, test_mni_df_labels_fake,\n",
    "                            test_mni_f2f_labels_fake, test_mni_fshifter_labels_fake, test_mni_fswap_labels_fake, test_mni_nt_labels_fake], axis=0)\n",
    "\n",
    "\n",
    "# Print the results for verification\n",
    "# Print the results for verification\n",
    "print(f\"Total train: {len(train_ff)} images, and shape:{train_ff.shape}\")\n",
    "print(f\"Total test: {len(test_ff)} images, and shape:{test_ff.shape}\")\n",
    "print(f\"Total val: {len(val_ff)} images, and shape:{val_ff.shape}\")\n",
    "\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Train Labels: {len(train_labels_ff)} \")\n",
    "print(f\"Test Labels: {len(test_labels_ff)} \")\n",
    "print(f\"Val Labels: {len(val_labels_ff)} \")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7a4081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "353/353 [==============================] - 34s 67ms/step - loss: 0.5839 - accuracy: 0.7210 - auc: 0.6045 - val_loss: 0.5947 - val_accuracy: 0.7373 - val_auc: 0.6371\n",
      "Epoch 2/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.5520 - accuracy: 0.7404 - auc: 0.6671 - val_loss: 0.5598 - val_accuracy: 0.7476 - val_auc: 0.6636\n",
      "Epoch 3/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.5340 - accuracy: 0.7549 - auc: 0.6983 - val_loss: 0.6136 - val_accuracy: 0.7484 - val_auc: 0.6678\n",
      "Epoch 4/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.5149 - accuracy: 0.7667 - auc: 0.7263 - val_loss: 0.5574 - val_accuracy: 0.7325 - val_auc: 0.6674\n",
      "Epoch 5/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.4964 - accuracy: 0.7769 - auc: 0.7475 - val_loss: 0.5596 - val_accuracy: 0.7373 - val_auc: 0.6383\n",
      "Epoch 6/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.4769 - accuracy: 0.7921 - auc: 0.7723 - val_loss: 0.5573 - val_accuracy: 0.7436 - val_auc: 0.6612\n",
      "Epoch 7/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.4538 - accuracy: 0.8011 - auc: 0.7971 - val_loss: 0.6203 - val_accuracy: 0.7094 - val_auc: 0.6673\n",
      "Epoch 8/100\n",
      "353/353 [==============================] - 23s 66ms/step - loss: 0.4201 - accuracy: 0.8161 - auc: 0.8301 - val_loss: 0.6852 - val_accuracy: 0.7412 - val_auc: 0.6803\n",
      "Epoch 9/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.3930 - accuracy: 0.8264 - auc: 0.8568 - val_loss: 0.5751 - val_accuracy: 0.7349 - val_auc: 0.6863\n",
      "Epoch 10/100\n",
      "353/353 [==============================] - 23s 64ms/step - loss: 0.3667 - accuracy: 0.8403 - auc: 0.8774 - val_loss: 0.6327 - val_accuracy: 0.7444 - val_auc: 0.6835\n",
      "Epoch 11/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.3330 - accuracy: 0.8515 - auc: 0.9016 - val_loss: 0.6462 - val_accuracy: 0.7468 - val_auc: 0.6990\n",
      "Epoch 12/100\n",
      "353/353 [==============================] - 36s 101ms/step - loss: 0.3101 - accuracy: 0.8647 - auc: 0.9172 - val_loss: 0.7157 - val_accuracy: 0.7588 - val_auc: 0.6931\n",
      "Epoch 13/100\n",
      "353/353 [==============================] - 69s 194ms/step - loss: 0.2829 - accuracy: 0.8781 - auc: 0.9315 - val_loss: 0.6735 - val_accuracy: 0.6752 - val_auc: 0.6816\n",
      "Epoch 14/100\n",
      "353/353 [==============================] - 66s 187ms/step - loss: 0.2617 - accuracy: 0.8868 - auc: 0.9425 - val_loss: 0.8656 - val_accuracy: 0.7452 - val_auc: 0.6766\n",
      "Epoch 15/100\n",
      "353/353 [==============================] - 62s 176ms/step - loss: 0.2364 - accuracy: 0.8975 - auc: 0.9543 - val_loss: 0.7281 - val_accuracy: 0.7086 - val_auc: 0.6860\n",
      "Epoch 16/100\n",
      "353/353 [==============================] - 61s 172ms/step - loss: 0.2216 - accuracy: 0.9062 - auc: 0.9596 - val_loss: 0.8468 - val_accuracy: 0.6943 - val_auc: 0.6645\n",
      "Epoch 17/100\n",
      "353/353 [==============================] - 61s 172ms/step - loss: 0.2050 - accuracy: 0.9160 - auc: 0.9663 - val_loss: 0.9170 - val_accuracy: 0.7396 - val_auc: 0.6948\n",
      "Epoch 18/100\n",
      "353/353 [==============================] - 33s 93ms/step - loss: 0.1855 - accuracy: 0.9223 - auc: 0.9722 - val_loss: 0.8577 - val_accuracy: 0.6807 - val_auc: 0.6874\n",
      "Epoch 19/100\n",
      "353/353 [==============================] - 23s 65ms/step - loss: 0.1746 - accuracy: 0.9271 - auc: 0.9761 - val_loss: 0.8774 - val_accuracy: 0.7301 - val_auc: 0.6927\n",
      "Epoch 20/100\n",
      "353/353 [==============================] - 22s 64ms/step - loss: 0.1630 - accuracy: 0.9363 - auc: 0.9778 - val_loss: 0.9300 - val_accuracy: 0.7142 - val_auc: 0.6844\n",
      "Epoch 21/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1525 - accuracy: 0.9380 - auc: 0.9808 - val_loss: 1.1325 - val_accuracy: 0.7524 - val_auc: 0.6875\n",
      "Epoch 22/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1468 - accuracy: 0.9413 - auc: 0.9831 - val_loss: 0.9594 - val_accuracy: 0.7349 - val_auc: 0.6943\n",
      "Epoch 23/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.1430 - accuracy: 0.9451 - auc: 0.9827 - val_loss: 0.9438 - val_accuracy: 0.6760 - val_auc: 0.6800\n",
      "Epoch 24/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.1263 - accuracy: 0.9477 - auc: 0.9872 - val_loss: 1.1237 - val_accuracy: 0.7229 - val_auc: 0.6747\n",
      "Epoch 25/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1205 - accuracy: 0.9538 - auc: 0.9878 - val_loss: 1.0184 - val_accuracy: 0.6919 - val_auc: 0.6713\n",
      "Epoch 26/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1191 - accuracy: 0.9552 - auc: 0.9882 - val_loss: 1.1438 - val_accuracy: 0.7094 - val_auc: 0.6932\n",
      "Epoch 27/100\n",
      "353/353 [==============================] - 23s 65ms/step - loss: 0.1159 - accuracy: 0.9573 - auc: 0.9878 - val_loss: 1.0842 - val_accuracy: 0.7412 - val_auc: 0.6781\n",
      "Epoch 28/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1063 - accuracy: 0.9623 - auc: 0.9900 - val_loss: 0.9692 - val_accuracy: 0.7166 - val_auc: 0.6966\n",
      "Epoch 29/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.1079 - accuracy: 0.9603 - auc: 0.9899 - val_loss: 1.0881 - val_accuracy: 0.7221 - val_auc: 0.6816\n",
      "Epoch 30/100\n",
      "353/353 [==============================] - 22s 61ms/step - loss: 0.1034 - accuracy: 0.9617 - auc: 0.9909 - val_loss: 1.1440 - val_accuracy: 0.7373 - val_auc: 0.6810\n",
      "Epoch 31/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0934 - accuracy: 0.9669 - auc: 0.9922 - val_loss: 1.2261 - val_accuracy: 0.7030 - val_auc: 0.6957\n",
      "Epoch 32/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0943 - accuracy: 0.9675 - auc: 0.9917 - val_loss: 1.1904 - val_accuracy: 0.7078 - val_auc: 0.6805\n",
      "Epoch 33/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0923 - accuracy: 0.9686 - auc: 0.9913 - val_loss: 0.9633 - val_accuracy: 0.7150 - val_auc: 0.7012\n",
      "Epoch 34/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0859 - accuracy: 0.9702 - auc: 0.9928 - val_loss: 1.2028 - val_accuracy: 0.7373 - val_auc: 0.6693\n",
      "Epoch 35/100\n",
      "353/353 [==============================] - 35s 100ms/step - loss: 0.0863 - accuracy: 0.9701 - auc: 0.9924 - val_loss: 1.1154 - val_accuracy: 0.7150 - val_auc: 0.6809\n",
      "Epoch 36/100\n",
      "353/353 [==============================] - 48s 136ms/step - loss: 0.0877 - accuracy: 0.9693 - auc: 0.9920 - val_loss: 1.0825 - val_accuracy: 0.7182 - val_auc: 0.6876\n",
      "Epoch 37/100\n",
      "353/353 [==============================] - 22s 64ms/step - loss: 0.0836 - accuracy: 0.9707 - auc: 0.9935 - val_loss: 1.2645 - val_accuracy: 0.7293 - val_auc: 0.6942\n",
      "Epoch 38/100\n",
      "353/353 [==============================] - 48s 137ms/step - loss: 0.0865 - accuracy: 0.9674 - auc: 0.9927 - val_loss: 1.1136 - val_accuracy: 0.7349 - val_auc: 0.7057\n",
      "Epoch 39/100\n",
      "353/353 [==============================] - 66s 188ms/step - loss: 0.0723 - accuracy: 0.9723 - auc: 0.9953 - val_loss: 1.2398 - val_accuracy: 0.7174 - val_auc: 0.6993\n",
      "Epoch 40/100\n",
      "353/353 [==============================] - 46s 129ms/step - loss: 0.0820 - accuracy: 0.9709 - auc: 0.9933 - val_loss: 1.0420 - val_accuracy: 0.7412 - val_auc: 0.6893\n",
      "Epoch 41/100\n",
      "353/353 [==============================] - 33s 94ms/step - loss: 0.0744 - accuracy: 0.9738 - auc: 0.9943 - val_loss: 1.1980 - val_accuracy: 0.7532 - val_auc: 0.6918\n",
      "Epoch 42/100\n",
      "353/353 [==============================] - 42s 118ms/step - loss: 0.0718 - accuracy: 0.9758 - auc: 0.9948 - val_loss: 1.2499 - val_accuracy: 0.7309 - val_auc: 0.6834\n",
      "Epoch 43/100\n",
      "353/353 [==============================] - 23s 64ms/step - loss: 0.0688 - accuracy: 0.9779 - auc: 0.9951 - val_loss: 1.1348 - val_accuracy: 0.6967 - val_auc: 0.6937\n",
      "Epoch 44/100\n",
      "353/353 [==============================] - 31s 87ms/step - loss: 0.0697 - accuracy: 0.9771 - auc: 0.9950 - val_loss: 1.1526 - val_accuracy: 0.7253 - val_auc: 0.6820\n",
      "Epoch 45/100\n",
      "353/353 [==============================] - 26s 75ms/step - loss: 0.0606 - accuracy: 0.9804 - auc: 0.9958 - val_loss: 1.0399 - val_accuracy: 0.7301 - val_auc: 0.6910\n",
      "Epoch 46/100\n",
      "353/353 [==============================] - 28s 79ms/step - loss: 0.0603 - accuracy: 0.9813 - auc: 0.9961 - val_loss: 1.4751 - val_accuracy: 0.7357 - val_auc: 0.6567\n",
      "Epoch 47/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0716 - accuracy: 0.9757 - auc: 0.9946 - val_loss: 1.3965 - val_accuracy: 0.7444 - val_auc: 0.6992\n",
      "Epoch 48/100\n",
      "353/353 [==============================] - 29s 83ms/step - loss: 0.0681 - accuracy: 0.9793 - auc: 0.9942 - val_loss: 1.0328 - val_accuracy: 0.7237 - val_auc: 0.6906\n",
      "Epoch 49/100\n",
      "353/353 [==============================] - 37s 106ms/step - loss: 0.0619 - accuracy: 0.9802 - auc: 0.9955 - val_loss: 1.1918 - val_accuracy: 0.7381 - val_auc: 0.7013\n",
      "Epoch 50/100\n",
      "353/353 [==============================] - 37s 104ms/step - loss: 0.0580 - accuracy: 0.9813 - auc: 0.9965 - val_loss: 1.1715 - val_accuracy: 0.7253 - val_auc: 0.7027\n",
      "Epoch 51/100\n",
      "353/353 [==============================] - 26s 75ms/step - loss: 0.0618 - accuracy: 0.9796 - auc: 0.9952 - val_loss: 1.2303 - val_accuracy: 0.7556 - val_auc: 0.6875\n",
      "Epoch 52/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0609 - accuracy: 0.9785 - auc: 0.9961 - val_loss: 1.1685 - val_accuracy: 0.7349 - val_auc: 0.6916\n",
      "Epoch 53/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0546 - accuracy: 0.9816 - auc: 0.9967 - val_loss: 1.1720 - val_accuracy: 0.7261 - val_auc: 0.6802\n",
      "Epoch 54/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0543 - accuracy: 0.9833 - auc: 0.9962 - val_loss: 1.1779 - val_accuracy: 0.7150 - val_auc: 0.7097\n",
      "Epoch 55/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0591 - accuracy: 0.9805 - auc: 0.9960 - val_loss: 1.2901 - val_accuracy: 0.7373 - val_auc: 0.6851\n",
      "Epoch 56/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0493 - accuracy: 0.9838 - auc: 0.9966 - val_loss: 1.0730 - val_accuracy: 0.7221 - val_auc: 0.6880\n",
      "Epoch 57/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0470 - accuracy: 0.9867 - auc: 0.9966 - val_loss: 1.1535 - val_accuracy: 0.7396 - val_auc: 0.6895\n",
      "Epoch 58/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0409 - accuracy: 0.9873 - auc: 0.9974 - val_loss: 1.2743 - val_accuracy: 0.7237 - val_auc: 0.6856\n",
      "Epoch 59/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0627 - accuracy: 0.9787 - auc: 0.9954 - val_loss: 1.2367 - val_accuracy: 0.7452 - val_auc: 0.6954\n",
      "Epoch 60/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0606 - accuracy: 0.9816 - auc: 0.9952 - val_loss: 1.1529 - val_accuracy: 0.7118 - val_auc: 0.6919\n",
      "Epoch 61/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0420 - accuracy: 0.9879 - auc: 0.9971 - val_loss: 1.2583 - val_accuracy: 0.7357 - val_auc: 0.6824\n",
      "Epoch 62/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0505 - accuracy: 0.9852 - auc: 0.9968 - val_loss: 1.1158 - val_accuracy: 0.7213 - val_auc: 0.6876\n",
      "Epoch 63/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0601 - accuracy: 0.9820 - auc: 0.9945 - val_loss: 1.2960 - val_accuracy: 0.7205 - val_auc: 0.6827\n",
      "Epoch 64/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0525 - accuracy: 0.9848 - auc: 0.9954 - val_loss: 1.1710 - val_accuracy: 0.7118 - val_auc: 0.6817\n",
      "Epoch 65/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0537 - accuracy: 0.9833 - auc: 0.9957 - val_loss: 1.2172 - val_accuracy: 0.7277 - val_auc: 0.6964\n",
      "Epoch 66/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0595 - accuracy: 0.9804 - auc: 0.9953 - val_loss: 1.0126 - val_accuracy: 0.7428 - val_auc: 0.7090\n",
      "Epoch 67/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0502 - accuracy: 0.9849 - auc: 0.9957 - val_loss: 1.0953 - val_accuracy: 0.7317 - val_auc: 0.7022\n",
      "Epoch 68/100\n",
      "353/353 [==============================] - 22s 64ms/step - loss: 0.0394 - accuracy: 0.9880 - auc: 0.9973 - val_loss: 1.2841 - val_accuracy: 0.7301 - val_auc: 0.6744\n",
      "Epoch 69/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0431 - accuracy: 0.9863 - auc: 0.9970 - val_loss: 1.3955 - val_accuracy: 0.7508 - val_auc: 0.6775\n",
      "Epoch 70/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0441 - accuracy: 0.9871 - auc: 0.9964 - val_loss: 1.1377 - val_accuracy: 0.7277 - val_auc: 0.6996\n",
      "Epoch 71/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0445 - accuracy: 0.9873 - auc: 0.9963 - val_loss: 1.2022 - val_accuracy: 0.7197 - val_auc: 0.7040\n",
      "Epoch 72/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0552 - accuracy: 0.9837 - auc: 0.9959 - val_loss: 1.0300 - val_accuracy: 0.7174 - val_auc: 0.7049\n",
      "Epoch 73/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0540 - accuracy: 0.9823 - auc: 0.9961 - val_loss: 1.1234 - val_accuracy: 0.7341 - val_auc: 0.6923\n",
      "Epoch 74/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0360 - accuracy: 0.9900 - auc: 0.9978 - val_loss: 1.1655 - val_accuracy: 0.7524 - val_auc: 0.7010\n",
      "Epoch 75/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0421 - accuracy: 0.9866 - auc: 0.9969 - val_loss: 1.2206 - val_accuracy: 0.7094 - val_auc: 0.6866\n",
      "Epoch 76/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0557 - accuracy: 0.9837 - auc: 0.9954 - val_loss: 1.1760 - val_accuracy: 0.7277 - val_auc: 0.6949\n",
      "Epoch 77/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0410 - accuracy: 0.9877 - auc: 0.9972 - val_loss: 1.1840 - val_accuracy: 0.7396 - val_auc: 0.6940\n",
      "Epoch 78/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0383 - accuracy: 0.9880 - auc: 0.9982 - val_loss: 1.2253 - val_accuracy: 0.7189 - val_auc: 0.6902\n",
      "Epoch 79/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0381 - accuracy: 0.9871 - auc: 0.9978 - val_loss: 1.3623 - val_accuracy: 0.7452 - val_auc: 0.6630\n",
      "Epoch 80/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0344 - accuracy: 0.9903 - auc: 0.9976 - val_loss: 1.4144 - val_accuracy: 0.7293 - val_auc: 0.6766\n",
      "Epoch 81/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0378 - accuracy: 0.9890 - auc: 0.9974 - val_loss: 1.3787 - val_accuracy: 0.7197 - val_auc: 0.6607\n",
      "Epoch 82/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0412 - accuracy: 0.9874 - auc: 0.9970 - val_loss: 1.2324 - val_accuracy: 0.7285 - val_auc: 0.6822\n",
      "Epoch 83/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0340 - accuracy: 0.9909 - auc: 0.9976 - val_loss: 1.2892 - val_accuracy: 0.7325 - val_auc: 0.6847\n",
      "Epoch 84/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0323 - accuracy: 0.9910 - auc: 0.9975 - val_loss: 1.2576 - val_accuracy: 0.7389 - val_auc: 0.6943\n",
      "Epoch 85/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0354 - accuracy: 0.9896 - auc: 0.9978 - val_loss: 1.1422 - val_accuracy: 0.7245 - val_auc: 0.6832\n",
      "Epoch 86/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0418 - accuracy: 0.9864 - auc: 0.9979 - val_loss: 1.2891 - val_accuracy: 0.7349 - val_auc: 0.6915\n",
      "Epoch 87/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0483 - accuracy: 0.9853 - auc: 0.9959 - val_loss: 1.1576 - val_accuracy: 0.7197 - val_auc: 0.7001\n",
      "Epoch 88/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0362 - accuracy: 0.9885 - auc: 0.9981 - val_loss: 1.3636 - val_accuracy: 0.7373 - val_auc: 0.6892\n",
      "Epoch 89/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0411 - accuracy: 0.9880 - auc: 0.9972 - val_loss: 1.1598 - val_accuracy: 0.7317 - val_auc: 0.6739\n",
      "Epoch 90/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0313 - accuracy: 0.9915 - auc: 0.9978 - val_loss: 1.3059 - val_accuracy: 0.7301 - val_auc: 0.6899\n",
      "Epoch 91/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0269 - accuracy: 0.9934 - auc: 0.9979 - val_loss: 1.2673 - val_accuracy: 0.7317 - val_auc: 0.7092\n",
      "Epoch 92/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0339 - accuracy: 0.9910 - auc: 0.9977 - val_loss: 1.1817 - val_accuracy: 0.7349 - val_auc: 0.6919\n",
      "Epoch 93/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0395 - accuracy: 0.9882 - auc: 0.9977 - val_loss: 1.0547 - val_accuracy: 0.7420 - val_auc: 0.7221\n",
      "Epoch 94/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0382 - accuracy: 0.9899 - auc: 0.9966 - val_loss: 1.0136 - val_accuracy: 0.7341 - val_auc: 0.7064\n",
      "Epoch 95/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0376 - accuracy: 0.9901 - auc: 0.9971 - val_loss: 1.2961 - val_accuracy: 0.7500 - val_auc: 0.6942\n",
      "Epoch 96/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0376 - accuracy: 0.9899 - auc: 0.9969 - val_loss: 1.1686 - val_accuracy: 0.7158 - val_auc: 0.7008\n",
      "Epoch 97/100\n",
      "353/353 [==============================] - 22s 63ms/step - loss: 0.0412 - accuracy: 0.9881 - auc: 0.9973 - val_loss: 1.1242 - val_accuracy: 0.7333 - val_auc: 0.7099\n",
      "Epoch 98/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0308 - accuracy: 0.9914 - auc: 0.9979 - val_loss: 1.3019 - val_accuracy: 0.7428 - val_auc: 0.6889\n",
      "Epoch 99/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0301 - accuracy: 0.9914 - auc: 0.9978 - val_loss: 1.5503 - val_accuracy: 0.7389 - val_auc: 0.6745\n",
      "Epoch 100/100\n",
      "353/353 [==============================] - 22s 62ms/step - loss: 0.0367 - accuracy: 0.9891 - auc: 0.9977 - val_loss: 1.4344 - val_accuracy: 0.7357 - val_auc: 0.6757\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ff, train_labels_ff,\n",
    "    validation_data=(val_ff, val_labels_ff),\n",
    "    epochs=100,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59aa49e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 5s 32ms/step - loss: 1.5210 - accuracy: 0.7382 - auc: 0.6626\n",
      "\n",
      "Test Accuracy of FF++ on Inceptionnet: 73.82%\n",
      "169/169 [==============================] - 5s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.34      0.41      1468\n",
      "           1       0.78      0.89      0.83      3911\n",
      "\n",
      "    accuracy                           0.74      5379\n",
      "   macro avg       0.66      0.61      0.62      5379\n",
      "weighted avg       0.71      0.74      0.72      5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'\\nTest Accuracy of FF++ on Inceptionnet: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test_ff)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels_ff, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84a37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# After training the model\n",
    "model.save('Inceptionnet_160_ff.h5')  # Saves the entire model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907ed965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 11.8%\n",
      "RAM Used: 7973.4 MB\n",
      "Time Usage: 2830.3 s\n",
      "GPU Memory Used: 1980.9 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05661c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA LOADING ===\n",
      "169/169 [==============================] - 12s 48ms/step - loss: 1.5210 - accuracy: 0.7381 - auc: 0.6626\n",
      "Test Accuracy of ff++ on InceptionV3: 73.81%\n",
      "\n",
      "=== RESOURCE USAGE ===\n",
      "CPU Usage: 1.8%\n",
      "RAM Used: 3414.1 MB\n",
      "Time Usage: 18.7 s\n",
      "GPU Memory Used: 681.0 MB\n",
      "Power Consumption: 93W\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "print(\"=== DATA LOADING ===\")\n",
    "start = monitor.get_stats()\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('Inceptionnet_160_ff.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc,test_auc = model.evaluate(test_ff, test_labels_ff)\n",
    "print(f'Test Accuracy of ff++ on InceptionV3: {test_acc * 100:.2f}%')\n",
    "# Your data loading operations here\n",
    "time.sleep(2)  # Simulate loading time\n",
    "\n",
    "end = monitor.get_stats()\n",
    "duration = end['timestamp'] - start['timestamp']\n",
    "\n",
    "print(\"\\n=== RESOURCE USAGE ===\")\n",
    "print(f\"CPU Usage: {end['cpu_%']:.1f}%\")\n",
    "print(f\"RAM Used: {end['ram_mb'] - start['ram_mb']:.1f} MB\")\n",
    "print(f\"Time Usage: {duration:.1f} s\")\n",
    "print(f\"GPU Memory Used: {end['gpu_mem_mb']:.1f} MB\")\n",
    "print(f\"Power Consumption: {int(end['power_w'])}W\")  # Rounded to whole watts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c667401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 25s 250ms/step - loss: 2.7801 - accuracy: 0.4980 - auc: 0.5066\n",
      "Test Accuracy of dfc on Inceptionnet from FF++: 49.80%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('Inceptionnet_160_ff.h5')\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_hog, test_labels)\n",
    "print(f'Test Accuracy of dfc on Inceptionnet from FF++: {test_acc * 100:.2f}%')\n",
    "# You can now use the model for testing or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb52e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 207s 607ms/step - loss: 2.3547 - accuracy: 0.7264 - auc: 0.5482\n",
      "\n",
      "Test Accuracy of Celeb-df dataset on Inceptionnet for FF++: 72.64%\n",
      "340/340 [==============================] - 102s 298ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.03      0.06      2961\n",
      "           1       0.73      0.99      0.84      7891\n",
      "\n",
      "    accuracy                           0.73     10852\n",
      "   macro avg       0.60      0.51      0.45     10852\n",
      "weighted avg       0.66      0.73      0.63     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('Inceptionnet_160_ff.h5')\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_auc = model.evaluate(test, test_labels)\n",
    "print(f'\\nTest Accuracy of Celeb-df dataset on Inceptionnet for FF++: {test_acc*100:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred_binary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
